{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neurogym/neurogym/blob/main/docs/examples/reinforcement_learning.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "4zW6CU8F69Zp"
   },
   "source": [
    "## Reinforcement learning example with stable-baselines3\n",
    "\n",
    "NeuroGym is a toolkit that allows training any network model on many established neuroscience tasks techniques such as standard Supervised Learning or Reinforcement Learning (RL). In this notebook we will use RL to train an LSTM network on the classical Random Dots Motion (RDM) task (Britten et al. 1992).\n",
    "\n",
    "We first show how to install the relevant toolboxes. We then show how build the task of interest (in the example the RDM task), wrapp it with the pass-reward wrapper in one line and visualize the structure of the final task. Finally we train an LSTM network on the task using the A2C algorithm [Mnih et al. 2016](https://arxiv.org/abs/1602.01783) implemented in the [stable-baselines3](https://stable-baselines3.readthedocs.io/en/master/) toolbox, and plot the results.\n",
    "\n",
    "It is straightforward to change the code to train a network on any other available task or using a different RL algorithm (e.g. ACER, PPO2).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k3YKMoHkR2xL"
   },
   "source": [
    "### Installation\n",
    "\n",
    "**Google Colab:** Uncomment and execute cell below when running this notebook on google colab.\n",
    "\n",
    "**Local:** Follow [these instructions](https://github.com/neurogym/neurogym?tab=readme-ov-file#installation) when running\n",
    "this notebook locally.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "hidden": true,
    "id": "Mp-K8wKGtBoE",
    "outputId": "7d01e64f-7f6e-4e4b-895f-6638f6701bf9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ! pip install neurogym[rl]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jzF5leN1R2xU"
   },
   "source": [
    "### Task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1vb3YFfIoOA4"
   },
   "source": [
    "here we build the Random Dots Motion task, specifying the duration of each trial period (fixation, stimulus, decision) and wrapp it with the pass-reward wrapper which appends the previous reward to the observation. We then plot the structure of the task in a figure that shows:\n",
    "\n",
    "1. The observations received by the agent (top panel).\n",
    "2. The actions taken by a random agent and the correct action at each timestep (second panel).\n",
    "3. The rewards provided by the environment at each timestep (third panel).\n",
    "4. The performance of the agent at each trial (bottom panel).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "RaH9CcJdHY5G",
    "outputId": "25cc6eaa-0531-4e53-df94-c8fe3979aaf0"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import neurogym as ngym\n",
    "from neurogym.wrappers import pass_reward\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Task name\n",
    "name = 'PerceptualDecisionMaking-v0'\n",
    "# task specification (here we only specify the duration of the different trial periods)\n",
    "timing = {'fixation': ('constant', 300),\n",
    "          'stimulus': ('constant', 500),\n",
    "          'decision': ('constant', 300)}\n",
    "kwargs = {'dt': 100, 'timing': timing}\n",
    "# build task\n",
    "env = gym.make(name, **kwargs)\n",
    "# print task properties\n",
    "print(env)\n",
    "# wrapp task with pass-reward wrapper\n",
    "env = pass_reward.PassReward(env)\n",
    "# plot example trials with random agent\n",
    "data = ngym.utils.plot_env(env, fig_kwargs={'figsize': (12, 12)}, num_steps=100, ob_traces=['Fixation cue', 'Stim 1', 'Stim 2', 'Previous reward'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OCFMPbzX38Wj"
   },
   "source": [
    "### Train a network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "jAxTPbzL38Wl",
    "outputId": "b7370af5-5628-4cb3-e734-3accfe0fb0e9"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from stable_baselines3.common.policies import LstmPolicy  # TODO: this no longer exists in stable_baselines3\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3 import A2C  # ACER, PPO2\n",
    "warnings.filterwarnings('default')\n",
    "\n",
    "# Optional: PPO2 requires a vectorized environment to run\n",
    "# the env is now wrapped automatically when passing it to the constructor\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "model = A2C(LstmPolicy, env, verbose=1, policy_kwargs={'feature_extraction':\"mlp\"})\n",
    "model.learn(total_timesteps=100000, log_interval=1000)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "svUQlptJAVv9"
   },
   "source": [
    "### Visualize results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "qPrh-kiI8rbB",
    "outputId": "be1e5cbf-23e0-49ca-8468-a93836888772"
   },
   "outputs": [],
   "source": [
    "env = gym.make(name, **kwargs)\n",
    "# print task properties\n",
    "print(env)\n",
    "# wrapp task with pass-reward wrapper\n",
    "env = pass_reward.PassReward(env)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "# plot example trials with random agent\n",
    "data = ngym.utils.plot_env(env, fig_kwargs={'figsize': (12, 12)}, num_steps=100, ob_traces=['Fixation cue', 'Stim 1', 'Stim 2', 'Previous reward'], model=model)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "example_neurogym_rl.ipynb",
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "ngym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
