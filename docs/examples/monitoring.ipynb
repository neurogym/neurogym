{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "4zW6CU8F69Zp"
   },
   "source": [
    "## Monitoring and visualising model activation traces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "xIE7qx_a7D0e"
   },
   "source": [
    "In this notebook, we will use RL to train a network on a simple task, and we will visualise the activation traces of the neurons in the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jzF5leN1R2xU"
   },
   "source": [
    "### Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1vb3YFfIoOA4"
   },
   "source": [
    "We will use the [AnnubesEnv](https://github.com/neurogym/neurogym/blob/dev/neurogym/envs/annubes.py) environment for this demo. We set the duration of each trial period (fixation, stimulus, decision) and wrap the environment with a Monitor wrapper, which can be used to keep track of activation traces. After training, we can evaluate the agent and visualise the recorded traces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "RaH9CcJdHY5G",
    "outputId": "25cc6eaa-0531-4e53-df94-c8fe3979aaf0"
   },
   "outputs": [],
   "source": [
    "# Ignore warnings.\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "from sb3_contrib.common.recurrent.policies import RecurrentActorCriticPolicy\n",
    "from sb3_contrib import RecurrentPPO\n",
    "\n",
    "from neurogym.envs.native.annubes import AnnubesEnv\n",
    "from neurogym.wrappers.monitor import Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fixation': 500, 'stimulus': 1000, 'iti': 0}\n",
      "----------------\n",
      "Box(0.0, 1.0, (4,), float32)\n",
      "{'fixation': 0, 'start': 1, 'auditory': 2, 'visual': 3}\n",
      "----------------\n",
      "Discrete(2)\n",
      "{'fixation': 0, 'choice': [1]}\n"
     ]
    }
   ],
   "source": [
    "# Time step duration in ms\n",
    "dt = 10\n",
    "\n",
    "session = {\"auditory\": 0.5, \"visual\": 0.5}\n",
    "catch_prob = 0.25\n",
    "\n",
    "env = AnnubesEnv(session, catch_prob, dt=dt)\n",
    "\n",
    "# Check the custom environment and output additional warnings (if any)\n",
    "check_env(env)\n",
    "\n",
    "# Check the environment with a random agent\n",
    "obs, info = env.reset()\n",
    "n_steps = 10\n",
    "for _ in range(n_steps):\n",
    "    # random action\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    if terminated:\n",
    "        obs, info = env.reset()\n",
    "\n",
    "print(env.timing)\n",
    "print(\"----------------\")\n",
    "print(env.observation_space)\n",
    "print(env.observation_space.name)\n",
    "print(\"----------------\")\n",
    "print(env.action_space)\n",
    "print(env.action_space.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a recurrent model and show the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecurrentActorCriticPolicy(\n",
       "  (features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (pi_features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (vf_features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (mlp_extractor): MlpExtractor(\n",
       "    (policy_net): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "    (value_net): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (action_net): Linear(in_features=64, out_features=2, bias=True)\n",
       "  (value_net): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (lstm_actor): RNN(4, 64)\n",
       "  (lstm_critic): RNN(4, 64)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the agent that will be trained in this environment\n",
    "model = RecurrentPPO(\n",
    "    RecurrentActorCriticPolicy, env, policy_kwargs={\"lstm_hidden_size\": 64, \"recurrent_layer_type\": \"rnn\"},\n",
    ")\n",
    "\n",
    "model.policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a reference to the action net from the policy. For now, it is referred to as `lstm_actor`, even though it is a vanilla RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = model.policy.get_submodule(\"lstm_actor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(4, 64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that the time step is correctly registered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the agent\n",
    "\n",
    "The agent can be trained on different environments. calls both the actor and the critic networks, so if the activations of the value net are needed, now is the opportunity to record them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an environment for evaluation\n",
    "annubes_env = AnnubesEnv(session, catch_prob, dt=dt)\n",
    "\n",
    "# Create a monitor\n",
    "env = Monitor(env, name=f\"NeuroGym Monitor | {annubes_env.__class__.__qualname__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the agent for a number of steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sb3_contrib.ppo_recurrent.ppo_recurrent.RecurrentPPO at 0x7f22425ab230>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the number timesteps for training\n",
    "total_timesteps = 10000\n",
    "\n",
    "# Train the agent.\n",
    "model.learn(total_timesteps=total_timesteps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the shape of a single trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register the action net (a single layer in our case) with the monitor using the `record_activations()` method. The activations will be automatically extracted and stored after each step only for that layer. You can add as many layers as necessary in this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "It seems that a monitor for layers of type '<class 'torch.nn.modules.rnn.RNN'>' has not been implemented yet.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m am = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecord_activations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mActor net\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/github/neurogym/remote/neurogym/wrappers/monitor.py:528\u001b[39m, in \u001b[36mMonitor.record_activations\u001b[39m\u001b[34m(self, layer, name, steps)\u001b[39m\n\u001b[32m    525\u001b[39m     steps = \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m.tmax / \u001b[38;5;28mself\u001b[39m.dt)\n\u001b[32m    527\u001b[39m \u001b[38;5;66;03m# Create an activation monitor\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m528\u001b[39m am = \u001b[43mActivationMonitor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    530\u001b[39m \u001b[38;5;66;03m# Check if we are replacing an existing monitor\u001b[39;00m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.activations:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/github/neurogym/remote/neurogym/wrappers/components/parameters/activation.py:20\u001b[39m, in \u001b[36mActivationMonitor.__init__\u001b[39m\u001b[34m(self, module, steps, name, populations)\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Activation monitoring component.\u001b[39;00m\n\u001b[32m     11\u001b[39m \n\u001b[32m     12\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m \n\u001b[32m     18\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mself\u001b[39m.steps: \u001b[38;5;28mint\u001b[39m = steps\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28mself\u001b[39m.monitor = \u001b[43mLayerMonitorBase\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_monitor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fw_activation_hook\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpopulations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;28mself\u001b[39m.name = name\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# The neuron count\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/github/neurogym/remote/neurogym/wrappers/components/layers/base.py:56\u001b[39m, in \u001b[36mLayerMonitorBase.get_monitor\u001b[39m\u001b[34m(layer, hook, populations)\u001b[39m\n\u001b[32m     53\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m layer_monitor(layer, hook, populations=populations)  \u001b[38;5;66;03m# type: ignore[abstract]\u001b[39;00m\n\u001b[32m     55\u001b[39m msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIt seems that a monitor for layers of type \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m has not been implemented yet.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(msg)\n",
      "\u001b[31mNotImplementedError\u001b[39m: It seems that a monitor for layers of type '<class 'torch.nn.modules.rnn.RNN'>' has not been implemented yet."
     ]
    }
   ],
   "source": [
    "am = env.record_activations(actor, \"Actor net\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the shapes of the neuron populations in this layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.monitor.get_population_shapes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the agent over a number of trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = 10\n",
    "\n",
    "# Evaluate the policy\n",
    "env.evaluate_policy(n_trials, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the length of the history. Should be equal to the number of trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am.history[\"hidden\"][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the activations in a separate panel for each neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = env.plot_activations(am.name, \"hidden\", neurons=[0,1,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also easily plot the __mean__ activations for each neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_mean, axes_mean = env.plot_activations(am.name, \"hidden\", neurons = [0,1,2], mean = True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "example_neurogym_rl.ipynb",
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "neurogym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
