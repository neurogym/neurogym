{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context-Dependent Decision Making Task\n",
    "\n",
    "This environment implements a context-dependent perceptual decision-making task, in which the agent has to perform a perceptual decision that depends on a context that is explicitly indicated on each trial. The environment is a simplified version of the original task ([Mante et al. 2013](https://www.nature.com/articles/nature12742)), that tests the agents' ability to flexibly switch between different contexts by making them choose between two options (left or right) based on the stimulus evidence associated with the relevant context (e.g., motion and color). The key features of the task are:\n",
    "\n",
    "1. The relevant context is explicitly signaled on each trial.\n",
    "2. Choices are represented as angles evenly spaced around a circle. Note that the number of choices can be configured via `dim_ring`. With the default of 2 choices, this corresponds to:\n",
    "\n",
    "   - Position 1: 0° (left choice)\n",
    "   - Position 2: 180° (right choice)\n",
    "\n",
    "3. Stimulus for each context is represented as a cosine modulation peaked at one of these positions.\n",
    "4. The correct choice (ground truth) is randomly chosen on each trials (for 2 choices: left or right).\n",
    "5. The stimulus evidence (coherence) of each context is also randomly chosen on each trial.\n",
    "\n",
    "For example, if the context signal indicates modality 0, the agent must:\n",
    "\n",
    "- Choose position 1 (left) when context 0's coherence peaks at 0°\n",
    "- Choose position 2 (right) when context 0's coherence peaks at 180°\n",
    "\n",
    "In this notebook, we will:\n",
    "\n",
    "1. Train an agent on the task using supervised learning and reinforcement learning with [Stable-Baselines3](https://stable-baselines3.readthedocs.io/).\n",
    "2. Compare the behavior of agents trained with supervised learning and reinforcement learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training an Agent on the Context-Dependent Decision Making Task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Install Dependencies\n",
    "\n",
    "To begin, install the `neurogym` package. This will automatically install all required dependencies, including Stable-Baselines3.\n",
    "\n",
    "For detailed instructions on how to install `neurogym` within a conda environment or in editable mode, refer to the [installation instructions](https://github.com/neurogym/neurogym?tab=readme-ov-file#installation).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to install\n",
    "# ! pip install neurogym[rl]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Environment Setup and Initial Agent Behavior\n",
    "\n",
    "Let's now create and explore the environment using the `ContextDecisionMaking` class from neurogym. We'll use the default configuration for explicit context mode (`use_expl_context = True`) which includes:\n",
    "\n",
    "- `dim_ring = 2`: Two possible choices (left/right) represented at 0° and 180°. Note that the ring architecture can support any number of choices, making it suitable for more complex decision-making scenarios. The environment will provide context signals indicating which modality is relevant for each trial, allowing the agent to flexibly adapt its decision strategy.\n",
    "- `timing = {'fixation': 300, 'stimulus': 750, 'delay': ~600, 'decision': 100}` (in milliseconds).\n",
    "- `rewards = {'abort': -0.1, 'correct': +1.0}`; abort is a penalty applied when the agent fails to fixate. The task allows the trial to be aborted if fixation does not occur, which is where the name of this penalty comes from.\n",
    "- `sigma = 1.0`: Standard deviation of the noise added to the inputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import neurogym as ngym\n",
    "import gymnasium as gym\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from neurogym.wrappers.monitor import Monitor\n",
    "from sb3_contrib import RecurrentPPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from collections import Counter\n",
    "from neurogym.utils.ngym_random import TruncExp\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Environment Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment parameters\n",
    "EVAL_TRIALS = 1000\n",
    "use_expl_context = True  # Use explicit context mode\n",
    "# These are the default values, shown here to demonstrate how they can be modified:\n",
    "dt = 100\n",
    "dim_ring = 2  # Number of choices in the ring representation\n",
    "abort = False  # Whether to allow aborting the trial if the agent does not fixate\n",
    "rewards = {\n",
    "    \"abort\": -0.1,\n",
    "    \"correct\": +1.0,\n",
    "    \"fail\": 0.0\n",
    "}\n",
    "timing = {\n",
    "    \"fixation\": 300,\n",
    "    \"stimulus\": 750,\n",
    "    \"delay\": TruncExp(600, 300, 3000),\n",
    "    \"decision\": 100,\n",
    "}\n",
    "sigma = 1.0 # Standard deviation of the Gaussian noise in the ring representation\n",
    "\n",
    "# We can modify any of these parameters by passing them to gym.make():\n",
    "kwargs = {\n",
    "    \"use_expl_context\": use_expl_context,\n",
    "    \"dt\": dt,\n",
    "    \"dim_ring\": dim_ring,\n",
    "    \"rewards\": rewards,\n",
    "    \"timing\": timing,\n",
    "    \"sigma\": sigma,\n",
    "    \"abort\": abort,\n",
    "}\n",
    "\n",
    "# Create and wrap the environment\n",
    "task = \"ContextDecisionMaking-v0\"\n",
    "env = gym.make(task, **kwargs)\n",
    "\n",
    "# Check the custom environment and output additional warnings (if any)\n",
    "check_env(env)\n",
    "\n",
    "# Print environment specifications\n",
    "print(\"Trial timing (in milliseconds):\")\n",
    "print(env.timing)\n",
    "\n",
    "print(\"\\nObservation space structure:\")\n",
    "print(env.observation_space)\n",
    "print(\"Observation components:\")\n",
    "print(env.observation_space.name)\n",
    "\n",
    "print(\"\\nAction space structure:\")\n",
    "print(env.action_space)\n",
    "print(\"Action mapping:\")\n",
    "print(env.action_space.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Random Agent Behavior\n",
    "\n",
    "Let's now plot the behavior of a random agent on the task. The agent will randomly choose between the two options (left/right, blue line), and we will visualize its behavior over 5 trials. We will also plot the reward received by the agent at each time step, as well as the performance on each trial. Note that performance is only defined at the end of a trial: it is 1 if the agent made the correct choice, and 0 otherwise.\n",
    "\n",
    "To keep track of the agent's behavior, we will use the `Monitor` wrapper, which monitors training by:\n",
    "\n",
    "- Tracking and saving behavioral data (rewards, actions, observations) every `sv_per` steps.\n",
    "- Generating visualization figures during training if` sv_fig=True`.\n",
    "- Providing progress information if `verbose=True`.\n",
    "\n",
    "Here, we’ll use the wrapper solely to compute the agent’s performance, but later it will help us assess learning and save intermediate results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, info = env.reset()\n",
    "\n",
    "# Visualize example trials\n",
    "fig = ngym.utils.plot_env(\n",
    "    env,\n",
    "    name='Mante et al.',\n",
    "    ob_traces=[\n",
    "        'Fixation',\n",
    "        'Stim 1, Mod 1',  # First stimulus component of modality 1\n",
    "        'Stim 2, Mod 1',  # Second stimulus component of modality 1\n",
    "        'Stim 1, Mod 2',  # First stimulus component of modality 2\n",
    "        'Stim 2, Mod 2',  # Second stimulus component of modality 2\n",
    "        'Context 1',      # First context signal\n",
    "        'Context 2',      # Second context signal\n",
    "    ],\n",
    "    num_trials=5\n",
    ")\n",
    "\n",
    "# Evaluate performance of the environment before training\n",
    "eval_monitor = Monitor(\n",
    "    env\n",
    ")\n",
    "print(\"\\nEvaluating random policy performance...\")\n",
    "metrics = eval_monitor.evaluate_policy(num_trials=EVAL_TRIALS)\n",
    "print(f\"\\nRandom policy metrics ({EVAL_TRIALS:,} trials):\")\n",
    "print(f\"Mean performance: {metrics['mean_performance']:.4f}\")\n",
    "print(f\"Mean reward: {metrics['mean_reward']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the agent's behavior is entirely random. It does not learn to fixate or to choose the correct option based on contextual cues. As a result, its performance is also random. Through training, we expect the agent to improve by learning to respect the fixation period, use context cues to infer the relevant context, and map signal peaks to the correct choices in the ring representation. Let’s move on to training the agent to see whether it can learn these key aspects of the task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Training and Evaluating the Agent\n",
    "\n",
    "We will now train the agent using Stable-Baselines3’s implementation of [PPO (Proximal Policy Optimization)](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html), a widely used reinforcement learning algorithm known for its stability and efficiency.\n",
    "\n",
    "To support recurrent policies, we will use [RecurrentPPO](https://sb3-contrib.readthedocs.io/en/master/modules/ppo_recurrent.html#recurrent-ppo), which extends PPO with recurrent neural networks, specifically LSTMs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Training the Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of trials to train on\n",
    "trial_length_stats = env.trial_length_stats(num_trials=EVAL_TRIALS)\n",
    "avg_timesteps = int(trial_length_stats[\"mean\"])\n",
    "TRAIN_TRIALS = 10000  # Choose the desired number of trials\n",
    "total_timesteps = TRAIN_TRIALS * avg_timesteps\n",
    "print(f\"Training for {TRAIN_TRIALS:,} trials ≈ {total_timesteps:,} timesteps\")\n",
    "\n",
    "# Configure monitoring with trial-appropriate parameters\n",
    "trials_per_figure = 10  # Show 10 trials in each figure\n",
    "steps_per_figure = int(trials_per_figure * avg_timesteps)\n",
    "\n",
    "train_monitor = Monitor(\n",
    "    env,\n",
    "    trigger=\"trial\",              # Save based on completed trials\n",
    "    interval=1000,                # Save data every 1000 trials\n",
    "    plot_create=True,             # Save visualization figures\n",
    "    plot_steps=steps_per_figure,  # Number of steps to visualize on the figure\n",
    "    verbose=True,                 # Print stats when data is saved\n",
    ")\n",
    "\n",
    "# DummyVecEnv is Stable-Baselines3 wrapper that converts the environment\n",
    "# into a vectorized form (required by PPO), allowing for parallel training of multiple environments\n",
    "env_vec = DummyVecEnv([lambda: train_monitor])\n",
    "\n",
    "# Create and train Recurrent PPO agent\n",
    "# Set n_steps to be a multiple of your average trial length\n",
    "trials_per_batch = 64\n",
    "n_steps = int(avg_timesteps * trials_per_batch)  # Collect approximately 64 trials per update\n",
    "batch_size = 32  # Small batch size for short episodes\n",
    "policy_kwargs = {\n",
    "    \"lstm_hidden_size\": 64,  # Small LSTM for short sequences\n",
    "    \"n_lstm_layers\": 1,      # Single layer is sufficient\n",
    "    \"shared_lstm\": True,     # Share LSTM to reduce parameters\n",
    "    \"enable_critic_lstm\": False,  # Disable separate LSTM for critic when sharing\n",
    "}\n",
    "rl_model = RecurrentPPO(\n",
    "    \"MlpLstmPolicy\",\n",
    "    env_vec,\n",
    "    learning_rate=5e-4,       # Slightly higher for faster learning in short episodes\n",
    "    n_steps=n_steps,          # Align with multiple complete episodes\n",
    "    batch_size=32,            # Smaller batch size\n",
    "    ent_coef=0.01,            # Promote exploration\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "rl_model.learn(total_timesteps=total_timesteps, log_interval=int(total_timesteps/10))\n",
    "env_vec.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Plot the Behavior of the Trained Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot example trials with trained agent\n",
    "fig = ngym.utils.plot_env(\n",
    "    env_vec,\n",
    "    name='Mante et al. (trained)',\n",
    "    ob_traces=[\n",
    "        'Fixation',\n",
    "        'Stim 1, Mod 1',  # First stimulus component of modality 1\n",
    "        'Stim 2, Mod 1',  # Second stimulus component of modality 1\n",
    "        'Stim 1, Mod 2',  # First stimulus component of modality 2\n",
    "        'Stim 2, Mod 2',  # Second stimulus component of modality 2\n",
    "        'Context 1',      # First context signal\n",
    "        'Context 2',      # Second context signal\n",
    "    ],\n",
    "    num_trials=5,\n",
    "    model=rl_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, we visualize the agent's behavior on a few example trials. In contrast to the random agent, we should now see:\n",
    "\n",
    "- Consistent fixation maintenance during the fixation period\n",
    "- Choices that correlate with the evidence in the modality indicated by the context signal\n",
    "- No systematic relationship between choices and signals from the irrelevant modality\n",
    "- Performance significantly above chance level (0.5), reflecting successful context-dependent decisions\n",
    "\n",
    "The plot shows the trained agent's behavior across 5 example trials, allowing us to visualize how well it has learned to flexibly switch its attention between modalities based on the context signal and make appropriate choices using the ring representation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Evaluate the Agent's Performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate performance of the trained model\n",
    "print(\"\\nEvaluating trained model performance...\")\n",
    "rl_trained_metrics = eval_monitor.evaluate_policy(num_trials=EVAL_TRIALS, model=rl_model)\n",
    "print(f\"\\nTrained model metrics ({EVAL_TRIALS:,} trials):\")\n",
    "print(f\"Mean performance: {rl_trained_metrics['mean_performance']:.4f}\")\n",
    "print(f\"Mean reward: {rl_trained_metrics['mean_reward']:.4f}\")\n",
    "\n",
    "fig = train_monitor.plot_training_history(figsize=(6, 4), plot_performance=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning the Task as a Supervised Problem\n",
    "\n",
    "We will now train the agent using supervised learning. NeuroGym provides functionality to generate a dataset directly from the environment, allowing us to sample batches of inputs and corresponding labels for training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Converting the Environment to a Supervised Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment parameters\n",
    "task = 'ContextDecisionMaking-v0'\n",
    "kwargs = {\n",
    "    \"use_expl_context\": use_expl_context,\n",
    "    \"dt\": dt,\n",
    "    \"dim_ring\": dim_ring,\n",
    "    \"rewards\": rewards,\n",
    "    \"timing\": timing,\n",
    "    \"sigma\": sigma,\n",
    "    \"abort\": abort,\n",
    "}\n",
    "\n",
    "# Set seq_len to capture 95% of trials, and multiply by 2\n",
    "seq_len = 2*int(trial_length_stats[\"percentile_95\"])\n",
    "print(f\"Using sequence length: {seq_len}\")\n",
    "\n",
    "# Make supervised dataset\n",
    "print(f\"Creating dataset with batch_size={batch_size}\")\n",
    "dataset = ngym.Dataset(task, env_kwargs=kwargs, batch_size=batch_size, seq_len=seq_len)\n",
    "\n",
    "env = dataset.env\n",
    "\n",
    "# Extract dimensions from environment\n",
    "ob_size = env.observation_space.shape[0]\n",
    "act_size = env.action_space.n\n",
    "\n",
    "print(f\"Observation size: {ob_size}\")\n",
    "print(f\"Action size: {act_size}\")\n",
    "\n",
    "# Get a batch of data\n",
    "inputs, target = dataset()\n",
    "\n",
    "# Display shapes and content\n",
    "print(f\"Input batch shape: {inputs.shape}\")\n",
    "print(f\"Target batch shape: {target.shape}\")\n",
    "\n",
    "# Set device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=False)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        # Forward pass with hidden state tracking\n",
    "        lstm_out, hidden = self.lstm(x, hidden)\n",
    "        output = self.fc(lstm_out)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        \"\"\"Initialize hidden state\"\"\"\n",
    "        h0 = torch.zeros(1, batch_size, self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(1, batch_size, self.hidden_size).to(device)\n",
    "        return (h0, c0)\n",
    "\n",
    "# Create the model\n",
    "hidden_size = 128\n",
    "sl_model = Net(\n",
    "    input_size=ob_size,\n",
    "    hidden_size=hidden_size,\n",
    "    output_size=act_size,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Training and Evaluating a Neural Network Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This weighting deprioritizes class 0 while keeping classes 1 and 2 equally important,\n",
    "# aligning with the reward distribution idea from the RL setting\n",
    "class_weights = torch.tensor([0.05, 1, 1]).to(device)\n",
    "# Define the optimizer and loss function\n",
    "optimizer = torch.optim.Adam(sl_model.parameters(), lr=0.01, weight_decay=1e-5)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "EPOCHS = 1000\n",
    "# Training loop\n",
    "loss_history = []\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    # Get a batch of data\n",
    "    inputs, targets = dataset()\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    inputs = torch.from_numpy(inputs).float().to(device)\n",
    "    targets = torch.from_numpy(targets).long().to(device)\n",
    "\n",
    "    # Initialize hidden state\n",
    "    hidden = sl_model.init_hidden(inputs.size(1), device)\n",
    "\n",
    "    # Zero gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass with hidden state tracking\n",
    "    outputs, _ = sl_model(inputs, hidden)\n",
    "\n",
    "    # Reshape for CrossEntropyLoss\n",
    "    outputs_flat = outputs.reshape(-1, outputs.size(2))\n",
    "    targets_flat = targets.reshape(-1)\n",
    "\n",
    "    # Calculate loss\n",
    "    # Weight the loss to account for class imbalance (very low weight to 0s, higher weights to 1s and 2s)\n",
    "    loss = criterion(outputs_flat, targets_flat)\n",
    "\n",
    "    # Backward pass and optimize\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # print statistics\n",
    "    loss_history.append(loss.item())\n",
    "    if i % 50 == 0:\n",
    "        print('Epoch [{}/{}], Loss: {:.4f}'.format(i, EPOCHS, loss.item()))\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# Plot the loss curve\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(loss_history)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss (50-iteration moving average)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = False\n",
    "# Evaluate performance of the trained model\n",
    "sl_model.eval()\n",
    "\n",
    "total_correct = 0\n",
    "total_trials = 0\n",
    "\n",
    "# Evaluate for specified number of trials\n",
    "print(f\"Evaluating model performance across {EVAL_TRIALS} trials...\")\n",
    "\n",
    "for i in range(EVAL_TRIALS):\n",
    "    # Generate a new trial\n",
    "    env.new_trial()\n",
    "    ob, gt = env.ob, env.gt\n",
    "    if verbose:\n",
    "        print(\"observation shape:\", ob.shape)\n",
    "        print(\"ground truth shape:\", gt.shape)\n",
    "\n",
    "    # Handle potentially variable-length trials\n",
    "    trial_length = ob.shape[0]\n",
    "\n",
    "    # Add batch dimension to observation\n",
    "    ob = ob[:, np.newaxis, :]\n",
    "\n",
    "    # Convert to tensor\n",
    "    inputs = torch.from_numpy(ob).float().to(device)\n",
    "    if verbose:\n",
    "        print(\"inputs shape:\", inputs.shape)\n",
    "\n",
    "    # Initialize hidden state for new trial\n",
    "    hidden = sl_model.init_hidden(1, device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs, _ = sl_model(inputs, hidden)\n",
    "        pred_actions = torch.argmax(outputs, dim=2)\n",
    "        if verbose:\n",
    "            print(\"outputs shape:\", outputs.shape)\n",
    "            print(\"predicted actions shape:\", pred_actions.shape)\n",
    "            print(\"predicted actions:\", pred_actions)\n",
    "            print(\"ground truth:\", gt)\n",
    "\n",
    "    # Check only the decision period\n",
    "    decision_idx = trial_length - 1  # Assuming decision is at the end\n",
    "    is_correct = (gt[decision_idx] == pred_actions[decision_idx, 0].cpu().numpy())\n",
    "\n",
    "    total_correct += is_correct\n",
    "\n",
    "    if (i+1) % 100 == 0:\n",
    "        print(f\"Completed {i+1}/{EVAL_TRIALS} trials | Accuracy: {total_correct/(i+1):.4f}\")\n",
    "\n",
    "# Calculate mean performance\n",
    "sl_mean_performance = total_correct / EVAL_TRIALS\n",
    "\n",
    "print(f\"Mean performance across {EVAL_TRIALS} trials: {sl_mean_performance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing RL and SL Approaches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print performance comparison\n",
    "rl_performance = rl_trained_metrics['mean_performance']\n",
    "sl_performance = sl_mean_performance\n",
    "\n",
    "print(f\"RL model performance: {rl_performance:.4f}\")\n",
    "print(f\"SL model performance: {sl_performance:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neurogym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
