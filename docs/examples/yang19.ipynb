{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Task Contextual Decision-Making (Yang19 Cognitive Tasks)\n",
    "\n",
    "This environment implements a suite of 20 context-dependent cognitive tasks inspired by the work of [Yang et al. (2019)](https://pubmed.ncbi.nlm.nih.gov/30643294/#:~:text=The%20brain%20has%20the%20ability,task%20representations%2C%20a%20critical%20feature). In these tasks, the agent must flexibly switch between different context rules on each trial, similar in spirit to the classic [context-dependent decision-making](https://neurogym.github.io/neurogym/latest/api/envs/#neurogym.envs.native.contextdecisionmaking.ContextDecisionMaking) experiment of [Mante et al. (2013)](https://www.nature.com/articles/nature12742). The key features of this multi-task environment are:\n",
    "\n",
    "1. **Two-Choice Outputs** (`dim_ring=2`): All tasks share a common action space represented as positions on a ring. With `dim_ring = 2`, there are two choice outputs located at 0° and 180° on this ring (interpreted as \"left\" and \"right\" choices, respectively). The agent's goal on each trial is to select the correct one of these two outputs.\n",
    "2. **Stimulus Inputs with Modality-Specific Evidence**: Each task provides sensory evidence (stimuli) that may come from one or two modalities. The stimuli are often encoded via a cosine-tuned input bump centered on one of the choice positions, with its amplitude representing the evidence strength. For example, a stimulus favoring the left choice might produce an activity peak at 0° on the input ring.\n",
    "3. **Randomized Ground-Truth Choices**: The correct choice (ground truth) is randomized on each trial (for two choices: left or right). This means on some trials left is the correct answer, and on others right is correct, preventing the agent from biasing toward one action.\n",
    "4. **Variable Difficulty Evidence**: The strength of the stimulus evidence (e.g., coherence level in a motion stimulus) is also randomly sampled each trial. Sometimes the evidence strongly favors one choice, and other times it is weak or ambiguous.\n",
    "\n",
    "This collection of 20 tasks exercises various cognitive skills, including working memory (e.g., remembering stimuli across a delay in matching tasks), perceptual decision-making (integrating noisy sensory evidence over time), rule-based categorization, and inhibitory control (suppressing or initiating actions under certain rules). All tasks are implemented in a consistent format so that a single neural network agent can learn them together via supervised training.\n",
    "\n",
    "In this notebook, we will:\n",
    "\n",
    "1. Train an agent on the 20-task suite using supervised learning. We will generate trial data from the environment and train a recurrent neural network to predict the correct choices.\n",
    "2. Evaluate the agent's performance and behavior across tasks. After training, we will examine how well the agent learned each of the 20 tasks and whether it can flexibly switch contexts from trial to trial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Install Dependencies\n",
    "\n",
    "To begin, install the `neurogym` package. This will automatically install all required dependencies, including Stable-Baselines3.\n",
    "\n",
    "For detailed instructions on how to install `neurogym` within a conda environment or in editable mode, refer to the [installation instructions](https://github.com/neurogym/neurogym?tab=readme-ov-file#installation).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to install\n",
    "# ! pip install neurogym[rl]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Training an Agent on Yang19 Cognitive Tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Environment Setup and Initial Agent Behavior\n",
    "\n",
    "Let's begin by creating and exploring the environment using the `yang19` collection of tasks from NeuroGym. We’ll use the default configuration for all parameters except `dim_ring`, which we set to 2 in order to represent two alternative choices (left/right) arranged on a ring.\n",
    "\n",
    "To get a sense of the environment dynamics, we’ll visualize a couple of representative tasks (`dm1` and `multidm`) to better understand their structure and how the agent is expected to interact with them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import neurogym as ngym\n",
    "from neurogym.wrappers import ScheduleEnvs\n",
    "from neurogym.utils.scheduler import RandomSchedule\n",
    "from neurogym.utils import plot_env\n",
    "import gymnasium as gym\n",
    "from neurogym.wrappers.monitor import Monitor\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "from IPython.display import clear_output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Environment Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment parameters\n",
    "EVAL_TRIALS = 1000\n",
    "dt = 100\n",
    "dim_ring = 2  # Number of choices in the ring representation\n",
    "rewards = {\n",
    "    \"abort\": -0.1,\n",
    "    \"correct\": +1.0,\n",
    "    \"fail\": 0.0\n",
    "}\n",
    "sigma = 1.0 # Standard deviation of the Gaussian noise in the ring representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### dm1\n",
    "timing = {\n",
    "    \"fixation\": (\"uniform\", (200, 500)),\n",
    "    \"stimulus\": (\"choice\", [200, 400, 600]),\n",
    "    \"decision\": 200,\n",
    "}\n",
    "kwargs = {\n",
    "    \"dt\": dt,\n",
    "    \"dim_ring\": dim_ring,\n",
    "    \"rewards\": rewards,\n",
    "    \"timing\": timing,\n",
    "    \"sigma\": sigma,\n",
    "}\n",
    "task = 'dm1'\n",
    "task = f\"yang19.{task}-v0\"\n",
    "env_dm = gym.make(task, **kwargs)\n",
    "\n",
    "# Print environment specifications\n",
    "print(\"Trial timing (in milliseconds):\")\n",
    "print(env_dm.timing)\n",
    "\n",
    "print(\"\\nObservation space structure:\")\n",
    "print(env_dm.observation_space)\n",
    "\n",
    "print(\"\\nAction space structure:\")\n",
    "print(env_dm.action_space)\n",
    "print(\"Action mapping:\")\n",
    "print(env_dm.action_space.name)\n",
    "\n",
    "obs, info = env_dm.reset()\n",
    "\n",
    "stim1 = [f'Stim {i}, Mod 1' for i in range(1, kwargs['dim_ring'] + 1)]\n",
    "stim2 = [f'Stim {i}, Mod 2' for i in range(1, kwargs['dim_ring'] + 1)]\n",
    "\n",
    "# Visualize example trials\n",
    "fig = plot_env(\n",
    "    env_dm,\n",
    "    name='DM1',\n",
    "    ob_traces=['Fixation'] + stim1 + stim2,\n",
    "    num_trials=5,\n",
    "    plot_performance=True,\n",
    "    fig_kwargs={'figsize': (9, 5)},\n",
    ")\n",
    "\n",
    "for text_obj in fig.findobj(match=plt.Text):\n",
    "    text_obj.set_fontsize(11)\n",
    "fig.savefig(f\"untrained_{task}.pdf\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### multidm\n",
    "timing = {\n",
    "    \"fixation\": (\"uniform\", (200, 500)),\n",
    "    \"stimulus\": 500,\n",
    "    \"decision\": 200,\n",
    "}\n",
    "kwargs = {\n",
    "    \"dt\": dt,\n",
    "    \"dim_ring\": dim_ring,\n",
    "    \"rewards\": rewards,\n",
    "    \"timing\": timing,\n",
    "    \"sigma\": sigma,\n",
    "}\n",
    "task = 'multidm'\n",
    "task = f\"yang19.{task}-v0\"\n",
    "env_multidm = gym.make(task, **kwargs)\n",
    "\n",
    "# Print environment specifications\n",
    "print(\"Trial timing (in milliseconds):\")\n",
    "print(env_multidm.timing)\n",
    "\n",
    "print(\"\\nObservation space structure:\")\n",
    "print(env_multidm.observation_space)\n",
    "\n",
    "print(\"\\nAction space structure:\")\n",
    "print(env_multidm.action_space)\n",
    "print(\"Action mapping:\")\n",
    "print(env_multidm.action_space.name)\n",
    "\n",
    "obs, info = env_multidm.reset()\n",
    "\n",
    "stim1 = [f'Stim {i}, Mod 1' for i in range(1, kwargs['dim_ring'] + 1)]\n",
    "stim2 = [f'Stim {i}, Mod 2' for i in range(1, kwargs['dim_ring'] + 1)]\n",
    "\n",
    "# Visualize example trials\n",
    "fig = plot_env(\n",
    "    env_multidm,\n",
    "    name='MultiDM',\n",
    "    ob_traces=['Fixation'] + stim1 + stim2,\n",
    "    num_trials=5,\n",
    "    plot_performance=True,\n",
    "    fig_kwargs={'figsize': (9, 5)},\n",
    ")\n",
    "\n",
    "for text_obj in fig.findobj(match=plt.Text):\n",
    "    text_obj.set_fontsize(11)\n",
    "fig.savefig(f\"untrained_{task}.pdf\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wrap the `yang19` environments using NeuroGym’s `ScheduleEnvs` wrapper, which allows us to interleave multiple tasks into a single training loop. Each cognitive task is instantiated as a separate environment, and a scheduling policy (`RandomSchedule`) determines which task is sampled on each trial. When `env_input=True`, a one-hot vector is appended to the observation to indicate the currently active task, allowing the agent to learn task-specific behavior within a unified architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment parameters\n",
    "# This setting is low to speed up testing, we recommend setting it to at least 1000\n",
    "EVAL_TRIALS = 100\n",
    "kwargs = {'dt': 100, 'dim_ring': 2}\n",
    "seq_len = 100\n",
    "\n",
    "# Make supervised dataset\n",
    "tasks = ngym.get_collection('yang19')\n",
    "envs = [gym.make(task, **kwargs) for task in tasks]\n",
    "schedule = RandomSchedule(len(envs))\n",
    "env = ScheduleEnvs(envs, schedule=schedule, env_input=True)\n",
    "\n",
    "# Print environment specifications\n",
    "print(\"Trial timing (in milliseconds):\")\n",
    "print(env.timing)\n",
    "\n",
    "print(\"\\nObservation space structure:\")\n",
    "print(env.observation_space)\n",
    "\n",
    "print(\"\\nAction space structure:\")\n",
    "print(env.action_space)\n",
    "print(\"Action mapping:\")\n",
    "print(env.action_space.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 Random Agent Behavior\n",
    "\n",
    "Let's now plot the behavior of a random agent on the task. The agent will randomly choose between the two options (left/right, blue line), and we will visualize its behavior over 5 trials. We will also plot the reward received by the agent at each time step, as well as the performance on each trial. Note that performance is only defined at the end of a trial: it is 1 if the agent made the correct choice, and 0 otherwise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, info = env.reset()\n",
    "\n",
    "# Visualize example trials\n",
    "from neurogym.utils.plotting import plot_env as plot_env_orig\n",
    "\n",
    "fig = plot_env_orig(\n",
    "    env,\n",
    "    name='Yang et al.',\n",
    "    ob_traces=None,\n",
    "    num_trials=5,\n",
    "    plot_performance=True,\n",
    "    fig_kwargs={'figsize': (9, 5)},\n",
    ")\n",
    "for text_obj in fig.findobj(match=plt.Text):\n",
    "    text_obj.set_fontsize(11)\n",
    "fig.savefig(\"untrained_env.pdf\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Evaluate performance of random policy\n",
    "eval_monitor = Monitor(env)\n",
    "print(\"\\nEvaluating random policy performance...\")\n",
    "metrics = eval_monitor.evaluate_policy(num_trials=EVAL_TRIALS)\n",
    "print(f\"\\nRandom policy metrics ({EVAL_TRIALS:,} trials):\")\n",
    "print(f\"Mean performance: {metrics['mean_performance']:.4f}\")\n",
    "print(f\"Mean reward: {metrics['mean_reward']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Learning the Task as a Supervised Problem\n",
    "\n",
    "We will now train the agent using supervised learning. NeuroGym provides functionality to generate a dataset directly from the environment, allowing us to sample batches of inputs and corresponding labels for training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Converting the Environment to a Supervised Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Dataset Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 100\n",
    "print(f\"Using sequence length: {seq_len}\")\n",
    "\n",
    "# Make supervised dataset\n",
    "batch_size = 32\n",
    "print(f\"Creating dataset with batch_size={batch_size}\")\n",
    "dataset = ngym.Dataset(env, batch_size=batch_size, seq_len=seq_len)\n",
    "\n",
    "env = dataset.env\n",
    "\n",
    "# Extract dimensions\n",
    "ob_size = env.observation_space.shape[0]\n",
    "act_size = env.action_space.n\n",
    "print(f\"Observation size: {ob_size}\") # 20 one hot env encoding, 1 fixation, 2 mod 1, 2 mod 2\n",
    "print(f\"Action size: {act_size}\")\n",
    "\n",
    "# Get a batch of data\n",
    "inputs, target = dataset()\n",
    "print(f\"Input batch shape: {inputs.shape}\")\n",
    "print(f\"Target batch shape: {target.shape}\")\n",
    "\n",
    "# Set device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Model Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=False)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        lstm_out, hidden = self.lstm(x, hidden)\n",
    "        output = self.fc(lstm_out)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        h0 = torch.zeros(1, batch_size, self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(1, batch_size, self.hidden_size).to(device)\n",
    "        return (h0, c0)\n",
    "\n",
    "# Create the model\n",
    "hidden_size = 128\n",
    "sl_model = Net(\n",
    "    input_size=ob_size,\n",
    "    hidden_size=hidden_size,\n",
    "    output_size=act_size,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Training and Evaluating a Neural Network Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Training the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This setting is low to speed up testing, we recommend setting it to at least 1000\n",
    "EPOCHS = 100\n",
    "\n",
    "# This weighting deprioritizes class 0 while keeping the other 16 classes equally important,\n",
    "# aligning with the reward distribution idea from the RL setting\n",
    "class_weights = torch.tensor([0.05] + [1.0]*(act_size - 1)).to(device)\n",
    "# Define the optimizer and loss function\n",
    "optimizer = torch.optim.Adam(sl_model.parameters(), lr=0.01, weight_decay=1e-5)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# Training loop\n",
    "loss_history = []\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    # Get a batch of data\n",
    "    inputs, targets = dataset()\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    inputs = torch.from_numpy(inputs).float().to(device)\n",
    "    targets = torch.from_numpy(targets).long().to(device)\n",
    "\n",
    "    # Initialize hidden state\n",
    "    hidden = sl_model.init_hidden(inputs.size(1), device)\n",
    "\n",
    "    # Zero gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass with hidden state tracking\n",
    "    outputs, _ = sl_model(inputs, hidden)\n",
    "\n",
    "    # Reshape for CrossEntropyLoss\n",
    "    outputs_flat = outputs.reshape(-1, outputs.size(2))\n",
    "    targets_flat = targets.reshape(-1)\n",
    "\n",
    "    # Calculate loss\n",
    "    # Weight the loss to account for class imbalance (very low weight to 0s, higher weights to 1s and 2s)\n",
    "    loss = criterion(outputs_flat, targets_flat)\n",
    "\n",
    "    # Backward pass and optimize\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # print statistics\n",
    "    loss_history.append(loss.item())\n",
    "    if i % 100 == 0:\n",
    "        print('Epoch [{}/{}], Loss: {:.4f}'.format(i, EPOCHS, loss.item()))\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# Plot the loss curve\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(loss_history)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss (50-iteration moving average)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Evaluate the Model's Performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = False\n",
    "# Evaluate performance of the trained model\n",
    "sl_model.eval()\n",
    "# Initialize storage for each environment\n",
    "sl_mean_performance = {}\n",
    "for e_i in env.envs:\n",
    "    sl_mean_performance[e_i.spec.id] = []\n",
    "\n",
    "# Evaluate for specified number of trials\n",
    "print(f\"Evaluating model performance across {EVAL_TRIALS} trials...\\n\")\n",
    "\n",
    "for env_idx, e_i in enumerate(env.envs):\n",
    "    data = {\"action\": [], \"gt\": [], \"trial\": []}\n",
    "    total_correct = 0\n",
    "    for trial_idx in range(EVAL_TRIALS):\n",
    "        trial = e_i.new_trial()\n",
    "        data[\"trial\"].append(trial)\n",
    "        ob, gt = e_i.ob, e_i.gt\n",
    "        data[\"gt\"].append(gt[-1])\n",
    "        trial_length = ob.shape[0]\n",
    "\n",
    "        # Add one-hot encoding for the environment\n",
    "        env_one_hot = np.zeros((trial_length, len(env.envs)))\n",
    "        env_one_hot[:, env_idx] = 1.0  # Set the current environment index to 1\n",
    "\n",
    "        # Concatenate original observation with one-hot encoding\n",
    "        ob_with_env = np.concatenate([ob, env_one_hot], axis=1)\n",
    "\n",
    "        ob_with_env = ob_with_env[:, np.newaxis, :]  # Add batch dimension\n",
    "\n",
    "        inputs = torch.from_numpy(ob_with_env).float().to(device)\n",
    "        hidden = sl_model.init_hidden(1, device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs, _ = sl_model(inputs, hidden)\n",
    "            pred_actions = torch.argmax(outputs, dim=2)\n",
    "            data[\"action\"].append(pred_actions[-1, 0].cpu().numpy())\n",
    "\n",
    "        decision_idx = trial_length - 1\n",
    "        is_correct = (gt[decision_idx] == pred_actions[decision_idx, 0].cpu().numpy())\n",
    "        total_correct += is_correct\n",
    "\n",
    "    accuracy = total_correct / EVAL_TRIALS\n",
    "    sl_mean_performance[e_i.spec.id].append(accuracy)\n",
    "    for key in data:\n",
    "        if key != \"trial\":\n",
    "            data[key] = np.array(data[key])\n",
    "\n",
    "\n",
    "# Print average performance\n",
    "print(\"Average performance across all environments:\")\n",
    "for e_i in env.envs:\n",
    "    mean_acc = np.mean(sl_mean_performance[e_i.spec.id])\n",
    "    print(f\"{e_i.spec.id}: {mean_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neurogym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
