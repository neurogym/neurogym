{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptual Decision Making Task\n",
    "\n",
    "This environment implements a two-alternative forced choice perceptual decision-making task, where the agent must integrate noisy sensory evidence over time to make accurate decisions. The task is based on classic motion discrimination experiments ([Britten et al. 1992](https://www.jneurosci.org/content/12/12/4745)) and has been adapted for studying neural mechanisms of decision-making in computational models. The key features of the task are:\n",
    "\n",
    "1. On each trial, a noisy stimulus appears on either the left or right side of the visual field with varying coherence levels (evidence strength).\n",
    "\n",
    "2. Choices are represented as angles evenly spaced around a circle. With the default of 2 choices (`dim_ring=2`), this corresponds to:\n",
    "\n",
    "   - Position 1: 0° (left choice)\n",
    "   - Position 2: 180° (right choice)\n",
    "\n",
    "3. The stimulus is presented as a cosine modulation with additive Gaussian noise, requiring the agent to integrate evidence over time to overcome noise and make accurate decisions.\n",
    "\n",
    "4. The agent can respond at any time after stimulus onset.\n",
    "\n",
    "5. The environment includes blocks where one side is more likely than the other, and augments observations with previous actions and rewards.\n",
    "\n",
    "In this notebook, we will:\n",
    "\n",
    "1. Train an agent on the task using reinforcement learning with [Stable-Baselines3](https://stable-baselines3.readthedocs.io/).\n",
    "2. Analyze the agent's psychometric curves and compare performance across different coherence levels and block contexts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Install Dependencies\n",
    "\n",
    "To begin, install the `neurogym` package. This will automatically install all required dependencies, including Stable-Baselines3.\n",
    "\n",
    "For detailed instructions on how to install `neurogym` within a conda environment or in editable mode, refer to the [installation instructions](https://github.com/neurogym/neurogym?tab=readme-ov-file#installation).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to install\n",
    "# ! pip install neurogym[rl]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Training an Agent on the Perceptual Decision Making Task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Environment Setup and Initial Agent Behavior\n",
    "\n",
    "Let's now create and explore the environment using the `PerceptualDecisionMaking` class from neurogym. We'll use the default configuration which includes:\n",
    "\n",
    "- `dim_ring = 2`: Two possible choices (left/right) represented at 0° and 180°. Note that the ring architecture can support any number of choices, making it suitable for more complex decision-making scenarios.\n",
    "- `timing = {'fixation': ~600, 'stimulus': 2000, 'delay': 0, 'decision': 100}` (in milliseconds).\n",
    "- `rewards = {'abort': -0.1, 'correct': +1.0, 'fail': 0.0}`; abort is a penalty applied when the agent fails to fixate. The task allows the trial to be aborted if fixation does not occur, which is where the name of this penalty comes from.\n",
    "- `sigma = 1.0`: Standard deviation of the noise added to the inputs.\n",
    "\n",
    "In this notebook, several wrappers are used to modify the environment's behavior:\n",
    "\n",
    "- `ReactionTime` wrapper allows the agent to respond at any time after stimulus onset.\n",
    "- `SideBias` wrapper introduces blocks where one side is more likely than the other. It uses two key parameters:\n",
    "  - `probs = [[0.2, 0.8], [0.8, 0.2]]`: Probability matrices defining the likelihood of each choice (only two choices in this example) in different blocks\n",
    "  - `block_dur = (20, 100)`: Block duration randomly sampled between 20-100 trials, determining how long each bias condition persists\n",
    "- `PassAction` and `PassReward` wrappers augment the observations with the previous step's action and reward, respectively, enabling the agent to use recent history in decision-making.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import neurogym as ngym\n",
    "import gymnasium as gym\n",
    "from neurogym.wrappers.monitor import Monitor\n",
    "from sb3_contrib import RecurrentPPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from pathlib import Path\n",
    "from neurogym.utils.ngym_random import TruncExp\n",
    "from neurogym.wrappers.reaction_time import ReactionTime\n",
    "from neurogym.wrappers.pass_action import PassAction\n",
    "from neurogym.wrappers.pass_reward import PassReward\n",
    "from neurogym.wrappers.side_bias import SideBias\n",
    "from neurogym.utils.psychometric import plot_psychometric\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Environment Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment parameters\n",
    "# These settings are low to speed up testing; we recommend setting EVAL_TRIALS to at least 1000\n",
    "EVAL_TRIALS = 100\n",
    "dt = 100\n",
    "dim_ring = 2  # Number of choices in the ring representation\n",
    "abort = False  # Whether to allow aborting the trial if the agent does not fixate\n",
    "rewards = {\n",
    "    \"abort\": -0.1,\n",
    "    \"correct\": +1.0,\n",
    "    \"fail\": 0.0\n",
    "}\n",
    "timing = {\n",
    "    \"fixation\": TruncExp(600, 400, 700),\n",
    "    \"stimulus\": 2000,\n",
    "    \"delay\": 0,\n",
    "    \"decision\": 100,\n",
    "}\n",
    "sigma = 1.0 # Standard deviation of the Gaussian noise in the ring representation\n",
    "\n",
    "kwargs = {\n",
    "    \"dt\": dt,\n",
    "    \"dim_ring\": dim_ring,\n",
    "    \"rewards\": rewards,\n",
    "    \"timing\": timing,\n",
    "    \"sigma\": sigma,\n",
    "    \"abort\": abort,\n",
    "}\n",
    "block_dur = (20, 100) # Extremes of the block duration in milliseconds\n",
    "probs = [[0.2, 0.8], [0.8, 0.2]] # Probabilities of choosing left or right in the two blocks\n",
    "\n",
    "# Create and wrap the environment\n",
    "task = \"PerceptualDecisionMaking-v0\"\n",
    "env = gym.make(task, **kwargs)\n",
    "env = ReactionTime(env, end_on_stimulus=True)\n",
    "env = PassReward(env)\n",
    "env = PassAction(env)\n",
    "env = SideBias(env, probs=probs, block_dur=block_dur)\n",
    "\n",
    "# Print environment specifications\n",
    "print(\"Trial timing (in milliseconds):\")\n",
    "print(env.timing)\n",
    "\n",
    "print(\"\\nObservation space structure:\")\n",
    "print(env.observation_space)\n",
    "\n",
    "print(\"\\nAction space structure:\")\n",
    "print(env.action_space)\n",
    "print(\"Action mapping:\")\n",
    "print(env.action_space.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 Random Agent Behavior\n",
    "\n",
    "Let's now plot the behavior of a random agent on the task. The agent will randomly choose between the two options (left/right), and we will visualize its behavior over 5 trials. We will also plot the reward received by the agent at each time step, as well as the performance on each trial. Note that performance is only defined at the end of a trial: it is 1 if the agent made the correct choice, and 0 otherwise.\n",
    "\n",
    "To keep track of the agent's behavior, we will use the `Monitor` wrapper, which monitors training by:\n",
    "\n",
    "- Tracking and saving behavioral data (rewards, actions, observations) every `sv_per` steps.\n",
    "- Generating visualization figures during training if` sv_fig=True`.\n",
    "- Providing progress information if `verbose=True`.\n",
    "\n",
    "Here, we’ll use the wrapper solely to compute the agent’s performance, but later it will help us assess learning and save intermediate results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize example trials\n",
    "fig = ngym.utils.plot_env(\n",
    "    env,\n",
    "    name='Perceptual Decision Making',\n",
    "    ob_traces=[\n",
    "        'Fixation',\n",
    "        'Stim 1',\n",
    "        'Stim 2',\n",
    "        'PassReward', # Reward for the previous action\n",
    "        'PassAction' # Action taken in the previous step\n",
    "    ],\n",
    "    num_trials=5,\n",
    ")\n",
    "\n",
    "# Evaluate performance of the environment before training\n",
    "eval_monitor = Monitor(\n",
    "    env\n",
    ")\n",
    "print(\"\\nEvaluating random policy performance...\")\n",
    "metrics = eval_monitor.evaluate_policy(num_trials=EVAL_TRIALS)\n",
    "print(f\"\\nRandom policy metrics ({EVAL_TRIALS:,} trials):\")\n",
    "print(f\"Mean performance: {metrics['mean_performance']:.4f}\")\n",
    "print(f\"Mean reward: {metrics['mean_reward']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the agent's behavior is entirely random. Through training, we expect the agent to improve by learning to respect the fixation period, and map signal peaks to the correct choices in the ring representation. Let’s move on to training the agent to see whether it can learn these key aspects of the task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Training and Evaluating the Agent\n",
    "\n",
    "We will now train the agent using Stable-Baselines3’s implementation of [PPO (Proximal Policy Optimization)](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html), a widely used reinforcement learning algorithm known for its stability and efficiency.\n",
    "\n",
    "To support recurrent policies, we will use [RecurrentPPO](https://sb3-contrib.readthedocs.io/en/master/modules/ppo_recurrent.html#recurrent-ppo), which extends PPO with recurrent neural networks, specifically LSTMs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Training the Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of trials to train on\n",
    "# These settings are low to speed up testing; we recommend setting TRAIN_TRIALS to at least 10000 and `interval` in Monitor to 1000\n",
    "avg_timesteps = 7 # Observed\n",
    "TRAIN_TRIALS = 100  # Choose the desired number of trials\n",
    "total_timesteps = TRAIN_TRIALS * avg_timesteps\n",
    "print(f\"Training for {TRAIN_TRIALS:,} trials ≈ {total_timesteps:,} timesteps\")\n",
    "\n",
    "# Configure monitoring with trial-appropriate parameters\n",
    "trials_per_figure = 10  # Show 10 trials in each figure\n",
    "steps_per_figure = int(trials_per_figure * avg_timesteps)\n",
    "\n",
    "train_monitor = Monitor(\n",
    "    env,\n",
    "    trigger=\"trial\",              # Save based on completed trials\n",
    "    interval=100,                # Save data every 100 trials\n",
    "    plot_create=True,             # Save visualization figures\n",
    "    plot_steps=steps_per_figure,  # Number of steps to visualize on the figure\n",
    "    verbose=True,                 # Print stats when data is saved\n",
    ")\n",
    "\n",
    "# DummyVecEnv is Stable-Baselines3 wrapper that converts the environment\n",
    "# into a vectorized form (required by PPO), allowing for parallel training of multiple environments\n",
    "env_vec = DummyVecEnv([lambda: train_monitor])\n",
    "\n",
    "# Create and train Recurrent PPO agent\n",
    "# Set n_steps to be a multiple of your average trial length\n",
    "trials_per_batch = 64\n",
    "n_steps = int(avg_timesteps * trials_per_batch)  # Collect approximately 64 trials per update\n",
    "batch_size = 32  # Small batch size for short episodes\n",
    "policy_kwargs = {\n",
    "    \"lstm_hidden_size\": 128,      # Small LSTM for short sequences\n",
    "    \"n_lstm_layers\": 2,           # Single layer is sufficient\n",
    "    \"shared_lstm\": True,          # Share LSTM to reduce parameters\n",
    "    \"enable_critic_lstm\": False,  # Disable separate LSTM for critic when sharing\n",
    "}\n",
    "rl_model = RecurrentPPO(\n",
    "    \"MlpLstmPolicy\",\n",
    "    env_vec,\n",
    "    learning_rate=3e-4,       # Learning rate for the optimizer\n",
    "    n_steps=n_steps,          # Align with multiple complete episodes\n",
    "    batch_size=32,            # Smaller batch size\n",
    "    ent_coef=0.0,             # Entropy coefficient for exploration\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "rl_model.learn(total_timesteps=total_timesteps, log_interval=int(total_timesteps/10))\n",
    "env_vec.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Plot the Behavior of the Trained Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot example trials with trained agent\n",
    "fig = ngym.utils.plot_env(\n",
    "    env_vec,\n",
    "    name='Perceptual Decision Making (trained)',\n",
    "    ob_traces=[\n",
    "        'Fixation',\n",
    "        'Stim 1',\n",
    "        'Stim 2',\n",
    "        'PassReward',\n",
    "        'PassAction'\n",
    "    ],\n",
    "    num_trials=5,\n",
    "    model=rl_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, we visualize the agent's behavior on a few example trials. In contrast to the random agent, we should now see:\n",
    "\n",
    "- Consistent fixation maintenance during the fixation period\n",
    "- Choices that correlate with the evidence strength of the stimulus\n",
    "- Performance significantly above chance level (0.5), reflecting successful context-dependent decisions\n",
    "\n",
    "The plot shows the trained agent's behavior across 5 example trials, allowing us to visualize how well it has learned to make appropriate choices using the ring representation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Evaluate the Agent's Performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate performance of the last trained model\n",
    "print(\"\\nEvaluating trained model performance...\")\n",
    "rl_trained_metrics = eval_monitor.evaluate_policy(num_trials=EVAL_TRIALS, model=rl_model)\n",
    "print(f\"\\nTrained model metrics ({EVAL_TRIALS:,} trials):\")\n",
    "print(f\"Mean performance: {rl_trained_metrics['mean_performance']:.4f}\")\n",
    "print(f\"Mean reward: {rl_trained_metrics['mean_reward']:.4f}\")\n",
    "\n",
    "fig = train_monitor.plot_training_history(figsize=(6, 4), plot_performance=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.4 Plot the Agent's Psychometric Curves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.getenv(\"GITHUB_ACTIONS\"):\n",
    "    # Evaluate policy and extract data\n",
    "    eval_monitor.evaluate_policy(num_trials=EVAL_TRIALS, model=rl_model)\n",
    "    data = eval_monitor.data_eval\n",
    "\n",
    "    # Extract trial-level fields\n",
    "    trials = data['trial']\n",
    "    coh = np.array([t['coh'] for t in trials])\n",
    "    block = np.array([np.array_equal(t['probs'], np.array(probs[1])) for t in trials]).astype(int) # block 1 is 0, block 2 is 1\n",
    "\n",
    "    # Filter out trials where action is 0 (no action taken)\n",
    "    actions_only_mask = data['action'] != 0\n",
    "    coh = coh[actions_only_mask]\n",
    "    block = block[actions_only_mask]\n",
    "    data_action = data['action'][actions_only_mask]\n",
    "    data_gt = data['gt'][actions_only_mask]\n",
    "\n",
    "    # Convert actions and ground truth to binary (0 = left, 1 = right)\n",
    "    ch = (data_action == 2).astype(int)\n",
    "    gt = (data_gt == 2).astype(int)\n",
    "\n",
    "    # Plotting setup\n",
    "    fig, ax = plt.subplots(figsize=(3, 3))\n",
    "\n",
    "    # Plot psychometric curves for each block\n",
    "    for i, blk in enumerate(range(len(probs))):\n",
    "        # Filter trials matching current block\n",
    "        mask = block == blk\n",
    "        ev = coh[mask]\n",
    "        ch_m = ch[mask]\n",
    "        ref = gt[mask]\n",
    "\n",
    "        # Signed evidence: negative if correct answer is left\n",
    "        sig_ev = np.where(ref == 0, -ev, ev)\n",
    "\n",
    "        plot_psychometric(sig_ev, ch_m, ax, legend=f'Block {blk+1}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neurogym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
