{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to NeuroGym","text":"<p>NeuroGym is a curated collection of neuroscience tasks with a common interface. The goal is to facilitate training of neural network models on neuroscience tasks.</p> <p>NeuroGym inherits from the machine learning toolkit Gymnasium, a maintained fork of OpenAI\u2019s Gym library. It allows a wide range of well established machine learning algorithms to be easily trained on behavioral paradigms relevant for the neuroscience community. NeuroGym also incorporates several properties and functions (e.g. continuous-time and trial-based tasks) that are important for neuroscience applications. The toolkit also includes various modifier functions that allow easy configuration of new tasks.</p> <p></p> <p>\ud83d\udc1b Bugs reports and \u2b50 features requests here</p> <p>\ud83d\udd27 Pull Requests</p> <p>For more details about how to contribute, see the contribution guidelines.</p>"},{"location":"code_of_conduct/","title":"Code of Conduct","text":""},{"location":"code_of_conduct/#our-pledge","title":"Our Pledge","text":"<p>In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p>"},{"location":"code_of_conduct/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to creating a positive environment include:</p> <ul> <li>Using welcoming and inclusive language</li> <li>Being respectful of differing viewpoints and experiences</li> <li>Gracefully accepting constructive criticism</li> <li>Focusing on what is best for the community</li> <li>Showing empathy towards other community members</li> </ul> <p>Examples of unacceptable behavior by participants include:</p> <ul> <li>The use of sexualized language or imagery and unwelcome sexual attention or   advances</li> <li>Trolling, insulting/derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or electronic   address, without explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"code_of_conduct/#our-responsibilities","title":"Our Responsibilities","text":"<p>Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.</p> <p>Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.</p>"},{"location":"code_of_conduct/#scope","title":"Scope","text":"<p>This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.</p>"},{"location":"code_of_conduct/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at g.crocioni@esciencecenter.nl or manuelmolanomazon@gmail.com. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.</p> <p>Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.</p>"},{"location":"code_of_conduct/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html</p>"},{"location":"contributing/","title":"Contributing","text":"<p>We welcome any kind of contribution to our software, from simple comment or question to a full fledged pull request. Please read and follow our Code of Conduct.</p> <p>A contribution can be one of the following cases:</p> <ol> <li>you have a question;</li> <li>you think you may have found a bug (including unexpected behavior);</li> <li>you want to make some kind of change to the code base (e.g. to fix a bug, to add a new feature, to update documentation);</li> <li>you want to make a new release of the code base.</li> </ol> <p>The sections below outline the steps in each case.</p>"},{"location":"contributing/#you-have-a-question","title":"You have a question","text":"<ol> <li>use the search functionality here to see if someone already filed the same issue;</li> <li>if your issue search did not yield any relevant results, make a new issue;</li> <li>apply the \"Question\" label; apply other labels when relevant.</li> </ol>"},{"location":"contributing/#you-think-you-may-have-found-a-bug","title":"You think you may have found a bug","text":"<ol> <li>use the search functionality here to see if someone already filed the same issue;</li> <li>if your issue search did not yield any relevant results, make a new issue, making sure to provide enough information to the rest of the community to understand the cause and context of the problem. Depending on the issue, you may want to include:</li> <li>the SHA hashcode of the commit that is causing your problem;</li> <li>some identifying information (name and version number) for dependencies you're using;</li> <li>information about the operating system;</li> <li>apply relevant labels to the newly created issue.</li> </ol>"},{"location":"contributing/#you-want-to-make-some-kind-of-change-to-the-code-base","title":"You want to make some kind of change to the code base","text":"<ol> <li>(important) announce your plan to the rest of the community before you start working. This announcement should be in the form of a (new) issue;</li> <li>(important) wait until some kind of consensus is reached about your idea being a good idea;</li> <li>if needed, fork the repository to your own Github profile and create your own feature branch off of the latest master commit. While working on your feature branch, make sure to stay up to date with the master branch by pulling in changes, possibly from the 'upstream' repository (follow the instructions here and here);</li> <li>make sure the existing tests still work by running <code>pytest</code>;</li> <li>add your own tests (if necessary);</li> <li>update or expand the documentation;</li> <li>update the <code>CHANGELOG.md</code> file with change;</li> <li>push your feature branch to (your fork of) the current repository on GitHub;</li> <li>create the pull request, e.g. following the instructions here.</li> </ol> <p>In case you feel like you've made a valuable contribution, but you don't know how to write or run tests for it, or how to generate the documentation: don't let this discourage you from making the pull request; we can help you! Just go ahead and submit the pull request, but keep in mind that you might be asked to append additional commits to your pull request.</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#1-create-a-virtual-environment","title":"1. Create a Virtual Environment","text":"<p>Create and activate a virtual environment to install the current package, e.g. using conda (please refer to their site for questions about creating the environment):</p> <pre><code>conda activate # ensures you are in the base environment\nconda create -n neurogym python=3.11 -y\nconda activate neurogym\n</code></pre>"},{"location":"installation/#2-install-neurogym","title":"2. Install NeuroGym","text":"<p>Install the latest stable release of <code>neurogym</code> using pip:</p> <pre><code>pip install neurogym\n</code></pre>"},{"location":"installation/#21-reinforcement-learning-support","title":"2.1 Reinforcement Learning Support","text":"<p>NeuroGym includes optional reinforcement learning (RL) features via Stable-Baselines3. To install these, choose one of the two options below depending on your hardware setup:</p>"},{"location":"installation/#option-a-cpu-only-recommended-for-most-users","title":"Option A \u2014 CPU-only (recommended for most users):","text":"<p>NeuroGym includes optional reinforcement learning (RL) features via Stable-Baselines3. To install these, choose one of the two options below depending on your hardware setup:</p> <pre><code>pip install neurogym[rl]\n</code></pre> <p>NOTE for Linux/WSL users: If you do not have access to a CUDA-capable NVIDIA GPU (which is the case for most users), above line will install up to 1.5GB of unnecessary GPU libraries. To avoid excessive overhead, we recommend first isntalling the CPU-only version of PyTorch:</p> <pre><code>pip install torch --index-url https://download.pytorch.org/whl/cpu\npip install neurogym[rl]\n</code></pre>"},{"location":"installation/#22-editabledevelopment-mode","title":"2.2 Editable/Development Mode","text":"<p>To contribute to NeuroGym or run it from source with live code updates:</p> <pre><code>git clone https://github.com/neurogym/neurogym.git\ncd neurogym\npip install -e .\n</code></pre> <p>This installs the package in editable mode, so changes in source files are reflected without reinstalling.</p> <p>To include both RL and development tools (e.g., for testing, linting, documentation):</p> <pre><code>pip install -e .[rl,dev]\n</code></pre>"},{"location":"installation/#3-psychopy-installation-optional","title":"3. Psychopy Installation (Optional)","text":"<p>NOTE: psycohopy installation is currently not working</p> <p>If you need psychopy for your project, additionally run</p> <pre><code>pip install psychopy\n</code></pre>"},{"location":"neurogym/","title":"NeuroGym","text":""},{"location":"neurogym/#tasks","title":"Tasks","text":"<p>Currently implemented tasks can be found here.</p>"},{"location":"neurogym/#wrappers","title":"Wrappers","text":"<p>Wrappers (see here) are short scripts that allow introducing modifications the original tasks. For instance, the Random Dots Motion task can be transformed into a reaction time task by passing it through the reaction_time wrapper. Alternatively, the combine wrapper allows training an agent in two different tasks simultaneously.</p>"},{"location":"neurogym/#configuration","title":"Configuration","text":"<p>\ud83e\uddea Beta Feature \u2014 The configuration system is optional and currently under development. You can still instantiate environments, agents, and wrappers with direct parameters. It is only used in a small portion of the codebase and is not required for typical usage. See the <code>demo.ipynb</code> notebook for the only current example of this system in action.</p> <p>NeuroGym includes a flexible configuration mechanism using <code>Pydantic Settings</code>, allowing configuration via TOML files, Python objects, or plain dictionaries.</p> <p>Using a TOML file can be especially useful for sharing experiment configurations in a portable way (e.g., sending <code>config.toml</code> to a colleague), reliably saving and loading experiment setups, and easily switching between multiple configurations for the same environment by changing just one line of code. While the system isn't at that stage yet, these are intended future capabilities.</p>"},{"location":"neurogym/#1-from-a-toml-file","title":"1. From a TOML file","text":"<p>Create a <code>config.toml</code> file (see template) and load it:</p> <pre><code>from neurogym import Config\nconfig = Config('path/to/config.toml')\n</code></pre> <p>You can then pass this config to any component that supports it:</p> <pre><code>from neurogym.wrappers import monitor\nenv = gym.make('GoNogo-v0')\nenv = monitor.Monitor(env, config=config)\n</code></pre> <p>Or directly pass the path:</p> <pre><code>env = monitor.Monitor(env, config='path/to/config.toml')\n</code></pre>"},{"location":"neurogym/#2-with-python-class","title":"2. With Python Class","text":"<pre><code>from neurogym import Config\nconfig = Config(\n    local_dir=\"logs/\",\n    env={\"name\": \"GoNogo-v0\"},\n    monitor={\"name\": \"MyMonitor\"}\n)\n</code></pre>"},{"location":"neurogym/#3-with-a-dictionary","title":"3. With a Dictionary","text":"<pre><code>from neurogym import Config\nconfig_dict = {\n    \"env\": {\"name\": \"GoNogo-v0\"},\n    \"monitor\": {\n        \"name\": \"MyMonitor\",\n        \"plot\": {\"trigger\": \"step\", \"value\": 500, \"create\": True}\n    },\n    \"local_dir\": \"./outputs\"\n}\nconfig = Config.model_validate(config_dict)\n</code></pre>"},{"location":"neurogym/#examples","title":"Examples","text":"<p>NeuroGym is compatible with most packages that use gymnasium. In this example jupyter notebook we show how to train a neural network with reinforcement learning algorithms using the Stable-Baselines3 toolbox.</p>"},{"location":"neurogym/#vanilla-rnn-support-in-recurrentppo","title":"Vanilla RNN Support in RecurrentPPO","text":"<p>We extended the <code>RecurrentPPO</code> implementation from <code>stable-baselines3-contrib</code> to support vanilla RNNs (<code>torch.nn.RNN</code>) in addition to LSTMs. This is particularly useful for neuroscience applications, where simpler recurrent architectures can be more biologically interpretable.</p> <p>You can enable vanilla RNNs by setting <code>recurrent_layer_type=\"rnn\"</code> in the <code>policy_kwargs</code>:</p> <pre><code>from sb3_contrib import RecurrentPPO\n\npolicy_kwargs = {\"recurrent_layer_type\": \"rnn\"}  # \"lstm\" is the default\nmodel = RecurrentPPO(\"MlpLstmPolicy\", env_vec, policy_kwargs=policy_kwargs, verbose=1)\nmodel.learn(5000)\n</code></pre> <p>Note: This feature is part of an open pull request to the upstream repository and is currently under review by the maintainers. Until the pull request is merged, you can use this functionality by installing NeuroGym organization's fork of the repository. To do so, uninstall the original package and install from the custom branch:</p> <pre><code>pip uninstall stable-baselines3-contrib -y\npip install git+https://github.com/neurogym/stable-baselines3-contrib.git@rnn_policy_addition\n</code></pre> <p>This will install the version with vanilla RNN support from the <code>rnn_policy_addition</code> branch in our fork.</p>"},{"location":"neurogym/#custom-tasks","title":"Custom Tasks","text":"<p>Creating custom new tasks should be easy. You can contribute tasks using the regular gymnasium format. If your task has a trial/period structure, this template provides the basic structure that we recommend a task to have:</p> <pre><code>from gymnasium import spaces\nimport neurogym as ngym\n\nclass YourTask(ngym.PeriodEnv):\n    metadata = {}\n\n    def __init__(self, dt=100, timing=None, extra_input_param=None):\n        super().__init__(dt=dt)\n\n\n    def new_trial(self, **kwargs):\n        \"\"\"\n        new_trial() is called when a trial ends to generate the next trial.\n        Here you have to set:\n        The trial periods: fixation, stimulus...\n        Optionally, you can set:\n        The ground truth: the correct answer for the created trial.\n        \"\"\"\n\n    def _step(self, action):\n        \"\"\"\n        _step receives an action and returns:\n            a new observation, obs\n            reward associated with the action, reward\n            a boolean variable indicating whether the experiment has terminated, terminated\n                See more at https://gymnasium.farama.org/tutorials/gymnasium_basics/handling_time_limits/#termination\n            a boolean variable indicating whether the experiment has been truncated, truncated\n                See more at https://gymnasium.farama.org/tutorials/gymnasium_basics/handling_time_limits/#truncation\n            a dictionary with extra information:\n                ground truth correct response, info['gt']\n                boolean indicating the end of the trial, info['new_trial']\n        \"\"\"\n\n        return obs, reward, terminated, truncated, {'new_trial': new_trial, 'gt': gt}\n</code></pre>"},{"location":"project-license/","title":"License","text":"<p>Apache License</p> <p>Copyright 2020-2024, Manuel Molano, Guangyu Robert Yang, &amp; contributors</p> <p>Copyright 2024, Giulia Crocioni, Dani L. Bodor, The Netherlands eScience Center</p> <p>Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License.</p> <p>You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0.</p> <p>Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p>"},{"location":"api/core/","title":"Core","text":""},{"location":"api/core/#neurogym.core.TrialEnv","title":"TrialEnv","text":"<pre><code>TrialEnv(\n    dt=100, num_trials_before_reset=10000000, r_tmax=0\n)\n</code></pre> <p>               Bases: <code>BaseEnv</code></p> <p>The main Neurogym class for trial-based envs.</p> Source code in <code>neurogym/core.py</code> <pre><code>def __init__(self, dt=100, num_trials_before_reset=10000000, r_tmax=0) -&gt; None:\n    super().__init__(dt=dt)\n    self.r_tmax = r_tmax\n    self.num_tr = 0\n    self.num_tr_exp = num_trials_before_reset\n    self.trial: dict | None = None\n    self._ob_built = False\n    self._gt_built = False\n    self._has_gt = False  # check if the task ever defined gt\n\n    self._default_ob_value = None  # default to 0\n\n    # For optional periods\n    self.timing: dict = {}\n    self.start_t: dict = {}\n    self.end_t: dict = {}\n    self.start_ind: dict = {}\n    self.end_ind: dict = {}\n    self._tmax = 0  # Length of each trial\n\n    self._top = self\n    self._duration: dict = {}\n</code></pre>"},{"location":"api/core/#neurogym.core.TrialEnv.seed","title":"seed","text":"<pre><code>seed(seed=None)\n</code></pre> <p>Set random seed.</p> Source code in <code>neurogym/core.py</code> <pre><code>def seed(self, seed=None):\n    \"\"\"Set random seed.\"\"\"\n    self.rng = np.random.RandomState(seed)\n    if hasattr(self, \"action_space\") and self.action_space is not None:\n        self.action_space.seed(seed)\n    for val in self.timing.values():\n        with contextlib.suppress(AttributeError):\n            val.seed(seed)\n    return [seed]\n</code></pre>"},{"location":"api/core/#neurogym.core.TrialEnv.post_step","title":"post_step  <code>staticmethod</code>","text":"<pre><code>post_step(ob, reward, terminated, truncated, info)\n</code></pre> <p>Optional task-specific wrapper applied at the end of step.</p> <p>It allows to modify ob online (e.g. provide a specific observation for different actions made by the agent)</p> Source code in <code>neurogym/core.py</code> <pre><code>@staticmethod\ndef post_step(ob, reward, terminated, truncated, info):\n    \"\"\"Optional task-specific wrapper applied at the end of step.\n\n    It allows to modify ob online (e.g. provide a specific observation for different actions made by the agent)\n    \"\"\"\n    return ob, reward, terminated, truncated, info\n</code></pre>"},{"location":"api/core/#neurogym.core.TrialEnv.new_trial","title":"new_trial","text":"<pre><code>new_trial(**kwargs)\n</code></pre> <p>Public interface for starting a new trial.</p> <p>Returns:</p> Name Type Description <code>trial</code> <p>dict of trial information. Available to step function as self.trial</p> Source code in <code>neurogym/core.py</code> <pre><code>def new_trial(self, **kwargs):\n    \"\"\"Public interface for starting a new trial.\n\n    Returns:\n        trial: dict of trial information. Available to step function as\n            self.trial\n    \"\"\"\n    # Reset for next trial\n    self._tmax = 0  # reset, self.tmax not reset so it can be used in step\n    self._ob_built = False\n    self._gt_built = False\n    trial = self._new_trial(**kwargs)\n    self.trial = trial\n    self.num_tr += 1  # Increment trial count\n    self._has_gt = self._gt_built\n    return trial\n</code></pre>"},{"location":"api/core/#neurogym.core.TrialEnv.step","title":"step","text":"<pre><code>step(action)\n</code></pre> <p>Public interface for the environment.</p> Source code in <code>neurogym/core.py</code> <pre><code>def step(self, action):\n    \"\"\"Public interface for the environment.\"\"\"\n    ob, reward, terminated, truncated, info = self._step(action)\n\n    if \"new_trial\" not in info:\n        info[\"new_trial\"] = False\n\n    if self._has_gt and \"gt\" not in info:\n        # If gt is built, default gt to gt_now\n        # must run before incrementing t\n        info[\"gt\"] = self.gt_now\n\n    self.t += self.dt  # increment within trial time count\n    self.t_ind += 1\n\n    if self.t + self.dt &gt; self.tmax and not info[\"new_trial\"]:\n        info[\"new_trial\"] = True\n        reward += self.r_tmax\n\n    # TODO: new_trial happens after step, so trial indx precedes obs change\n    if info[\"new_trial\"]:\n        info[\"performance\"] = self.performance\n        self.t = self.t_ind = 0  # Reset within trial time count\n        trial = self._top.new_trial()\n        self.performance = 0\n        info[\"trial\"] = trial\n    if ob is OBNOW:\n        ob = self.ob[self.t_ind]\n    return self.post_step(ob, reward, terminated, truncated, info)\n</code></pre>"},{"location":"api/core/#neurogym.core.TrialEnv.reset","title":"reset","text":"<pre><code>reset(seed=None, options=None)\n</code></pre> <p>Reset the environment.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <p>random seed, overwrites self.seed if not None</p> <code>None</code> <code>options</code> <p>additional options used to reset the env. Can include 'step_fn' and 'no_step'. <code>step_fn</code> can be a function or None. If function, overwrite original <code>self.step</code> method. <code>no_step</code> is a bool. If True, no step is taken and observation randomly sampled. It defaults to False.</p> <code>None</code> Source code in <code>neurogym/core.py</code> <pre><code>def reset(self, seed=None, options=None):\n    \"\"\"Reset the environment.\n\n    Args:\n        seed: random seed, overwrites self.seed if not None\n        options: additional options used to reset the env.\n            Can include 'step_fn' and 'no_step'.\n            `step_fn` can be a function or None. If function, overwrite original\n            `self.step` method.\n            `no_step` is a bool. If True, no step is taken and observation randomly\n            sampled. It defaults to False.\n    \"\"\"\n    super().reset(seed=seed)\n\n    self.num_tr = 0\n    self.t = self.t_ind = 0\n\n    step_fn = options.get(\"step_fn\") if options else None\n    no_step = options.get(\"no_step\", False) if options else False\n\n    self._top.new_trial()\n\n    # have to also call step() to get the initial ob since some wrappers modify step() but not new_trial()\n    self.action_space.seed(0)\n    if no_step:\n        return self.observation_space.sample(), {}\n    if step_fn is None:\n        ob, _, _, _, _ = self._top.step(self.action_space.sample())\n    else:\n        ob, _, _, _, _ = step_fn(self.action_space.sample())\n    return ob, {}\n</code></pre>"},{"location":"api/core/#neurogym.core.TrialEnv.render","title":"render","text":"<pre><code>render(mode='human') -&gt; None\n</code></pre> <p>Plots relevant variables/parameters.</p> Source code in <code>neurogym/core.py</code> <pre><code>def render(self, mode=\"human\") -&gt; None:\n    \"\"\"Plots relevant variables/parameters.\"\"\"\n</code></pre>"},{"location":"api/core/#neurogym.core.TrialEnv.set_top","title":"set_top","text":"<pre><code>set_top(wrapper) -&gt; None\n</code></pre> <p>Set top to be wrapper.</p> Source code in <code>neurogym/core.py</code> <pre><code>def set_top(self, wrapper) -&gt; None:\n    \"\"\"Set top to be wrapper.\"\"\"\n    self._top = wrapper\n</code></pre>"},{"location":"api/core/#neurogym.core.TrialEnv.add_period","title":"add_period","text":"<pre><code>add_period(\n    period,\n    duration=None,\n    before=None,\n    after=None,\n    last_period=False,\n) -&gt; None\n</code></pre> <p>Add an period.</p> <p>Parameters:</p> Name Type Description Default <code>period</code> <p>string or list of strings, name of the period</p> required <code>duration</code> <p>float or None, duration of the period if None, inferred from timing_fn</p> <code>None</code> <code>before</code> <p>(optional) str, name of period that this period is before</p> <code>None</code> <code>after</code> <p>(optional) str, name of period that this period is after or float, time of period start</p> <code>None</code> <code>last_period</code> <p>bool, default False. If True, then this is last period will generate self.tmax, self.tind, and self.ob</p> <code>False</code> Source code in <code>neurogym/core.py</code> <pre><code>def add_period(\n    self,\n    period,\n    duration=None,\n    before=None,\n    after=None,\n    last_period=False,\n) -&gt; None:\n    \"\"\"Add an period.\n\n    Args:\n        period: string or list of strings, name of the period\n        duration: float or None, duration of the period\n            if None, inferred from timing_fn\n        before: (optional) str, name of period that this period is before\n        after: (optional) str, name of period that this period is after\n            or float, time of period start\n        last_period: bool, default False. If True, then this is last period\n            will generate self.tmax, self.tind, and self.ob\n    \"\"\"\n    if self._ob_built:\n        msg = \"Cannot add period after ob is built, i.e. after running add_ob.\"\n        raise InvalidOperationError(msg)\n    if isinstance(period, str):\n        pass\n    else:\n        if duration is None:\n            duration = [None] * len(period)\n        elif len(duration) != len(period):\n            msg = f\"{len(duration)=} and {len(period)=} must be the same.\"\n            raise ValueError(msg)\n\n        # Recursively calling itself\n        self.add_period(period[0], duration=duration[0], after=after)\n        for i in range(1, len(period)):\n            is_last = (i == len(period) - 1) and last_period\n            self.add_period(\n                period[i],\n                duration=duration[i],\n                after=period[i - 1],\n                last_period=is_last,\n            )\n        return\n\n    if duration is None:\n        duration = self.sample_time(period)\n    self._duration[period] = duration\n\n    if after is not None:\n        start = self.end_t[after] if isinstance(after, str) else after\n    elif before is not None:\n        start = self.start_t[before] - duration\n    else:\n        start = 0  # default start with 0\n\n    self.start_t[period] = start\n    self.end_t[period] = start + duration\n    self.start_ind[period] = int(start / self.dt)\n    self.end_ind[period] = int((start + duration) / self.dt)\n\n    self._tmax = max(self._tmax, start + duration)\n    self.tmax = int(self._tmax / self.dt) * self.dt\n</code></pre>"},{"location":"api/core/#neurogym.core.TrialEnv.view_ob","title":"view_ob","text":"<pre><code>view_ob(period=None)\n</code></pre> <p>View observation of an period.</p> Source code in <code>neurogym/core.py</code> <pre><code>def view_ob(self, period=None):\n    \"\"\"View observation of an period.\"\"\"\n    if not self._ob_built:\n        self._init_ob()\n\n    if period is None:\n        return self.ob\n    return self.ob[self.start_ind[period] : self.end_ind[period]]\n</code></pre>"},{"location":"api/core/#neurogym.core.TrialEnv.add_ob","title":"add_ob","text":"<pre><code>add_ob(value, period=None, where=None) -&gt; None\n</code></pre> <p>Add value to observation.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <p>array-like (ob_space.shape, ...)</p> required <code>period</code> <p>string, must be name of an added period</p> <code>None</code> <code>where</code> <p>string or np array, location of stimulus to be added</p> <code>None</code> Source code in <code>neurogym/core.py</code> <pre><code>def add_ob(self, value, period=None, where=None) -&gt; None:\n    \"\"\"Add value to observation.\n\n    Args:\n        value: array-like (ob_space.shape, ...)\n        period: string, must be name of an added period\n        where: string or np array, location of stimulus to be added\n    \"\"\"\n    self._add_ob(value, period, where, reset=False)\n</code></pre>"},{"location":"api/core/#neurogym.core.TrialEnv.set_groundtruth","title":"set_groundtruth","text":"<pre><code>set_groundtruth(value, period=None, where=None) -&gt; None\n</code></pre> <p>Set groundtruth value.</p> Source code in <code>neurogym/core.py</code> <pre><code>def set_groundtruth(self, value, period=None, where=None) -&gt; None:\n    \"\"\"Set groundtruth value.\"\"\"\n    if not self._gt_built:\n        self._init_gt()\n\n    if where is not None:\n        # TODO: Only works for Discrete action_space, make it work for Box\n        value = self.action_space.name[where][value]  # type: ignore[attr-defined]\n    if isinstance(period, str):\n        self.gt[self.start_ind[period] : self.end_ind[period]] = value\n    elif period is None:\n        self.gt[:] = value\n    else:\n        for p in period:\n            self.set_groundtruth(value, p)\n</code></pre>"},{"location":"api/core/#neurogym.core.TrialEnv.view_groundtruth","title":"view_groundtruth","text":"<pre><code>view_groundtruth(period)\n</code></pre> <p>View observation of an period.</p> Source code in <code>neurogym/core.py</code> <pre><code>def view_groundtruth(self, period):\n    \"\"\"View observation of an period.\"\"\"\n    if not self._gt_built:\n        self._init_gt()\n    return self.gt[self.start_ind[period] : self.end_ind[period]]\n</code></pre>"},{"location":"api/core/#neurogym.core.TrialEnv.in_period","title":"in_period","text":"<pre><code>in_period(period, t=None)\n</code></pre> <p>Check if current time or time t is in period.</p> Source code in <code>neurogym/core.py</code> <pre><code>def in_period(self, period, t=None):\n    \"\"\"Check if current time or time t is in period.\"\"\"\n    if t is None:\n        t = self.t  # Default\n    return self.start_t[period] &lt;= t &lt; self.end_t[period]\n</code></pre>"},{"location":"api/core/#neurogym.core.TrialEnv.trial_length_stats","title":"trial_length_stats","text":"<pre><code>trial_length_stats(num_trials: int = 10000) -&gt; dict\n</code></pre> <p>Calculate statistics about trial lengths.</p> <p>Parameters:</p> Name Type Description Default <code>num_trials</code> <code>int</code> <p>Number of trials to sample. Defaults to 10000.</p> <code>10000</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Contains 'mean', 'std', 'percentile_95', and 'max' as floats.</p> Source code in <code>neurogym/core.py</code> <pre><code>def trial_length_stats(self, num_trials: int = 10000) -&gt; dict:\n    \"\"\"Calculate statistics about trial lengths.\n\n    Args:\n        num_trials (int, optional): Number of trials to sample. Defaults to 10000.\n\n    Returns:\n        dict: Contains 'mean', 'std', 'percentile_95', and 'max' as floats.\n    \"\"\"\n    # For environments with very simple timing (all fixed durations),\n    # we can calculate exactly without sampling\n    if self.timing and all(isinstance(timing, int | float) for timing in self.timing.values()):\n        fixed_length = int(sum(self.timing.values()) / self.dt)\n        return {\n            \"mean\": fixed_length,\n            \"std\": 0,\n            \"percentile_95\": fixed_length,\n            \"max\": fixed_length,\n        }\n\n    # For more complex environments, we sample trials\n    trial_lengths_list = []\n\n    # Store current RNG state to restore later\n    rng_state = self.rng.get_state()\n\n    # Sample trials\n    for _ in range(num_trials):\n        self.new_trial()\n        if hasattr(self, \"ob\") and self.ob is not None:\n            trial_lengths_list.append(self.ob.shape[0])\n\n    # Restore RNG state\n    self.rng.set_state(rng_state)\n\n    # Calculate statistics from sampled trials\n    if len(trial_lengths_list) == 0:\n        warnings.warn(\"No trials were sampled. Returning default values.\", stacklevel=2)\n        return {\n            \"mean\": 0,\n            \"std\": 0,\n            \"percentile_95\": 0,\n            \"max\": 0,\n        }\n    trial_lengths = np.array(trial_lengths_list)\n    return {\n        \"mean\": round(np.mean(trial_lengths), 3),\n        \"std\": round(np.std(trial_lengths), 3),\n        \"percentile_95\": round(np.percentile(trial_lengths, 95), 3),\n        \"max\": round(np.max(trial_lengths), 3),\n    }\n</code></pre>"},{"location":"api/core/#neurogym.core.BaseEnv","title":"BaseEnv","text":"<pre><code>BaseEnv(dt=100)\n</code></pre> <p>               Bases: <code>Env</code></p> <p>The base Neurogym class to include dt.</p> Source code in <code>neurogym/core.py</code> <pre><code>def __init__(self, dt=100) -&gt; None:\n    super().__init__()\n    self.dt = dt\n    self.t = self.t_ind = 0\n    self.tmax = 10000  # maximum time steps\n    self.performance = 0\n    self.rewards: dict = {}\n    self.rng = np.random.RandomState()\n</code></pre>"},{"location":"api/core/#neurogym.core.BaseEnv.seed","title":"seed","text":"<pre><code>seed(seed=None)\n</code></pre> <p>Set random seed.</p> Source code in <code>neurogym/core.py</code> <pre><code>def seed(self, seed=None):\n    \"\"\"Set random seed.\"\"\"\n    self.rng = np.random.RandomState(seed)\n    if self.action_space is not None:\n        self.action_space.seed(seed)\n    return [seed]\n</code></pre>"},{"location":"api/envs/","title":"Environments","text":""},{"location":"api/envs/#neurogym.envs.registration","title":"registration","text":""},{"location":"api/envs/#neurogym.envs.registration.all_envs","title":"all_envs","text":"<pre><code>all_envs(\n    tag: str | None = None,\n    psychopy: bool = False,\n    contrib: bool = False,\n    collections: bool = False,\n) -&gt; list[str]\n</code></pre> <p>Return a list of all envs in neurogym.</p> Source code in <code>neurogym/envs/registration.py</code> <pre><code>def all_envs(\n    tag: str | None = None,\n    psychopy: bool = False,\n    contrib: bool = False,\n    collections: bool = False,\n) -&gt; list[str]:\n    \"\"\"Return a list of all envs in neurogym.\"\"\"\n    envs = ALL_NATIVE_ENVS.copy()\n    if psychopy:\n        envs.update(ALL_PSYCHOPY_ENVS)\n    if contrib:\n        envs.update(ALL_CONTRIB_ENVS)\n    if collections:\n        envs.update(ALL_COLLECTIONS_ENVS)\n    env_list = sorted(envs.keys())\n    if tag is None:\n        return env_list\n    if not isinstance(tag, str):\n        msg = f\"{type(tag)=} must be a string.\"\n        raise TypeError(msg)\n\n    new_env_list: list[str] = []\n    for env in env_list:\n        from_, class_ = envs[env].split(\":\")\n        imported = getattr(__import__(from_, fromlist=[class_]), class_)\n        env_tag = imported.metadata.get(\"tags\", [])\n        if tag in env_tag:\n            new_env_list.append(env)\n    return new_env_list\n</code></pre>"},{"location":"api/envs/#neurogym.envs.registration.make","title":"make","text":"<pre><code>make(id_: str, **kwargs) -&gt; Env\n</code></pre> <p>Creates an environment previously registered with :meth:<code>ngym.register</code>.</p> <p>This function calls the Gymnasium <code>make</code> function with the <code>disable_env_checker</code> argument set to True.</p> <p>Parameters:</p> Name Type Description Default <code>id_</code> <code>str</code> <p>A string representing the environment ID.</p> required <code>kwargs</code> <p>Additional arguments to pass to the environment constructor.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Env</code> <p>An instance of the environment.</p> Source code in <code>neurogym/envs/registration.py</code> <pre><code>def make(id_: str, **kwargs) -&gt; gym.Env:\n    \"\"\"Creates an environment previously registered with :meth:`ngym.register`.\n\n    This function calls the Gymnasium `make` function with the `disable_env_checker` argument set to True.\n\n    Args:\n        id_: A string representing the environment ID.\n        kwargs: Additional arguments to pass to the environment constructor.\n\n    Returns:\n        An instance of the environment.\n    \"\"\"\n    # built in env_checker raises warnings or errors when ob not in observation_space\n    return gym.make(id_, disable_env_checker=True, **kwargs)\n</code></pre>"},{"location":"api/envs/#neurogym.envs.native.annubes","title":"annubes","text":""},{"location":"api/envs/#neurogym.envs.native.annubes.AnnubesEnv","title":"AnnubesEnv","text":"<pre><code>AnnubesEnv(\n    session: dict[str, float] | None = None,\n    stim_intensities: list[float] | None = None,\n    stim_time: int = 1000,\n    catch_prob: float = 0.5,\n    max_sequential: int | None = None,\n    fix_intensity: float = 0,\n    fix_time: Any = 500,\n    iti: Any = 0,\n    dt: int = 100,\n    tau: int = 100,\n    output_behavior: list[float] | None = None,\n    noise_std: float = 0.01,\n    rewards: dict[str, float] | None = None,\n    random_seed: int | None = None,\n)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>General class for the Annubes type of tasks.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>dict[str, float] | None</code> <p>Configuration of the trials that can appear during a session. It is given by a dictionary representing the ratio (values) of the different trials (keys) within the task. Trials with a single modality (e.g., a visual trial) must be represented by single characters, while trials with multiple modalities (e.g., an audiovisual trial) are represented by the character combination of those trials. Note that values are read relative to each other, such that e.g. <code>{\"v\": 0.25, \"a\": 0.75}</code> is equivalent to <code>{\"v\": 1, \"a\": 3}</code>. Defaults to {\"v\": 0.5, \"a\": 0.5}.</p> <code>None</code> <code>stim_intensities</code> <code>list[float] | None</code> <p>List of possible intensity values of each stimulus, when the stimulus is present. Note that when the stimulus is not present, the intensity is set to 0. Defaults to [0.8, 0.9, 1].</p> <code>None</code> <code>stim_time</code> <code>int</code> <p>Duration of each stimulus in ms. Defaults to 1000.</p> <code>1000</code> <code>catch_prob</code> <code>float</code> <p>Probability of catch trials in the session. Must be between 0 and 1 (inclusive). Defaults to 0.5.</p> <code>0.5</code> <code>max_sequential</code> <code>int | None</code> <p>Maximum number of sequential trials of the same modality. It applies only to the modalities defined in <code>session</code>, i.e., it does not apply to catch trials. Defaults to None (no maximum).</p> <code>None</code> <code>fix_intensity</code> <code>float</code> <p>Intensity of input signal during fixation. Defaults to 0.</p> <code>0</code> <code>fix_time</code> <code>Any</code> <p>Fixation time specification. Can be one of the following: - A number (int or float): Fixed duration in milliseconds. - A callable: Function that returns the duration when called. - A list of numbers: Random choice from the list. - A tuple specifying a distribution:     - (\"uniform\", (min, max)): Uniform distribution between min and max.     - (\"choice\", [options]): Random choice from the given options.     - (\"truncated_exponential\", [parameters]): Truncated exponential distribution.     - (\"constant\", value): Always returns the given value.     - (\"until\", end_time): Sets duration to reach the specified end time. The final duration is rounded down to the nearest multiple of the simulation timestep (dt). Note that the duration of each input and output signal is increased by this time. Defaults to 500.</p> <code>500</code> <code>iti</code> <code>Any</code> <p>Inter-trial interval, or time window between sequential trials, in ms. Same format as <code>fix_time</code>. Defaults to 0.</p> <code>0</code> <code>dt</code> <code>int</code> <p>Time step in ms. Defaults to 100.</p> <code>100</code> <code>tau</code> <code>int</code> <p>Time constant in ms. Defaults to 100.</p> <code>100</code> <code>output_behavior</code> <code>list[float] | None</code> <p>List of possible intensity values of the behavioral output. Currently only the smallest and largest value of this list are used. Defaults to [0, 1].</p> <code>None</code> <code>noise_std</code> <code>float</code> <p>Standard deviation of the input noise. Defaults to 0.01.</p> <code>0.01</code> <code>rewards</code> <code>dict[str, float] | None</code> <p>Dictionary of rewards for different outcomes. The keys are \"abort\", \"correct\", and \"fail\". Defaults to {\"abort\": -0.1, \"correct\": +1.0, \"fail\": 0.0}.</p> <code>None</code> <code>random_seed</code> <code>int | None</code> <p>Seed for numpy's random number generator (rng). If an int is given, it will be used as the seed for <code>np.random.default_rng()</code>. Defaults to None (i.e. the initial state itself is random).</p> <code>None</code> Source code in <code>neurogym/envs/native/annubes.py</code> <pre><code>def __init__(\n    self,\n    session: dict[str, float] | None = None,\n    stim_intensities: list[float] | None = None,\n    stim_time: int = 1000,\n    catch_prob: float = 0.5,\n    max_sequential: int | None = None,\n    fix_intensity: float = 0,\n    fix_time: Any = 500,\n    iti: Any = 0,\n    dt: int = 100,\n    tau: int = 100,\n    output_behavior: list[float] | None = None,\n    noise_std: float = 0.01,\n    rewards: dict[str, float] | None = None,\n    random_seed: int | None = None,\n):\n    if session is None:\n        session = {\"v\": 0.5, \"a\": 0.5}\n    if output_behavior is None:\n        output_behavior = [0, 1]\n    if stim_intensities is None:\n        stim_intensities = [0.8, 0.9, 1.0]\n    if session is None:\n        session = {\"v\": 0.5, \"a\": 0.5}\n    super().__init__(dt=dt)\n    self.session = {i: session[i] / sum(session.values()) for i in session}\n    self.stim_intensities = stim_intensities\n    self.stim_time = stim_time\n    self.catch_prob = catch_prob\n    self.max_sequential = max_sequential\n    self.sequential_count = 1\n    self.last_modality: str | None = None\n    self.fix_intensity = fix_intensity\n    self.fix_time = fix_time\n    self.iti = iti\n    self.dt = dt\n    self.tau = tau\n    self.output_behavior = output_behavior\n    self.noise_std = noise_std\n    self.random_seed = random_seed\n    alpha = dt / self.tau\n    self.noise_factor = self.noise_std * np.sqrt(2 * alpha) / alpha\n    # Set random state\n    if random_seed is None:\n        rng = np.random.default_rng(random_seed)\n        self._random_seed = rng.integers(2**32)\n    else:\n        self._random_seed = random_seed\n    self._rng = np.random.default_rng(self._random_seed)\n    # Rewards\n    if rewards is None:\n        self.rewards = {\"abort\": -0.1, \"correct\": +1.0, \"fail\": 0.0}\n    else:\n        self.rewards = rewards\n    self.timing = {\"fixation\": self.fix_time, \"stimulus\": self.stim_time, \"iti\": self.iti}\n    # Set the name of each input dimension\n    obs_space_name = {\"fixation\": 0, \"start\": 1, **{trial: i for i, trial in enumerate(session, 2)}}\n    self.observation_space = ngym.spaces.Box(low=0.0, high=1.0, shape=(len(obs_space_name),), name=obs_space_name)\n    # Set the name of each action value\n    self.action_space = ngym.spaces.Discrete(\n        n=len(self.output_behavior),\n        name={\"fixation\": self.fix_intensity, \"choice\": self.output_behavior[1:]},\n    )\n</code></pre>"},{"location":"api/envs/#neurogym.envs.native.antireach","title":"antireach","text":"<p>Anti-reach or anti-saccade task.</p>"},{"location":"api/envs/#neurogym.envs.native.antireach.AntiReach","title":"AntiReach","text":"<pre><code>AntiReach(\n    dt=100,\n    anti=True,\n    rewards=None,\n    timing=None,\n    dim_ring=32,\n)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>Anti-response task.</p> <p>During the fixation period, the agent fixates on a fixation point. During the following stimulus period, the agent is then shown a stimulus away from the fixation point. Finally, the agent needs to respond in the opposite direction of the stimulus during the decision period.</p> <p>Parameters:</p> Name Type Description Default <code>anti</code> <p>bool, if True, requires an anti-response. If False, requires a pro-response, i.e. response towards the stimulus.</p> <code>True</code> Source code in <code>neurogym/envs/native/antireach.py</code> <pre><code>def __init__(self, dt=100, anti=True, rewards=None, timing=None, dim_ring=32) -&gt; None:\n    super().__init__(dt=dt)\n\n    self.anti = anti\n\n    # Rewards\n    self.rewards = {\"abort\": -0.1, \"correct\": +1.0, \"fail\": 0.0}\n    if rewards:\n        self.rewards.update(rewards)\n\n    self.timing = {\"fixation\": 500, \"stimulus\": 500, \"delay\": 0, \"decision\": 500}\n    if timing:\n        self.timing.update(timing)\n\n    self.abort = False\n\n    # action and observation spaces\n    self.dim_ring = dim_ring\n    self.theta = np.arange(0, 2 * np.pi, 2 * np.pi / dim_ring)\n    self.choices = np.arange(dim_ring)\n\n    name = {\"fixation\": 0, \"stimulus\": range(1, dim_ring + 1)}\n    self.observation_space = spaces.Box(\n        -np.inf,\n        np.inf,\n        shape=(1 + dim_ring,),\n        dtype=np.float32,\n        name=name,\n    )\n\n    name = {\"fixation\": 0, \"choice\": range(1, dim_ring + 1)}\n    self.action_space = spaces.Discrete(1 + dim_ring, name=name)\n</code></pre>"},{"location":"api/envs/#neurogym.envs.native.bandit","title":"bandit","text":"<p>Multi-arm Bandit task.</p>"},{"location":"api/envs/#neurogym.envs.native.bandit.Bandit","title":"Bandit","text":"<pre><code>Bandit(\n    dt: int = 100,\n    n: int = 2,\n    p: tuple[float, ...] | list[float] = (0.5, 0.5),\n    rewards: list[float] | ndarray | None = None,\n    timing: dict | None = None,\n)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>Multi-arm bandit task.</p> <p>On each trial, the agent is presented with multiple choices. Each option produces a reward of a certain magnitude given a certain probability.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>int, the number of choices (arms)</p> <code>2</code> <code>p</code> <code>tuple[float, ...] | list[float]</code> <p>tuple of length n, describes the probability of each arm leading to reward</p> <code>(0.5, 0.5)</code> <code>rewards</code> <code>list[float] | ndarray | None</code> <p>tuple of length n, describe the reward magnitude of each option when rewarded</p> <code>None</code> Source code in <code>neurogym/envs/native/bandit.py</code> <pre><code>def __init__(\n    self,\n    dt: int = 100,\n    n: int = 2,\n    p: tuple[float, ...] | list[float] = (0.5, 0.5),\n    rewards: list[float] | np.ndarray | None = None,\n    timing: dict | None = None,\n) -&gt; None:\n    super().__init__(dt=dt)\n    if timing is not None:\n        logger.warning(\"Bandit task does not require timing variable.\")\n\n    self.n = n\n    self._p = np.array(p)  # Reward probabilities\n\n    if rewards is not None:\n        self._rewards = np.array(rewards)\n    else:\n        self._rewards = np.ones(n)  # 1 for every arm\n\n    self.observation_space = spaces.Box(\n        -np.inf,\n        np.inf,\n        shape=(1,),\n        dtype=np.float32,\n    )\n    self.action_space = spaces.Discrete(n)\n</code></pre>"},{"location":"api/envs/#neurogym.envs.native.contextdecisionmaking","title":"contextdecisionmaking","text":""},{"location":"api/envs/#neurogym.envs.native.contextdecisionmaking.ContextDecisionMaking","title":"ContextDecisionMaking","text":"<pre><code>ContextDecisionMaking(\n    dt: int = 100,\n    use_expl_context: bool = False,\n    impl_context_modality: int = 0,\n    dim_ring: int = 2,\n    rewards: dict[str, float] | None = None,\n    timing: dict[str, int | TruncExp] | None = None,\n    sigma: float = 1.0,\n    abort: bool = False,\n)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>Context-dependent decision-making task.</p> <p>The agent simultaneously receives stimulus inputs from two modalities ( for example, a colored random dot motion pattern with color and motion modalities) and needs to make a perceptual decision based on one while ignoring the other. The task can operate in two modes: 1. Implicit context: The relevant modality is fixed and must be learned (    it is not explicitly signaled). 2. Explicit context: The agent simultaneously receives stimulus inputs    from two modalities, and the relevant modality is explicitly indicated    by a context signal. Both modes use ring representation for encoding stimulus inputs and choices.</p> <p>Parameters:</p> Name Type Description Default <code>dt</code> <code>int</code> <p>Timestep of the environment in milliseconds.</p> <code>100</code> <code>use_expl_context</code> <code>bool</code> <p>If True, the context is explicit (signaled) and changes per trial. If False, a fixed context is used throughout (set by <code>impl_context_modality</code>).</p> <code>False</code> <code>impl_context_modality</code> <code>int</code> <p>The fixed implicit modality to use if <code>use_expl_context</code> is False (0 or 1).</p> <code>0</code> <code>dim_ring</code> <code>int</code> <p>Number of stimulus locations (or choices).</p> <code>2</code> <code>rewards</code> <code>dict[str, float] | None</code> <p>Optional dictionary to override default rewards. The required keys are \"abort\" and \"correct\". Defaults to {\"abort\": -0.1, \"correct\": +1.0}.</p> <code>None</code> <code>timing</code> <code>dict[str, int | TruncExp] | None</code> <p>Optional dictionary to override default durations of task periods. The expected keys are \"fixation\", \"stimulus\" (required), \"delay\", and \"decision\" (required). Defaults to {\"fixation\": 300, \"stimulus\": 750, \"delay\": TruncExp(600, 300, 3000), \"decision\": 100}.</p> <code>None</code> <code>sigma</code> <code>float</code> <p>Standard deviation of Gaussian noise added to stimulus.</p> <code>1.0</code> <code>abort</code> <code>bool</code> <p>If True, incorrect actions during fixation abort the trial.</p> <code>False</code> Source code in <code>neurogym/envs/native/contextdecisionmaking.py</code> <pre><code>def __init__(\n    self,\n    dt: int = 100,\n    use_expl_context: bool = False,\n    impl_context_modality: int = 0,\n    dim_ring: int = 2,\n    rewards: dict[str, float] | None = None,\n    timing: dict[str, int | TruncExp] | None = None,\n    sigma: float = 1.0,\n    abort: bool = False,\n) -&gt; None:\n    super().__init__(dt=dt)\n\n    # Task parameters\n    self.dt = dt\n    self.use_expl_context = use_expl_context\n    self.dim_ring = dim_ring\n    self.sigma = sigma / np.sqrt(self.dt)\n\n    # Trial conditions and spaces setup\n    self.cohs: list[int] = [5, 15, 50]\n    if use_expl_context:\n        self.contexts: list[int] = [0, 1]\n    else:\n        if impl_context_modality not in [0, 1]:\n            msg = \"impl_context_modality must be 0 or 1 when `use_expl_context` is False.\"\n            raise ValueError(msg)\n        self.contexts = [impl_context_modality]\n    self._setup_spaces()\n\n    # Rewards\n    self.rewards: dict[str, float] = {\"abort\": -0.1, \"correct\": +1.0}\n    if rewards:\n        self.rewards.update(rewards)\n\n    # Timing\n    self.timing: dict[str, int | TruncExp] = {\n        \"fixation\": 300,\n        \"stimulus\": 750,\n        \"delay\": TruncExp(600, 300, 3000),\n        \"decision\": 100,\n    }\n    if timing:\n        self.timing.update(timing)\n\n    self.abort = abort\n</code></pre>"},{"location":"api/envs/#neurogym.envs.native.dawtwostep","title":"dawtwostep","text":""},{"location":"api/envs/#neurogym.envs.native.dawtwostep.DawTwoStep","title":"DawTwoStep","text":"<pre><code>DawTwoStep(dt=100, rewards=None, timing=None)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>Daw Two-step task.</p> <p>On each trial, an initial choice between two options lead to either of two, second-stage states. In turn, these both demand another two-option choice, each of which is associated with a different chance of receiving reward.</p> Source code in <code>neurogym/envs/native/dawtwostep.py</code> <pre><code>def __init__(self, dt=100, rewards=None, timing=None) -&gt; None:\n    super().__init__(dt=dt)\n    if timing is not None:\n        logger.warning(\"Daw Two-step task does not require timing variable.\")\n    # Actions are ('FIXATE', 'ACTION1', 'ACTION2')\n    self.actions = [0, 1, 2]\n\n    # trial conditions\n    self.p1 = 0.8  # prob of transitioning to state1 with action1 (&gt;=05)\n    self.p2 = 0.8  # prob of transitioning to state2 with action2 (&gt;=05)\n    self.p_switch = 0.025  # switch reward contingency\n    self.high_reward_p = 0.9\n    self.low_reward_p = 0.1\n    self.tmax = 3 * self.dt\n    self.mean_trial_duration = self.tmax\n    self.state1_high_reward = True\n    # Rewards\n    self.rewards = {\"abort\": -0.1, \"correct\": +1.0}\n    if rewards:\n        self.rewards.update(rewards)\n\n    self.action_space = spaces.Discrete(3)\n    self.observation_space = spaces.Box(\n        -np.inf,\n        np.inf,\n        shape=(3,),\n        dtype=np.float32,\n    )\n</code></pre>"},{"location":"api/envs/#neurogym.envs.native.delaycomparison","title":"delaycomparison","text":""},{"location":"api/envs/#neurogym.envs.native.delaycomparison.DelayComparison","title":"DelayComparison","text":"<pre><code>DelayComparison(\n    dt=100,\n    vpairs=None,\n    rewards=None,\n    timing=None,\n    sigma=1.0,\n)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>Delayed comparison.</p> <p>The agent needs to compare the magnitude of two stimuli are separated by a delay period. The agent reports its decision of the stronger stimulus during the decision period.</p> Source code in <code>neurogym/envs/native/delaycomparison.py</code> <pre><code>def __init__(self, dt=100, vpairs=None, rewards=None, timing=None, sigma=1.0) -&gt; None:\n    super().__init__(dt=dt)\n\n    # Pair of stimulus strengthes\n    if vpairs is None:\n        self.vpairs = [(18, 10), (22, 14), (26, 18), (30, 22), (34, 26)]\n    else:\n        self.vpairs = vpairs\n\n    self.sigma = sigma / np.sqrt(self.dt)  # Input noise\n\n    # Rewards\n    self.rewards = {\"abort\": -0.1, \"correct\": +1.0, \"fail\": 0.0}\n    if rewards:\n        self.rewards.update(rewards)\n\n    self.timing = {\n        \"fixation\": 500,\n        \"stimulus1\": 500,\n        \"delay\": 1000,\n        \"stimulus2\": 500,\n        \"decision\": 100,\n    }\n    if timing:\n        self.timing.update(timing)\n\n    self.abort = False\n\n    # Input scaling\n    self.vall = np.ravel(self.vpairs)\n    self.vmin = np.min(self.vall)\n    self.vmax = np.max(self.vall)\n\n    # action and observation space\n    name: dict[str, int | list] = {\"fixation\": 0, \"stimulus\": 1}\n    self.observation_space = spaces.Box(\n        -np.inf,\n        np.inf,\n        shape=(2,),\n        dtype=np.float32,\n        name=name,\n    )\n    name = {\"fixation\": 0, \"choice\": [1, 2]}\n    self.action_space = spaces.Discrete(3, name=name)\n\n    self.choices = [1, 2]\n</code></pre>"},{"location":"api/envs/#neurogym.envs.native.delaycomparison.DelayComparison.represent","title":"represent","text":"<pre><code>represent(v)\n</code></pre> <p>Input representation of stimulus value.</p> Source code in <code>neurogym/envs/native/delaycomparison.py</code> <pre><code>def represent(self, v):\n    \"\"\"Input representation of stimulus value.\"\"\"\n    # Scale to be between 0 and 1\n    v_ = (v - self.vmin) / (self.vmax - self.vmin)\n    # positive encoding, between 0.5 and 1\n    return (1 + v_) / 2\n</code></pre>"},{"location":"api/envs/#neurogym.envs.native.delaymatchcategory","title":"delaymatchcategory","text":""},{"location":"api/envs/#neurogym.envs.native.delaymatchcategory.DelayMatchCategory","title":"DelayMatchCategory","text":"<pre><code>DelayMatchCategory(\n    dt=100, rewards=None, timing=None, sigma=1.0, dim_ring=2\n)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>Delayed match-to-category task.</p> <p>A sample stimulus is shown during the sample period. The stimulus is characterized by a one-dimensional variable, such as its orientation between 0 and 360 degree. This one-dimensional variable is separated into two categories (for example, 0-180 degree and 180-360 degree). After a delay period, a test stimulus is shown. The agent needs to determine whether the sample and the test stimuli belong to the same category, and report that decision during the decision period.</p> Source code in <code>neurogym/envs/native/delaymatchcategory.py</code> <pre><code>def __init__(self, dt=100, rewards=None, timing=None, sigma=1.0, dim_ring=2) -&gt; None:\n    super().__init__(dt=dt)\n    self.choices = [\"match\", \"non-match\"]  # match, non-match\n\n    self.sigma = sigma / np.sqrt(self.dt)  # Input noise\n\n    # Rewards\n    self.rewards = {\"abort\": -0.1, \"correct\": +1.0, \"fail\": 0.0}\n    if rewards:\n        self.rewards.update(rewards)\n\n    self.timing = {\"fixation\": 500, \"sample\": 650, \"first_delay\": 1000, \"test\": 650}\n\n    if timing:\n        self.timing.update(timing)\n\n    self.abort = False\n\n    self.theta = np.linspace(0, 2 * np.pi, dim_ring + 1)[:-1]\n\n    name = {\"fixation\": 0, \"stimulus\": range(1, dim_ring + 1)}\n    self.observation_space = spaces.Box(\n        -np.inf,\n        np.inf,\n        shape=(1 + dim_ring,),\n        dtype=np.float32,\n        name=name,\n    )\n\n    name = {\"fixation\": 0, \"match\": 1, \"non-match\": 2}\n    self.action_space = spaces.Discrete(3, name=name)\n</code></pre>"},{"location":"api/envs/#neurogym.envs.native.delaymatchsample","title":"delaymatchsample","text":""},{"location":"api/envs/#neurogym.envs.native.delaymatchsample.DelayMatchSample","title":"DelayMatchSample","text":"<pre><code>DelayMatchSample(\n    dt=100, rewards=None, timing=None, sigma=1.0, dim_ring=2\n)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>Delayed match-to-sample task.</p> <p>A sample stimulus is shown during the sample period. The stimulus is characterized by a one-dimensional variable, such as its orientation between 0 and 360 degree. After a delay period, a test stimulus is shown. The agent needs to determine whether the sample and the test stimuli are equal, and report that decision during the decision period.</p> Source code in <code>neurogym/envs/native/delaymatchsample.py</code> <pre><code>def __init__(self, dt=100, rewards=None, timing=None, sigma=1.0, dim_ring=2) -&gt; None:\n    super().__init__(dt=dt)\n    self.choices = [1, 2]\n    self.sigma = sigma / np.sqrt(self.dt)  # Input noise\n\n    # Rewards\n    self.rewards = {\"abort\": -0.1, \"correct\": +1.0, \"fail\": 0.0}\n    if rewards:\n        self.rewards.update(rewards)\n\n    self.timing = {\n        \"fixation\": 300,\n        \"sample\": 500,\n        \"delay\": 1000,\n        \"test\": 500,\n        \"decision\": 900,\n    }\n    if timing:\n        self.timing.update(timing)\n\n    self.abort = False\n\n    self.theta = np.linspace(0, 2 * np.pi, dim_ring + 1)[:-1]\n\n    name = {\"fixation\": 0, \"stimulus\": range(1, dim_ring + 1)}\n    self.observation_space = spaces.Box(\n        -np.inf,\n        np.inf,\n        shape=(1 + dim_ring,),\n        dtype=np.float32,\n        name=name,\n    )\n\n    name = {\"fixation\": 0, \"match\": 1, \"non-match\": 2}\n    self.action_space = spaces.Discrete(3, name=name)\n</code></pre>"},{"location":"api/envs/#neurogym.envs.native.delaymatchsample.DelayMatchSampleDistractor1D","title":"DelayMatchSampleDistractor1D","text":"<pre><code>DelayMatchSampleDistractor1D(\n    dt=100, rewards=None, timing=None, sigma=1.0\n)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>Delayed match-to-sample with multiple, potentially repeating distractors.</p> <p>A sample stimulus is shown during the sample period. The stimulus is characterized by a one-dimensional variable, such as its orientation between 0 and 360 degree. After a delay period, the first test stimulus is shown. The agent needs to determine whether the sample and this test stimuli are equal. If so, it needs to produce the match response. If the first test is not equal to the sample stimulus, another delay period and then a second test stimulus follow, and so on.</p> Source code in <code>neurogym/envs/native/delaymatchsample.py</code> <pre><code>def __init__(self, dt=100, rewards=None, timing=None, sigma=1.0) -&gt; None:\n    super().__init__(dt=dt)\n    self.choices = [1, 2, 3]\n    self.sigma = sigma / np.sqrt(self.dt)  # Input noise\n\n    # Rewards\n    self.rewards = {\"abort\": -0.1, \"correct\": +1.0, \"fail\": -1.0}\n    if rewards:\n        self.rewards.update(rewards)\n\n    self.timing = {\n        \"fixation\": 300,\n        \"sample\": 500,\n        \"delay1\": 1000,\n        \"test1\": 500,\n        \"delay2\": 1000,\n        \"test2\": 500,\n        \"delay3\": 1000,\n        \"test3\": 500,\n    }\n    if timing:\n        self.timing.update(timing)\n\n    self.abort = False\n\n    self.theta = np.arange(0, 2 * np.pi, 2 * np.pi / 32)\n\n    name = {\"fixation\": 0, \"stimulus\": range(1, 33)}\n    self.observation_space = spaces.Box(\n        -np.inf,\n        np.inf,\n        shape=(33,),\n        dtype=np.float32,\n        name=name,\n    )\n\n    name = {\"fixation\": 0, \"match\": 1}\n    self.action_space = spaces.Discrete(2, name=name)\n</code></pre>"},{"location":"api/envs/#neurogym.envs.native.delaypairedassociation","title":"delaypairedassociation","text":""},{"location":"api/envs/#neurogym.envs.native.delaypairedassociation.DelayPairedAssociation","title":"DelayPairedAssociation","text":"<pre><code>DelayPairedAssociation(\n    dt=100, rewards=None, timing=None, sigma=1.0\n)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>Delayed paired-association task.</p> <p>The agent is shown a pair of two stimuli separated by a delay period. For half of the stimuli-pairs shown, the agent should choose the Go response. The agent is rewarded if it chose the Go response correctly.</p> Source code in <code>neurogym/envs/native/delaypairedassociation.py</code> <pre><code>def __init__(self, dt=100, rewards=None, timing=None, sigma=1.0) -&gt; None:\n    super().__init__(dt=dt)\n    self.choices = [0, 1]\n    # trial conditions\n    self.pairs = [(1, 3), (1, 4), (2, 3), (2, 4)]\n    self.association = 0  # GO if np.diff(self.pair)[0]%2==self.association\n    self.sigma = sigma / np.sqrt(self.dt)  # Input noise\n    # Durations (stimulus duration will be drawn from an exponential)\n\n    # Rewards\n    self.rewards = {\"abort\": -0.1, \"correct\": +1.0, \"fail\": -1.0, \"miss\": 0.0}\n    if rewards:\n        self.rewards.update(rewards)\n\n    self.timing = {\n        \"fixation\": 0,\n        \"stim1\": 1000,\n        \"delay_btw_stim\": 1000,\n        \"stim2\": 1000,\n        \"delay_aft_stim\": 1000,\n        \"decision\": 500,\n    }\n    if timing:\n        self.timing.update(timing)\n\n    self.abort = False\n    # action and observation spaces\n    name = {\"fixation\": 0, \"stimulus\": range(1, 5)}\n    self.observation_space = spaces.Box(\n        -np.inf,\n        np.inf,\n        shape=(5,),\n        dtype=np.float32,\n        name=name,\n    )\n\n    self.action_space = spaces.Discrete(2, name={\"fixation\": 0, \"go\": 1})\n</code></pre>"},{"location":"api/envs/#neurogym.envs.native.detection","title":"detection","text":"<p>Created on Mon Jan 27 11:00:26 2020.</p> <p>@author: martafradera</p>"},{"location":"api/envs/#neurogym.envs.native.detection.Detection","title":"Detection","text":"<pre><code>Detection(\n    dt=100,\n    rewards=None,\n    timing=None,\n    sigma=1.0,\n    delay=None,\n    stim_dur=100,\n)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>The agent has to GO if a stimulus is presented.</p> <p>Parameters:</p> Name Type Description Default <code>delay</code> <p>If not None indicates the delay, from the moment of the start of the stimulus period when the actual stimulus is presented. Otherwise, the delay is drawn from a uniform distribution. (def: None (ms), int)</p> <code>None</code> <code>stim_dur</code> <p>Stimulus duration. (def: 100 (ms), int)</p> <code>100</code> Source code in <code>neurogym/envs/native/detection.py</code> <pre><code>def __init__(\n    self,\n    dt=100,\n    rewards=None,\n    timing=None,\n    sigma=1.0,\n    delay=None,\n    stim_dur=100,\n) -&gt; None:\n    super().__init__(dt=dt)\n    # Possible decisions at the end of the trial\n    self.choices = [0, 1]\n\n    self.sigma = sigma / np.sqrt(self.dt)  # Input noise\n    self.delay = delay\n    self.stim_dur = int(stim_dur / self.dt)  # in steps should be greater\n    # than 1 stp else it wont have enough time to respond within the window\n    if self.stim_dur == 1:\n        self.extra_step = 1\n        if delay is None:\n            warnings.warn(\n                \"Added an extra stp after the actual stimulus, else model will not be able to respond \"\n                \"within response window (stimulus epoch).\",\n                UserWarning,\n                stacklevel=2,\n            )\n    else:\n        self.extra_step = 0\n\n    if self.stim_dur &lt; 1:\n        warnings.warn(\"Stimulus duration shorter than dt\", stacklevel=2)\n\n    # Rewards\n    self.rewards = {\"abort\": -0.1, \"correct\": +1.0, \"fail\": -1.0, \"miss\": -1}\n    if rewards:\n        self.rewards.update(rewards)\n\n    self.timing = {\n        \"fixation\": 500,\n        \"stimulus\": TruncExp(1000, 500, 1500),\n    }\n    if timing:\n        self.timing.update(timing)\n\n    # whether to abort (T) or not (F) the trial when breaking fixation:\n    self.abort = False\n\n    name = {\"fixation\": 0, \"stimulus\": 1}\n    self.observation_space = spaces.Box(\n        -np.inf,\n        np.inf,\n        shape=(2,),\n        dtype=np.float32,\n        name=name,\n    )\n\n    self.action_space = spaces.Discrete(2, name={\"fixation\": 0, \"go\": 1})\n</code></pre>"},{"location":"api/envs/#neurogym.envs.native.dualdelaymatchsample","title":"dualdelaymatchsample","text":""},{"location":"api/envs/#neurogym.envs.native.dualdelaymatchsample.DualDelayMatchSample","title":"DualDelayMatchSample","text":"<pre><code>DualDelayMatchSample(\n    dt=100, rewards=None, timing=None, sigma=1.0\n)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>Two-item Delay-match-to-sample.</p> <p>The trial starts with a fixation period. Then during the sample period, two sample stimuli are shown simultaneously. Followed by the first delay period, a cue is shown, indicating which sample stimulus will be tested. Then the first test stimulus is shown and the agent needs to report whether this test stimulus matches the cued sample stimulus. Then another delay and then test period follows, and the agent needs to report whether the other sample stimulus matches the second test stimulus.</p> Source code in <code>neurogym/envs/native/dualdelaymatchsample.py</code> <pre><code>def __init__(self, dt=100, rewards=None, timing=None, sigma=1.0) -&gt; None:\n    super().__init__(dt=dt)\n    self.choices = [1, 2]\n    self.cues = [0, 1]\n\n    self.sigma = sigma / np.sqrt(self.dt)  # Input noise\n\n    # Rewards\n    self.rewards = {\"abort\": -0.1, \"correct\": +1.0, \"fail\": 0.0}\n    if rewards:\n        self.rewards.update(rewards)\n\n    self.timing = {\n        \"fixation\": 500,\n        \"sample\": 500,\n        \"delay1\": 500,\n        \"cue1\": 500,\n        \"test1\": 500,\n        \"delay2\": 500,\n        \"cue2\": 500,\n        \"test2\": 500,\n    }\n    if timing:\n        self.timing.update(timing)\n\n    self.abort = False\n\n    name = {\n        \"fixation\": 0,\n        \"stimulus1\": range(1, 3),\n        \"stimulus2\": range(3, 5),\n        \"cue1\": 5,\n        \"cue2\": 6,\n    }\n    self.observation_space = spaces.Box(\n        -np.inf,\n        np.inf,\n        shape=(7,),\n        dtype=np.float32,\n        name=name,\n    )\n    name = {\"fixation\": 0, \"match\": 1, \"non-match\": 2}\n    self.action_space = spaces.Discrete(3, name=name)\n</code></pre>"},{"location":"api/envs/#neurogym.envs.native.economicdecisionmaking","title":"economicdecisionmaking","text":""},{"location":"api/envs/#neurogym.envs.native.economicdecisionmaking.EconomicDecisionMaking","title":"EconomicDecisionMaking","text":"<pre><code>EconomicDecisionMaking(dt=100, rewards=None, timing=None)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>Economic decision making task.</p> <p>A agent chooses between two options. Each option offers a certain amount of juice. Its amount is indicated by the stimulus. The two options offer different types of juice, and the agent prefers one over another.</p> Source code in <code>neurogym/envs/native/economicdecisionmaking.py</code> <pre><code>def __init__(self, dt=100, rewards=None, timing=None) -&gt; None:\n    super().__init__(dt=dt)\n\n    # trial conditions\n    self.B_to_A = 1 / 2.2\n    self.juices = [(\"a\", \"b\"), (\"b\", \"a\")]\n    self.offers = [\n        (0, 1),\n        (1, 3),\n        (1, 2),\n        (1, 1),\n        (2, 1),\n        (3, 1),\n        (4, 1),\n        (6, 1),\n        (2, 0),\n    ]\n\n    # Rewards\n    self.rewards = {\"abort\": -0.1, \"correct\": +0.22}\n    if rewards:\n        self.rewards.update(rewards)\n\n    self.timing = {\n        \"fixation\": 1500,\n        \"offer_on\": lambda: self.rng.uniform(1000, 2000),\n        \"decision\": 750,\n    }\n    if timing:\n        self.timing.update(timing)\n\n    self.R_B = self.B_to_A * self.rewards[\"correct\"]\n    self.R_A = self.rewards[\"correct\"]\n    self.abort = False\n    # Increase initial policy -&gt; baseline weights\n    self.baseline_Win = 10\n\n    name = {\n        \"fixation\": 0,\n        \"a1\": 1,\n        \"b1\": 2,  # a or b for choice 1\n        \"a2\": 3,\n        \"b2\": 4,  # a or b for choice 2\n        \"n1\": 5,\n        \"n2\": 6,  # amount for choice 1 or 2\n    }\n    self.observation_space = spaces.Box(\n        -np.inf,\n        np.inf,\n        shape=(7,),\n        dtype=np.float32,\n        name=name,\n    )\n\n    self.act_dict = {\"fixation\": 0, \"choice1\": 1, \"choice2\": 2}\n    self.action_space = spaces.Discrete(3, name=self.act_dict)\n</code></pre>"},{"location":"api/envs/#neurogym.envs.native.gonogo","title":"gonogo","text":""},{"location":"api/envs/#neurogym.envs.native.gonogo.GoNogo","title":"GoNogo","text":"<pre><code>GoNogo(dt=100, rewards=None, timing=None)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>Go/No-go task.</p> <p>A stimulus is shown during the stimulus period. The stimulus period is followed by a delay period, and then a decision period. If the stimulus is a Go stimulus, then the subject should choose the action Go during the decision period, otherwise, the subject should remain fixation.</p> Source code in <code>neurogym/envs/native/gonogo.py</code> <pre><code>def __init__(self, dt=100, rewards=None, timing=None) -&gt; None:\n    super().__init__(dt=dt)\n    # Actions are (FIXATE, GO)\n    self.actions = [0, 1]\n    # trial conditions\n    self.choices = [0, 1]\n\n    # Rewards\n    self.rewards = {\"abort\": -0.1, \"correct\": +1.0, \"fail\": -0.5, \"miss\": -0.5}\n    if rewards:\n        self.rewards.update(rewards)\n\n    self.timing = {\"fixation\": 0, \"stimulus\": 500, \"delay\": 500, \"decision\": 500}\n    if timing:\n        self.timing.update(timing)\n\n    self.abort = False\n    # set action and observation spaces\n    name = {\"fixation\": 0, \"nogo\": 1, \"go\": 2}\n    self.observation_space = spaces.Box(\n        -np.inf,\n        np.inf,\n        shape=(3,),\n        dtype=np.float32,\n        name=name,\n    )\n    self.action_space = spaces.Discrete(2, {\"fixation\": 0, \"go\": 1})\n</code></pre>"},{"location":"api/envs/#neurogym.envs.native.hierarchicalreasoning","title":"hierarchicalreasoning","text":"<p>Hierarchical reasoning tasks.</p>"},{"location":"api/envs/#neurogym.envs.native.hierarchicalreasoning.HierarchicalReasoning","title":"HierarchicalReasoning","text":"<pre><code>HierarchicalReasoning(dt=100, rewards=None, timing=None)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>Hierarchical reasoning of rules.</p> <p>On each trial, the subject receives two flashes separated by a delay period. The subject needs to judge whether the duration of this delay period is shorter than a threshold. Both flashes appear at the same location on each trial. For one trial type, the network should report its decision by going to the location of the flashes if the delay is shorter than the threshold. In another trial type, the network should go to the opposite direction of the flashes if the delay is short. The two types of trials are alternated across blocks, and the block transtion is unannouced.</p> Source code in <code>neurogym/envs/native/hierarchicalreasoning.py</code> <pre><code>def __init__(self, dt=100, rewards=None, timing=None) -&gt; None:\n    super().__init__(dt=dt)\n    self.choices = [0, 1]\n\n    self.rewards = {\"abort\": -0.1, \"correct\": +1.0, \"fail\": 0.0}\n    if rewards:\n        self.rewards.update(rewards)\n\n    self.timing = {\n        \"fixation\": TruncExp(600, 400, 800),\n        \"rule_target\": 1000,\n        \"fixation2\": TruncExp(600, 400, 900),\n        \"flash1\": 100,\n        \"delay\": (530, 610, 690, 770, 850, 930, 1010, 1090, 1170),\n        \"flash2\": 100,\n        \"decision\": 700,\n    }\n    if timing:\n        self.timing.update(timing)\n    self.mid_delay = np.median(self.timing[\"delay\"][1])\n\n    self.abort = False\n\n    name = {\"fixation\": 0, \"rule\": [1, 2], \"stimulus\": [3, 4]}\n    self.observation_space = spaces.Box(\n        -np.inf,\n        np.inf,\n        shape=(5,),\n        dtype=np.float32,\n        name=name,\n    )\n    name = {\"fixation\": 0, \"rule\": [1, 2], \"choice\": [3, 4]}\n    self.action_space = spaces.Discrete(5, name=name)\n\n    self.chose_correct_rule = False\n    self.rule = 0\n    self.trial_in_block = 0\n    self.block_size = 10\n    self.new_block()\n</code></pre>"},{"location":"api/envs/#neurogym.envs.native.intervaldiscrimination","title":"intervaldiscrimination","text":""},{"location":"api/envs/#neurogym.envs.native.intervaldiscrimination.IntervalDiscrimination","title":"IntervalDiscrimination","text":"<pre><code>IntervalDiscrimination(dt=80, rewards=None, timing=None)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>Comparing the time length of two stimuli.</p> <p>Two stimuli are shown sequentially, separated by a delay period. The duration of each stimulus is randomly sampled on each trial. The subject needs to judge which stimulus has a longer duration, and reports its decision during the decision period by choosing one of the two choice options.</p> Source code in <code>neurogym/envs/native/intervaldiscrimination.py</code> <pre><code>def __init__(self, dt=80, rewards=None, timing=None) -&gt; None:\n    super().__init__(dt=dt)\n    # Rewards\n    self.rewards = {\"abort\": -0.1, \"correct\": +1.0, \"fail\": 0.0}\n    if rewards:\n        self.rewards.update(rewards)\n\n    self.timing = {\n        \"fixation\": 300,\n        \"stim1\": lambda: self.rng.uniform(300, 600),\n        \"delay1\": lambda: self.rng.uniform(800, 1500),\n        \"stim2\": lambda: self.rng.uniform(300, 600),\n        \"delay2\": 500,\n        \"decision\": 300,\n    }\n    if timing:\n        self.timing.update(timing)\n\n    self.abort = False\n\n    name = {\"fixation\": 0, \"stim1\": 1, \"stim2\": 2}\n    self.observation_space = spaces.Box(\n        -np.inf,\n        np.inf,\n        shape=(3,),\n        dtype=np.float32,\n        name=name,\n    )\n    name = {\"fixation\": 0, \"choice1\": 1, \"choice2\": 2}\n    self.action_space = spaces.Discrete(3, name=name)\n</code></pre>"},{"location":"api/envs/#neurogym.envs.native.multisensory","title":"multisensory","text":"<p>Multi-Sensory Integration.</p>"},{"location":"api/envs/#neurogym.envs.native.multisensory.MultiSensoryIntegration","title":"MultiSensoryIntegration","text":"<pre><code>MultiSensoryIntegration(\n    dt=100, rewards=None, timing=None, sigma=1.0, dim_ring=2\n)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>Multi-sensory integration.</p> <p>Two stimuli are shown in two input modalities. Each stimulus points to one of the possible responses with a certain strength (coherence). The correct choice is the response with the highest summed strength from both stimuli. The agent is therefore encouraged to integrate information from both modalities equally.</p> Source code in <code>neurogym/envs/native/multisensory.py</code> <pre><code>def __init__(self, dt=100, rewards=None, timing=None, sigma=1.0, dim_ring=2) -&gt; None:\n    super().__init__(dt=dt)\n\n    # trial conditions\n    self.cohs = [5, 15, 50]\n\n    self.sigma = sigma / np.sqrt(self.dt)  # Input noise\n\n    # Rewards\n    self.rewards = {\"abort\": -0.1, \"correct\": +1.0}\n    if rewards:\n        self.rewards.update(rewards)\n\n    self.timing = {\"fixation\": 300, \"stimulus\": 750, \"decision\": 100}\n    if timing:\n        self.timing.update(timing)\n    self.abort = False\n\n    # set action and observation space\n    self.theta = np.linspace(0, 2 * np.pi, dim_ring + 1)[:-1]\n    self.choices = np.arange(dim_ring)\n\n    name = {\n        \"fixation\": 0,\n        \"stimulus_mod1\": range(1, dim_ring + 1),\n        \"stimulus_mod2\": range(dim_ring + 1, 2 * dim_ring + 1),\n    }\n    self.observation_space = spaces.Box(\n        -np.inf,\n        np.inf,\n        shape=(1 + 2 * dim_ring,),\n        dtype=np.float32,\n        name=name,\n    )\n\n    name = {\"fixation\": 0, \"choice\": range(1, dim_ring + 1)}\n    self.action_space = spaces.Discrete(1 + dim_ring, name=name)\n</code></pre>"},{"location":"api/envs/#neurogym.envs.native.null","title":"null","text":""},{"location":"api/envs/#neurogym.envs.native.null.Null","title":"Null","text":"<pre><code>Null(dt=100)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>Null task.</p> Source code in <code>neurogym/envs/native/null.py</code> <pre><code>def __init__(self, dt=100) -&gt; None:\n    super().__init__(dt=dt)\n    self.action_space = spaces.Discrete(1)\n    self.observation_space = spaces.Box(\n        -np.inf,\n        np.inf,\n        shape=(1,),\n        dtype=np.float32,\n    )\n</code></pre>"},{"location":"api/envs/#neurogym.envs.native.perceptualdecisionmaking","title":"perceptualdecisionmaking","text":""},{"location":"api/envs/#neurogym.envs.native.perceptualdecisionmaking.PerceptualDecisionMaking","title":"PerceptualDecisionMaking","text":"<pre><code>PerceptualDecisionMaking(\n    dt: int = 100,\n    dim_ring: int = 2,\n    rewards: dict[str, float] | None = None,\n    timing: dict[str, int | TruncExp] | None = None,\n    cohs: list[float] | None = None,\n    sigma: float = 1.0,\n    abort: bool = False,\n)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>Perceptual decision-making task.</p> <p>Two-alternative forced choice task where the agent integrates noisy stimuli to decide which option has a higher average value.</p> <p>The agent observes a noisy stimulus during the stimulus period, with varying coherence levels (signal strength). The correct response is the location with the stronger average evidence.</p> <p>Parameters:</p> Name Type Description Default <code>dt</code> <code>int</code> <p>Timestep of the environment in milliseconds.</p> <code>100</code> <code>dim_ring</code> <code>int</code> <p>Number of stimulus locations (or choices).</p> <code>2</code> <code>rewards</code> <code>dict[str, float] | None</code> <p>Optional dictionary to override default rewards. The required keys are \"abort\", \"correct\", and \"fail\". Defaults to {\"abort\": -0.1, \"correct\": 1.0, \"fail\": 0.0}.</p> <code>None</code> <code>timing</code> <code>dict[str, int | TruncExp] | None</code> <p>Optional dictionary to override default durations of task periods. The expected keys are \"fixation\", \"stimulus\" (required), \"delay\", and \"decision\" (required). Defaults to {\"fixation\": 100, \"stimulus\": 2000, \"delay\": 0, \"decision\": 100}.</p> <code>None</code> <code>cohs</code> <code>list[float] | None</code> <p>Optional list of coherence levels controlling task difficulty. Defaults to [0, 6.4, 12.8, 25.6, 51.2].</p> <code>None</code> <code>sigma</code> <code>float</code> <p>Standard deviation of Gaussian noise added to stimulus.</p> <code>1.0</code> <code>abort</code> <code>bool</code> <p>If True, incorrect actions during fixation abort the trial.</p> <code>False</code> Source code in <code>neurogym/envs/native/perceptualdecisionmaking.py</code> <pre><code>def __init__(\n    self,\n    dt: int = 100,\n    dim_ring: int = 2,\n    rewards: dict[str, float] | None = None,\n    timing: dict[str, int | TruncExp] | None = None,\n    cohs: list[float] | None = None,\n    sigma: float = 1.0,\n    abort: bool = False,\n) -&gt; None:\n    super().__init__(dt=dt)\n\n    self.dt = dt\n    self.dim_ring = dim_ring\n    self.abort = abort\n\n    self.rewards = {\"abort\": -0.1, \"correct\": 1.0, \"fail\": 0.0}\n    if rewards:\n        self.rewards.update(rewards)\n\n    self.timing = {\"fixation\": 100, \"stimulus\": 2000, \"delay\": 0, \"decision\": 100}\n    if timing:\n        self.timing.update(timing)\n\n    self.cohs = cohs or [0, 6.4, 12.8, 25.6, 51.2]\n    self.sigma = sigma / np.sqrt(self.dt)\n\n    self.theta = np.linspace(0, 2 * np.pi, self.dim_ring, endpoint=False)\n    self.choices = np.arange(self.dim_ring)\n\n    self.observation_space = spaces.Box(\n        -np.inf,\n        np.inf,\n        shape=(1 + self.dim_ring,),\n        dtype=np.float32,\n        name={\"fixation\": 0, \"stimulus\": list(range(1, self.dim_ring + 1))},\n    )\n    self.action_space = spaces.Discrete(\n        1 + self.dim_ring,\n        name={\"fixation\": 0, \"choice\": list(range(1, self.dim_ring + 1))},\n    )\n</code></pre>"},{"location":"api/envs/#neurogym.envs.native.perceptualdecisionmaking.PerceptualDecisionMakingDelayResponse","title":"PerceptualDecisionMakingDelayResponse","text":"<pre><code>PerceptualDecisionMakingDelayResponse(\n    dt=100,\n    rewards=None,\n    timing=None,\n    stim_scale=1.0,\n    sigma=1.0,\n)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>Perceptual decision-making with delayed responses.</p> <p>Agents have to integrate two stimuli and report which one is larger on average after a delay.</p> <p>Parameters:</p> Name Type Description Default <code>stim_scale</code> <p>Controls the difficulty of the experiment. (def: 1., float)</p> <code>1.0</code> Source code in <code>neurogym/envs/native/perceptualdecisionmaking.py</code> <pre><code>def __init__(self, dt=100, rewards=None, timing=None, stim_scale=1.0, sigma=1.0) -&gt; None:\n    super().__init__(dt=dt)\n    self.choices = [1, 2]\n    # cohs specifies the amount of evidence (modulated by stim_scale)\n    self.cohs = np.array([0, 6.4, 12.8, 25.6, 51.2]) * stim_scale\n    self.sigma = sigma / np.sqrt(self.dt)  # Input noise\n\n    # Rewards\n    self.rewards = {\"abort\": -0.1, \"correct\": +1.0, \"fail\": 0.0}\n    if rewards:\n        self.rewards.update(rewards)\n\n    self.timing = {\n        \"fixation\": 0,\n        \"stimulus\": 1150,\n        #  TODO: sampling of delays follows exponential\n        \"delay\": (300, 500, 700, 900, 1200, 2000, 3200, 4000),\n        # 'go_cue': 100,  # noqa: ERA001 TODO: Not implemented\n        \"decision\": 1500,\n    }\n    if timing:\n        self.timing.update(timing)\n\n    self.abort = False\n\n    # action and observation spaces\n    self.action_space = spaces.Discrete(3)\n    self.observation_space = spaces.Box(\n        -np.inf,\n        np.inf,\n        shape=(3,),\n        dtype=np.float32,\n    )\n</code></pre>"},{"location":"api/envs/#neurogym.envs.native.perceptualdecisionmaking.PulseDecisionMaking","title":"PulseDecisionMaking","text":"<pre><code>PulseDecisionMaking(\n    dt=10,\n    rewards=None,\n    timing=None,\n    p_pulse=(0.3, 0.7),\n    n_bin=6,\n)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>Pulse-based decision making task.</p> <p>Discrete stimuli are presented briefly as pulses.</p> <p>Parameters:</p> Name Type Description Default <code>p_pulse</code> <p>array-like, probability of pulses for each choice</p> <code>(0.3, 0.7)</code> <code>n_bin</code> <p>int, number of stimulus bins</p> <code>6</code> Source code in <code>neurogym/envs/native/perceptualdecisionmaking.py</code> <pre><code>def __init__(self, dt=10, rewards=None, timing=None, p_pulse=(0.3, 0.7), n_bin=6) -&gt; None:\n    super().__init__(dt=dt)\n    self.p_pulse = p_pulse\n    self.n_bin = n_bin\n\n    # Rewards\n    self.rewards = {\"abort\": -0.1, \"correct\": +1.0, \"fail\": 0.0}\n    if rewards:\n        self.rewards.update(rewards)\n\n    self.timing = {\"fixation\": 500, \"decision\": 500}\n    for i in range(n_bin):\n        self.timing[f\"cue{i}\"] = 10\n        self.timing[f\"bin{i}\"] = 240\n    if timing:\n        self.timing.update(timing)\n\n    self.abort = False\n\n    name = {\"fixation\": 0, \"stimulus\": [1, 2]}\n    self.observation_space = spaces.Box(\n        -np.inf,\n        np.inf,\n        shape=(3,),\n        dtype=np.float32,\n        name=name,\n    )\n    name = {\"fixation\": 0, \"choice\": [1, 2]}\n    self.action_space = spaces.Discrete(3, name=name)\n</code></pre>"},{"location":"api/envs/#neurogym.envs.native.postdecisionwager","title":"postdecisionwager","text":""},{"location":"api/envs/#neurogym.envs.native.postdecisionwager.PostDecisionWager","title":"PostDecisionWager","text":"<pre><code>PostDecisionWager(\n    dt=100, rewards=None, timing=None, dim_ring=2, sigma=1.0\n)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>Post-decision wagering task assessing confidence.</p> <p>The agent first performs a perceptual discrimination task (see for more details the PerceptualDecisionMaking task). On a random half of the trials, the agent is given the option to abort the sensory discrimination and to choose instead a sure-bet option that guarantees a small reward. Therefore, the agent is encouraged to choose the sure-bet option when it is uncertain about its perceptual decision.</p> Source code in <code>neurogym/envs/native/postdecisionwager.py</code> <pre><code>def __init__(self, dt=100, rewards=None, timing=None, dim_ring=2, sigma=1.0) -&gt; None:\n    super().__init__(dt=dt)\n\n    self.wagers = [True, False]\n    self.theta = np.linspace(0, 2 * np.pi, dim_ring + 1)[:-1]\n    self.choices = np.arange(dim_ring)\n    self.cohs = [0, 3.2, 6.4, 12.8, 25.6, 51.2]\n    self.sigma = sigma / np.sqrt(self.dt)  # Input noise\n\n    # Rewards\n    self.rewards = {\"abort\": -0.1, \"correct\": +1.0, \"fail\": 0.0}\n    if rewards:\n        self.rewards.update(rewards)\n    self.rewards[\"sure\"] = 0.7 * self.rewards[\"correct\"]\n\n    self.timing = {\n        \"fixation\": 100,\n        # 'target':  0,  # noqa: ERA001\n        \"stimulus\": TruncExp(180, 100, 900),\n        \"delay\": TruncExp(1350, 1200, 1800),\n        \"pre_sure\": lambda: self.rng.uniform(500, 750),\n        \"decision\": 100,\n    }\n    if timing:\n        self.timing.update(timing)\n\n    self.abort = False\n\n    # set action and observation space\n    name = {\"fixation\": 0, \"stimulus\": [1, 2], \"sure\": 3}\n    self.observation_space = spaces.Box(\n        -np.inf,\n        np.inf,\n        shape=(4,),\n        dtype=np.float32,\n        name=name,\n    )\n    name = {\"fixation\": 0, \"choice\": [1, 2], \"sure\": 3}\n    self.action_space = spaces.Discrete(4, name=name)\n</code></pre>"},{"location":"api/envs/#neurogym.envs.native.probabilisticreasoning","title":"probabilisticreasoning","text":"<p>Random dot motion task.</p>"},{"location":"api/envs/#neurogym.envs.native.probabilisticreasoning.ProbabilisticReasoning","title":"ProbabilisticReasoning","text":"<pre><code>ProbabilisticReasoning(\n    dt=100,\n    rewards=None,\n    timing=None,\n    shape_weight=None,\n    n_loc=4,\n)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>Probabilistic reasoning.</p> <p>The agent is shown a sequence of stimuli. Each stimulus is associated with a certain log-likelihood of the correct response being one choice versus the other. The final log-likelihood of the target response being, for example, option 1, is the sum of all log-likelihood associated with the presented stimuli. A delay period separates each stimulus, so the agent is encouraged to lean the log-likelihood association and integrate these values over time within a trial.</p> <p>Parameters:</p> Name Type Description Default <code>shape_weight</code> <p>array-like, evidence weight of each shape</p> <code>None</code> <code>n_loc</code> <p>int, number of location of show shapes</p> <code>4</code> Source code in <code>neurogym/envs/native/probabilisticreasoning.py</code> <pre><code>def __init__(self, dt=100, rewards=None, timing=None, shape_weight=None, n_loc=4) -&gt; None:\n    super().__init__(dt=dt)\n    # The evidence weight of each stimulus\n    if shape_weight is not None:\n        self.shape_weight = shape_weight\n    else:\n        self.shape_weight = [-10, -0.9, -0.7, -0.5, -0.3, 0.3, 0.5, 0.7, 0.9, 10]\n\n    self.n_shape = len(self.shape_weight)\n    dim_shape = self.n_shape\n    # Shape representation needs to be fixed cross-platform\n    self.shapes = np.eye(self.n_shape, dim_shape)\n    self.n_loc = n_loc\n\n    # Rewards\n    self.rewards = {\"abort\": -0.1, \"correct\": +1.0, \"fail\": 0.0}\n    if rewards:\n        self.rewards.update(rewards)\n\n    self.timing = {\n        \"fixation\": 500,\n        \"delay\": lambda: self.rng.uniform(450, 550),\n        \"decision\": 500,\n    }\n    for i_loc in range(n_loc):\n        self.timing[f\"stimulus{i_loc}\"] = 500\n    if timing:\n        self.timing.update(timing)\n\n    self.abort = False\n\n    obs_name: dict[str, int | range] = {\"fixation\": 0}\n    start = 1\n    for i_loc in range(n_loc):\n        obs_name[f\"loc{i_loc}\"] = range(start, start + dim_shape)\n        start += dim_shape\n    self.observation_space = spaces.Box(\n        -np.inf,\n        np.inf,\n        shape=(1 + dim_shape * n_loc,),\n        dtype=np.float32,\n        name=obs_name,\n    )\n\n    action_name = {\"fixation\": 0, \"choice\": [1, 2]}\n    self.action_space = spaces.Discrete(3, name=action_name)\n</code></pre>"},{"location":"api/envs/#neurogym.envs.native.reaching","title":"reaching","text":"<p>Reaching to target.</p>"},{"location":"api/envs/#neurogym.envs.native.reaching.Reaching1D","title":"Reaching1D","text":"<pre><code>Reaching1D(dt=100, rewards=None, timing=None, dim_ring=16)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>Reaching to the stimulus.</p> <p>The agent is shown a stimulus during the fixation period. The stimulus encodes a one-dimensional variable such as a movement direction. At the end of the fixation period, the agent needs to respond by reaching towards the stimulus direction.</p> Source code in <code>neurogym/envs/native/reaching.py</code> <pre><code>def __init__(self, dt=100, rewards=None, timing=None, dim_ring=16) -&gt; None:\n    super().__init__(dt=dt)\n    # Rewards\n    self.rewards = {\"correct\": +1.0, \"fail\": -0.1}\n    if rewards:\n        self.rewards.update(rewards)\n\n    self.timing = {\"fixation\": 500, \"reach\": 500}\n    if timing:\n        self.timing.update(timing)\n\n    # action and observation spaces\n    obs_name = {\"self\": range(dim_ring, 2 * dim_ring), \"target\": range(dim_ring)}\n    self.observation_space = spaces.Box(\n        -np.inf,\n        np.inf,\n        shape=(2 * dim_ring,),\n        dtype=np.float32,\n        name=obs_name,\n    )\n    action_name = {\"fixation\": 0, \"left\": 1, \"right\": 2}\n    self.action_space = spaces.Discrete(3, name=action_name)\n\n    self.theta = np.arange(0, 2 * np.pi, 2 * np.pi / dim_ring)\n    self.state = np.pi\n    self.dim_ring = dim_ring\n</code></pre>"},{"location":"api/envs/#neurogym.envs.native.reaching.Reaching1D.post_step","title":"post_step","text":"<pre><code>post_step(ob, reward, terminated, truncated, info)\n</code></pre> <p>Modify observation.</p> Source code in <code>neurogym/envs/native/reaching.py</code> <pre><code>def post_step(self, ob, reward, terminated, truncated, info):\n    \"\"\"Modify observation.\"\"\"\n    ob[self.dim_ring :] = np.cos(self.theta - self.state)\n    return ob, reward, terminated, truncated, info\n</code></pre>"},{"location":"api/envs/#neurogym.envs.native.reaching.Reaching1DWithSelfDistraction","title":"Reaching1DWithSelfDistraction","text":"<pre><code>Reaching1DWithSelfDistraction(\n    dt=100, rewards=None, timing=None\n)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>Reaching with self distraction.</p> <p>In this task, the reaching state itself generates strong inputs that overshadows the actual target input. This task is inspired by behavior in electric fish where the electric sensing organ is distracted by discharges from its own electric organ for active sensing. Similar phenomena in bats.</p> Source code in <code>neurogym/envs/native/reaching.py</code> <pre><code>def __init__(self, dt=100, rewards=None, timing=None) -&gt; None:\n    super().__init__(dt=dt)\n    # Rewards\n    self.rewards = {\"correct\": +1.0, \"fail\": -0.1}\n    if rewards:\n        self.rewards.update(rewards)\n\n    self.timing = {\"fixation\": 500, \"reach\": 500}\n    if timing:\n        self.timing.update(timing)\n\n    # action and observation spaces\n    self.action_space = spaces.Discrete(3)\n    self.observation_space = spaces.Box(\n        -np.inf,\n        np.inf,\n        shape=(32,),\n        dtype=np.float32,\n    )\n    self.theta = np.arange(0, 2 * np.pi, 2 * np.pi / 32)\n    self.state = np.pi\n</code></pre>"},{"location":"api/envs/#neurogym.envs.native.reaching.Reaching1DWithSelfDistraction.post_step","title":"post_step","text":"<pre><code>post_step(ob, reward, terminated, truncated, info)\n</code></pre> <p>Modify observation.</p> Source code in <code>neurogym/envs/native/reaching.py</code> <pre><code>def post_step(self, ob, reward, terminated, truncated, info):\n    \"\"\"Modify observation.\"\"\"\n    ob += np.cos(self.theta - self.state)\n    return ob, reward, terminated, truncated, info\n</code></pre>"},{"location":"api/envs/#neurogym.envs.native.reachingdelayresponse","title":"reachingdelayresponse","text":""},{"location":"api/envs/#neurogym.envs.native.reachingdelayresponse.ReachingDelayResponse","title":"ReachingDelayResponse","text":"<pre><code>ReachingDelayResponse(\n    dt=100,\n    rewards=None,\n    timing=None,\n    lowbound=0.0,\n    highbound=1.0,\n)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>Reaching task with a delay period.</p> <p>A reaching direction is presented by the stimulus during the stimulus period. Followed by a delay period, the agent needs to respond to the direction of the stimulus during the decision period.</p> Source code in <code>neurogym/envs/native/reachingdelayresponse.py</code> <pre><code>def __init__(self, dt=100, rewards=None, timing=None, lowbound=0.0, highbound=1.0) -&gt; None:\n    super().__init__(dt=dt)\n    self.lowbound = lowbound\n    self.highbound = highbound\n\n    # Rewards\n    self.rewards = {\"abort\": -0.1, \"correct\": +1.0, \"fail\": -0.0, \"miss\": -0.5}\n    if rewards:\n        self.rewards.update(rewards)\n\n    self.timing = {\"stimulus\": 500, \"delay\": (0, 1000, 2000), \"decision\": 500}\n    if timing:\n        self.timing.update(timing)\n\n    self.r_tmax = self.rewards[\"miss\"]\n    self.abort = False\n\n    name = {\"go\": 0, \"stimulus\": 1}\n    self.observation_space = spaces.Box(\n        low=np.array([0.0, -2]),\n        high=np.array([1, 2.0]),\n        dtype=np.float32,\n        name=name,\n    )\n\n    self.action_space = spaces.Box(\n        low=np.array((-1.0, -1.0)),\n        high=np.array((1.0, 2.0)),\n        dtype=np.float32,\n    )\n</code></pre>"},{"location":"api/envs/#neurogym.envs.native.readysetgo","title":"readysetgo","text":"<p>Ready-set-go task.</p>"},{"location":"api/envs/#neurogym.envs.native.readysetgo.ReadySetGo","title":"ReadySetGo","text":"<pre><code>ReadySetGo(\n    dt=80,\n    rewards=None,\n    timing=None,\n    gain=1,\n    prod_margin=0.2,\n)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>Agents have to measure and produce different time intervals.</p> <p>A stimulus is briefly shown during a ready period, then again during a set period. The ready and set periods are separated by a measure period, the duration of which is randomly sampled on each trial. The agent is required to produce a response after the set cue such that the interval between the response and the set cue is as close as possible to the duration of the measure period.</p> <p>Parameters:</p> Name Type Description Default <code>gain</code> <p>Controls the measure that the agent has to produce. (def: 1, int)</p> <code>1</code> <code>prod_margin</code> <p>controls the interval around the ground truth production time within which the agent receives proportional reward</p> <code>0.2</code> Source code in <code>neurogym/envs/native/readysetgo.py</code> <pre><code>def __init__(self, dt=80, rewards=None, timing=None, gain=1, prod_margin=0.2) -&gt; None:\n    super().__init__(dt=dt)\n    self.prod_margin = prod_margin\n\n    self.gain = gain\n\n    # Rewards\n    self.rewards = {\"abort\": -0.1, \"correct\": +1.0, \"fail\": 0.0}\n    if rewards:\n        self.rewards.update(rewards)\n\n    self.timing = {\n        \"fixation\": 100,\n        \"ready\": 83,\n        \"measure\": lambda: self.rng.uniform(800, 1500),\n        \"set\": 83,\n    }\n    if timing:\n        self.timing.update(timing)\n\n    self.abort = False\n    # set action and observation space\n    name = {\"fixation\": 0, \"ready\": 1, \"set\": 2}\n    self.observation_space = spaces.Box(\n        -np.inf,\n        np.inf,\n        shape=(3,),\n        dtype=np.float32,\n        name=name,\n    )\n\n    name = {\"fixation\": 0, \"go\": 1}\n    self.action_space = spaces.Discrete(2, name=name)  # (fixate, go)\n</code></pre>"},{"location":"api/envs/#neurogym.envs.native.readysetgo.MotorTiming","title":"MotorTiming","text":"<pre><code>MotorTiming(\n    dt=80, rewards=None, timing=None, prod_margin=0.2\n)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>Agents have to produce different time intervals using different effectors (actions).</p> <p>Parameters:</p> Name Type Description Default <code>prod_margin</code> <p>controls the interval around the ground truth production time within which the agent receives proportional reward.</p> <code>0.2</code> Source code in <code>neurogym/envs/native/readysetgo.py</code> <pre><code>def __init__(self, dt=80, rewards=None, timing=None, prod_margin=0.2) -&gt; None:\n    super().__init__(dt=dt)\n    self.prod_margin = prod_margin\n    self.production_ind = [0, 1]\n    self.intervals = [800, 1500]\n\n    # Rewards\n    self.rewards = {\"abort\": -0.1, \"correct\": +1.0, \"fail\": 0.0}\n    if rewards:\n        self.rewards.update(rewards)\n\n    self.timing = {\n        \"fixation\": 500,  # XXX: not specified\n        \"cue\": lambda: self.rng.uniform(1000, 3000),\n        \"set\": 50,\n    }\n    if timing:\n        self.timing.update(timing)\n\n    self.abort = False\n    # set action and observation space\n    self.action_space = spaces.Discrete(2)  # (fixate, go)\n    # Fixation, Interval indicator x2, Set\n    self.observation_space = spaces.Box(\n        -np.inf,\n        np.inf,\n        shape=(4,),\n        dtype=np.float32,\n    )\n</code></pre>"},{"location":"api/envs/#neurogym.envs.native.readysetgo.OneTwoThreeGo","title":"OneTwoThreeGo","text":"<pre><code>OneTwoThreeGo(\n    dt=80, rewards=None, timing=None, prod_margin=0.2\n)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>Agents reproduce time intervals based on two samples.</p> <p>Parameters:</p> Name Type Description Default <code>prod_margin</code> <p>controls the interval around the ground truth production         time within which the agent receives proportional reward</p> <code>0.2</code> Source code in <code>neurogym/envs/native/readysetgo.py</code> <pre><code>def __init__(self, dt=80, rewards=None, timing=None, prod_margin=0.2) -&gt; None:\n    super().__init__(dt=dt)\n\n    self.prod_margin = prod_margin\n\n    # Rewards\n    self.rewards = {\"abort\": -0.1, \"correct\": +1.0, \"fail\": 0.0}\n    if rewards:\n        self.rewards.update(rewards)\n\n    self.timing = {\n        \"fixation\": TruncExp(400, 100, 800),\n        \"target\": TruncExp(1000, 500, 1500),\n        \"s1\": 100,\n        \"interval1\": (600, 700, 800, 900, 1000),\n        \"s2\": 100,\n        \"interval2\": 0,\n        \"s3\": 100,\n        \"interval3\": 0,\n        \"response\": 1000,\n    }\n    if timing:\n        self.timing.update(timing)\n\n    self.abort = False\n    # set action and observation space\n    name = {\"fixation\": 0, \"stimulus\": 1, \"target\": 2}\n    self.observation_space = spaces.Box(\n        -np.inf,\n        np.inf,\n        shape=(3,),\n        dtype=np.float32,\n        name=name,\n    )\n    name = {\"fixation\": 0, \"go\": 1}\n    self.action_space = spaces.Discrete(2, name=name)\n</code></pre>"},{"location":"api/envs/#neurogym.envs.native.spatialsuppressmotion","title":"spatialsuppressmotion","text":""},{"location":"api/envs/#neurogym.envs.native.spatialsuppressmotion.SpatialSuppressMotion","title":"SpatialSuppressMotion","text":"<pre><code>SpatialSuppressMotion(dt=8.3, timing=None, rewards=None)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>Spatial suppression motion task.</p> <p>This task is useful to study center-surround interaction in monkey MT and human psychophysical performance in motion perception.</p> <p>Tha task is derived from (Tadin et al. Nature, 2003). In this task, there is no fixation or decision stage. We only present a stimulus and a subject needs to perform a 4-AFC motion direction judgement. The ground-truth is the probabilities for choosing the four directions at a given time point. The probabilities depend on stimulus contrast and size, and the probabilities are derived from emprically measured human psychophysical performance.</p> <p>In this version, the input size is 4 (directions) x 8 (size) = 32 neurons. This setting aims to simulate four pools (8 neurons in each pool) of neurons that are selective for four directions.</p> <p>Parameters:</p> Name Type Description Default <code>&lt;dt&gt;</code> <p>millisecs per image frame, default: 8.3 (given 120HZ monitor)</p> required <code>&lt;win_size&gt;</code> <p>size per image frame</p> required <code>&lt;timing&gt;</code> <p>millisecs, stimulus duration, default: 8.3 * 36 frames ~ 300 ms. This is the longest duration we need (i.e., probability reach ceilling)</p> required <p>Note that please input default seq_len = 36 frames when creating dataset object.</p>"},{"location":"api/envs/#neurogym.envs.native.spatialsuppressmotion.SpatialSuppressMotion--fixme-find-more-stable-way-of-enforcing-above","title":"FIXME: find more stable way of enforcing above.","text":"Source code in <code>neurogym/envs/native/spatialsuppressmotion.py</code> <pre><code>def __init__(self, dt=8.3, timing=None, rewards=None) -&gt; None:\n    if timing is None:\n        timing = {\"stimulus\": 300}\n    super().__init__(dt=dt)\n\n    # Rewards\n    self.rewards = {\"abort\": -0.1, \"correct\": +1.0, \"fail\": 0.0}\n    if rewards:\n        self.rewards.update(rewards)\n\n    # Timing\n    self.timing = {\n        \"stimulus\": 300,  # we only need stimulus period for psychophysical task\n    }\n    if timing:\n        self.timing.update(timing)\n\n    self.abort = False\n\n    # define action space four directions\n    self.action_space = spaces.Box(\n        0,\n        1,\n        shape=(4,),\n        dtype=np.float32,\n    )  # the probabilities for four direction\n\n    # define observation space\n    self.observation_space = spaces.Box(\n        0,\n        np.inf,\n        shape=(32,),\n        dtype=np.float32,\n    )  # observation space, 4 directions * 8 sizes\n    # larger stimulus could elicit more neurons to fire\n\n    self.directions = [1, 2, 3, 4]  # motion direction left/right/up/down\n    self.theta = [-np.pi / 2, np.pi / 2, 0, np.pi]  # direction angle of the four directions\n    self.directions_anti = [2, 1, 4, 3]\n    self.directions_ortho = [[3, 4], [3, 4], [1, 2], [1, 2]]\n</code></pre>"},{"location":"api/envs/#neurogym.envs.native.spatialsuppressmotion.SpatialSuppressMotion.getgroundtruth","title":"getgroundtruth","text":"<pre><code>getgroundtruth(trial)\n</code></pre> <p>The utility function to obtain ground truth probabilities for four direction.</p> <p>Input trial is a dict, contains fields , ,  <p>We output a (4,) tuple indicate the probabilities to perceive left/right/up/down direction. This label comes from emprically measured human performance</p> Source code in <code>neurogym/envs/native/spatialsuppressmotion.py</code> <pre><code>def getgroundtruth(self, trial):\n    \"\"\"The utility function to obtain ground truth probabilities for four direction.\n\n    Input trial is a dict, contains fields &lt;duration&gt;, &lt;contrast&gt;, &lt;diameter&gt;\n\n    We output a (4,) tuple indicate the probabilities to perceive left/right/up/down direction. This label comes\n    from emprically measured human performance\n    \"\"\"\n    frame_ind = [8, 9, 10, 13, 15, 18, 21, 28, 36, 37, 38, 39]\n    xx = [1, 2, 3, 4, 5, 6, 7]\n    yy = [0.249] * 7\n\n    frame_ind = xx + frame_ind  # to fill in the first a few frames\n    frame_ind = [i - 1 for i in frame_ind]  # frame index start from\n\n    seq_len = self.view_ob(period=\"stimulus\").shape[0]\n    xnew = np.arange(seq_len)\n\n    if trial[\"contrast\"] &gt; 0.5:\n        # large size (11 deg radius), High contrast\n        prob_corr = [*yy, 0.249, 0.249, 0.249, 0.27, 0.32, 0.4583, 0.65, 0.85, 0.99, 0.99, 0.99, 0.99]\n        prob_anti = [*yy, 0.249, 0.29, 0.31, 0.4, 0.475, 0.4167, 0.3083, 0.075, 0.04, 0.04, 0.03, 0.03]\n\n    elif trial[\"contrast\"] &lt; 0.5:\n        # large size (11 deg radius), low contrast\n        prob_corr = [*yy, 0.25, 0.26, 0.2583, 0.325, 0.45, 0.575, 0.875, 0.933, 0.99, 0.99, 0.99, 0.99]\n        prob_anti = [*yy, 0.25, 0.26, 0.2583, 0.267, 0.1417, 0.1167, 0.058, 0.016, 0.003, 0.003, 0.003, 0.003]\n\n    corr_prob = interp1d(\n        frame_ind,\n        prob_corr,\n        kind=\"slinear\",\n        fill_value=\"extrapolate\",\n    )(xnew)\n    anti_prob = interp1d(\n        frame_ind,\n        prob_anti,\n        kind=\"slinear\",\n        fill_value=\"extrapolate\",\n    )(xnew)\n    ortho_prob = (1 - (corr_prob + anti_prob)) / 2\n\n    direction = trial[\"direction\"] - 1\n    direction_anti = self.directions_anti[direction] - 1\n    direction_ortho = [i - 1 for i in self.directions_ortho[direction]]\n\n    gt = np.zeros((4, seq_len))\n    gt[direction, :] = corr_prob\n    gt[direction_anti, :] = anti_prob\n    gt[direction_ortho, :] = ortho_prob\n\n    return gt.T  # gt is a seq_len x 4 numpy array\n</code></pre>"},{"location":"api/envs/#neurogym.envs.native.tonedetection","title":"tonedetection","text":"<p>auditory tone detection task.</p>"},{"location":"api/envs/#neurogym.envs.native.tonedetection.ToneDetection","title":"ToneDetection","text":"<pre><code>ToneDetection(dt=50, sigma=0.2, timing=None)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>A subject is asked to report whether a pure tone is embeddied within a background noise.</p> <p>If yes, should indicate the position of the tone. The tone lasts 50ms and could appear at the 500ms, 1000ms, and 1500ms. The tone is embbeded within noises.</p> <p>By Ru-Yuan Zhang (ruyuanzhang@gmail.com)</p> <p>Note in this version we did not consider the fixation period as we mainly aim to model human data.</p> <p>For an animal version of this task, please consider to include fixation and saccade cues. See https://www.nature.com/articles/nn1386</p> <p>Note that the output labels is of shape (seq_len, batch_size). For a human perceptual task, you can simply run labels = labels[-1, :] get the final output.</p> <p>Parameters:</p> Name Type Description Default <code>&lt;dt&gt;</code> <p>milliseconds, delta time,</p> required <code>&lt;sigma&gt;</code> <p>float, input noise level, control the task difficulty</p> required <code>&lt;timing&gt;</code> <p>stimulus timing</p> required Source code in <code>neurogym/envs/native/tonedetection.py</code> <pre><code>def __init__(self, dt=50, sigma=0.2, timing=None) -&gt; None:\n    super().__init__(dt=dt)\n    \"\"\"\n    Here the key variables are\n    &lt;self.toneDur&gt;: ms, duration of the tone\n    &lt;self.toneTiming&gt;: ms, onset of the tone\n    \"\"\"\n    self.sigma = sigma / np.sqrt(self.dt)  # Input noise\n\n    # Rewards\n    self.rewards = {\n        \"abort\": -0.1,\n        \"correct\": +1.0,\n        \"noresp\": -0.1,\n    }  # need to change here\n\n    self.timing = {\n        \"stimulus\": 2000,\n        \"toneTiming\": [500, 1000, 1500],\n        \"toneDur\": 50,\n    }\n    if timing:\n        self.timing.update(timing)\n\n    self.toneTiming = self.timing[\"toneTiming\"]\n    self.toneDur = self.timing[\"toneDur\"]  # ms, the duration of a tone\n\n    if dt &gt; self.toneDur:\n        msg = f\"{dt=} must be smaller or equal tp tone duration {self.toneDur} (default=50).\"\n        raise ValueError(msg)\n\n    self.toneDurIdx = int(self.toneDur / dt)  # how many data point it lasts\n\n    self.toneTimingIdx = [int(i / dt) for i in self.toneTiming]\n    self.stimArray = np.zeros(int(self.timing[\"stimulus\"] / dt))\n\n    self.abort = False\n\n    self.signals = np.linspace(0, 1, 5)[:-1]  # signal strength\n    self.conditions = [0, 1, 2, 3]  # no tone, tone at position 1/2/3\n\n    self.observation_space = spaces.Box(\n        -np.inf,\n        np.inf,\n        shape=(1,),\n        dtype=np.float32,\n    )\n    self.ob_dict = {\"fixation\": 0, \"stimulus\": 1}\n    self.action_space = spaces.Discrete(4)\n    self.act_dict = {\"fixation\": 0, \"choice\": range(1, 5 + 1)}\n</code></pre>"},{"location":"api/utils/","title":"Utils","text":""},{"location":"api/utils/#neurogym.utils.data","title":"data","text":"<p>Utilities for data.</p>"},{"location":"api/utils/#neurogym.utils.data.Dataset","title":"Dataset","text":"<pre><code>Dataset(\n    env,\n    env_kwargs=None,\n    batch_size=1,\n    seq_len=None,\n    max_batch=inf,\n    batch_first=False,\n    cache_len=None,\n)\n</code></pre> <p>Make an environment into an iterable dataset for supervised learning.</p> <p>Create an iterator that at each call returns     inputs: numpy array (sequence_length, batch_size, input_units)     target: numpy array (sequence_length, batch_size, output_units)</p> <p>Parameters:</p> Name Type Description Default <code>env</code> <p>str for env id or gym.Env objects</p> required <code>env_kwargs</code> <p>dict, additional kwargs for environment, if env is str</p> <code>None</code> <code>batch_size</code> <p>int, batch size</p> <code>1</code> <code>seq_len</code> <p>int, sequence length</p> <code>None</code> <code>max_batch</code> <p>int, maximum number of batch for iterator, default infinite</p> <code>inf</code> <code>batch_first</code> <p>bool, if True, return (batch, seq_len, n_units), default False</p> <code>False</code> <code>cache_len</code> <p>int, default length of caching</p> <code>None</code> Source code in <code>neurogym/utils/data.py</code> <pre><code>def __init__(\n    self,\n    env,\n    env_kwargs=None,\n    batch_size=1,\n    seq_len=None,\n    max_batch=np.inf,\n    batch_first=False,\n    cache_len=None,\n) -&gt; None:\n    if not isinstance(env, str | gym.Env):\n        msg = f\"{type(env)=} must be `gym.Env` or `str`.\"\n        raise TypeError(msg)\n    if isinstance(env, gym.Env):\n        self.envs = [copy.deepcopy(env) for _ in range(batch_size)]\n    else:\n        if env_kwargs is None:\n            env_kwargs = {}\n        self.envs = [gym.make(env, **env_kwargs) for _ in range(batch_size)]\n    for env_ in self.envs:\n        env_.reset()\n    self.seed()\n\n    env = self.envs[0]\n    self.env = env\n    self.batch_size = batch_size\n    self.batch_first = batch_first\n\n    if seq_len is None:\n        # TODO: infer sequence length from task\n        seq_len = 1000\n\n    obs_shape = env.observation_space.shape\n    action_shape = env.action_space.shape\n    if obs_shape is None or action_shape is None:\n        msg = \"The observation and action spaces must have a shape.\"\n        raise ValueError(msg)\n\n    self._expand_action = len(action_shape) == 0\n\n    if cache_len is None:\n        # Infer cache len\n        cache_len = 1e5  # Probably too low\n        cache_len /= np.prod(obs_shape) + np.prod(action_shape)\n        cache_len /= batch_size\n    cache_len = int((1 + (cache_len // seq_len)) * seq_len)\n\n    self.seq_len = seq_len\n    self._cache_len = cache_len\n\n    if batch_first:\n        shape1, shape2 = [batch_size, seq_len], [batch_size, cache_len]\n    else:\n        shape1, shape2 = [seq_len, batch_size], [cache_len, batch_size]\n\n    self.inputs_shape = shape1 + list(obs_shape)\n    self.target_shape = shape1 + list(action_shape)\n    self._cache_inputs_shape = shape2 + list(obs_shape)\n    self._cache_target_shape = shape2 + list(action_shape)\n\n    self._inputs = np.zeros(\n        self._cache_inputs_shape,\n        dtype=env.observation_space.dtype,\n    )\n    self._target = np.zeros(self._cache_target_shape, dtype=env.action_space.dtype)\n\n    self._cache()\n\n    self._i_batch = 0\n    self.max_batch = max_batch\n</code></pre>"},{"location":"api/utils/#neurogym.utils.info","title":"info","text":"<p>Formatting information about envs and wrappers.</p>"},{"location":"api/utils/#neurogym.utils.info.info","title":"info","text":"<pre><code>info(env=None, show_code=False)\n</code></pre> <p>Script to get envs info.</p> Source code in <code>neurogym/utils/info.py</code> <pre><code>def info(env=None, show_code=False):\n    \"\"\"Script to get envs info.\"\"\"\n    string = \"\"\n    env_name = env\n    env = ngym.make(env)\n    # remove extra wrappers (make can add a OrderEnforcer wrapper)\n    env = env.unwrapped\n    string = env_string(env)\n    # show source code\n    if show_code:\n        string += \"\"\"\\n#### Source code #### \\n\\n\"\"\"\n        env_ref = ALL_ENVS[env_name]\n        from_, class_ = env_ref.split(\":\")\n        imported = getattr(__import__(from_, fromlist=[class_]), class_)\n        lines = inspect.getsource(imported)\n        string += lines + \"\\n\\n\"\n    return string\n</code></pre>"},{"location":"api/utils/#neurogym.utils.info.info_wrapper","title":"info_wrapper","text":"<pre><code>info_wrapper(wrapper=None, show_code=False)\n</code></pre> <p>Script to get wrappers info.</p> Source code in <code>neurogym/utils/info.py</code> <pre><code>def info_wrapper(wrapper=None, show_code=False):\n    \"\"\"Script to get wrappers info.\"\"\"\n    string = \"\"\n\n    wrapp_ref = ALL_WRAPPERS[wrapper]\n    from_, class_ = wrapp_ref.split(\":\")\n    imported = getattr(__import__(from_, fromlist=[class_]), class_)\n    metadata = imported.metadata\n\n    if not isinstance(metadata, dict):\n        metadata = {}\n\n    string += f\"### {wrapper}\\n\\n\"\n    paper_name = metadata.get(\"paper_name\", None)\n    paper_link = metadata.get(\"paper_link\", None)\n    wrapper_description = metadata.get(\"description\", None) or \"Missing description\"\n    string += f\"Logic: {wrapper_description}\\n\\n\"\n    if paper_name is not None:\n        string += \"Reference paper \\n\\n\"\n        if paper_link is None:\n            string += f\"{paper_name}\\n\\n\"\n        else:\n            string += f\"[{paper_name}]({paper_link})\\n\\n\"\n    # add extra info\n    other_info = list(set(metadata.keys()) - set(METADATA_DEF_KEYS))\n    if len(other_info) &gt; 0:\n        string += \"Input parameters: \\n\\n\"\n        for key in other_info:\n            string += f\"{key} : {metadata[key]}\\n\\n\"\n\n    # show source code\n    if show_code:\n        string += \"\"\"\\n#### Source code #### \\n\\n\"\"\"\n        lines = inspect.getsource(imported)\n        string += lines + \"\\n\\n\"\n\n    return string\n</code></pre>"},{"location":"api/utils/#neurogym.utils.info.all_tags","title":"all_tags","text":"<pre><code>all_tags(verbose=0)\n</code></pre> <p>Script to get all tags.</p> Source code in <code>neurogym/utils/info.py</code> <pre><code>def all_tags(verbose=0):\n    \"\"\"Script to get all tags.\"\"\"\n    envs = all_envs()\n    tags = []\n    for env_name in sorted(envs):\n        try:\n            env = ngym.make(env_name)\n            metadata = env.metadata\n            tags += metadata.get(\"tags\", [])\n        except BaseException as e:  # noqa: BLE001, PERF203 # FIXME: unclear which error is expected here.\n            logger.error(f\"Failure in {env_name}\")\n            logger.error(e)\n    tags = set(tags)\n    if verbose:\n        logger.info(\"\\nTAGS:\\n\")\n        for tag in tags:\n            logger.info(tag)\n    return tags\n</code></pre>"},{"location":"api/utils/#neurogym.utils.plotting","title":"plotting","text":"<p>Plotting functions.</p>"},{"location":"api/utils/#neurogym.utils.plotting.plot_env","title":"plot_env","text":"<pre><code>plot_env(\n    env,\n    num_steps=200,\n    num_trials=None,\n    def_act=None,\n    model=None,\n    name=None,\n    legend=True,\n    ob_traces=None,\n    fig_kwargs=None,\n    fname=None,\n    plot_performance=True,\n)\n</code></pre> <p>Plot environment with agent.</p> <p>Parameters:</p> Name Type Description Default <code>env</code> <p>already built neurogym task or name of it</p> required <code>num_steps</code> <p>number of steps to run the task</p> <code>200</code> <code>num_trials</code> <p>if not None, the number of trials to run</p> <code>None</code> <code>def_act</code> <p>if not None (and model=None), the task will be run with the specified action</p> <code>None</code> <code>model</code> <p>if not None, the task will be run with the actions predicted by model, which so far is assumed to be created and trained with the stable-baselines3 toolbox: (https://stable-baselines3.readthedocs.io/en/master/)</p> <code>None</code> <code>name</code> <p>title to show on the rewards panel</p> <code>None</code> <code>legend</code> <p>whether to show the legend for actions panel or not</p> <code>True</code> <code>ob_traces</code> <p>if != [] observations will be plot as traces, with the labels specified by ob_traces</p> <code>None</code> <code>fig_kwargs</code> <p>figure properties admitted by matplotlib.pyplot.subplots() function</p> <code>None</code> <code>fname</code> <p>if not None, save fig or movie to fname</p> <code>None</code> <code>plot_performance</code> <p>whether to show the performance subplot (default: True)</p> <code>True</code> Source code in <code>neurogym/utils/plotting.py</code> <pre><code>def plot_env(\n    env,\n    num_steps=200,\n    num_trials=None,\n    def_act=None,\n    model=None,\n    name=None,\n    legend=True,\n    ob_traces=None,\n    fig_kwargs=None,\n    fname=None,\n    plot_performance=True,\n):\n    \"\"\"Plot environment with agent.\n\n    Args:\n        env: already built neurogym task or name of it\n        num_steps: number of steps to run the task\n        num_trials: if not None, the number of trials to run\n        def_act: if not None (and model=None), the task will be run with the\n            specified action\n        model: if not None, the task will be run with the actions predicted by\n            model, which so far is assumed to be created and trained with the\n            stable-baselines3 toolbox:\n            (https://stable-baselines3.readthedocs.io/en/master/)\n        name: title to show on the rewards panel\n        legend: whether to show the legend for actions panel or not\n        ob_traces: if != [] observations will be plot as traces, with the labels\n            specified by ob_traces\n        fig_kwargs: figure properties admitted by matplotlib.pyplot.subplots() function\n        fname: if not None, save fig or movie to fname\n        plot_performance: whether to show the performance subplot (default: True)\n    \"\"\"\n    # We don't use monitor here because:\n    # 1) env could be already prewrapped with monitor\n    # 2) monitor will save data and so the function will need a folder\n    if fig_kwargs is None:\n        fig_kwargs = {}\n    if ob_traces is None:\n        ob_traces = []\n    if isinstance(env, str):\n        env = gym.make(env)\n    if name is None:\n        name = type(env).__name__\n    data = run_env(\n        env=env,\n        num_steps=num_steps,\n        num_trials=num_trials,\n        def_act=def_act,\n        model=model,\n    )\n    # Find trial start steps (0-based)\n    trial_starts_step_indices = np.where(np.array(data[\"actions_end_of_trial\"]) != -1)[0] + 1\n    # Shift again for plotting (since steps are 1-based)\n    trial_starts_axis = trial_starts_step_indices + 1\n\n    return fig_(\n        data[\"ob\"],\n        data[\"actions\"],\n        gt=data[\"gt\"],\n        rewards=data[\"rewards\"],\n        legend=legend,\n        performance=data[\"perf\"] if plot_performance else None,\n        states=data[\"states\"],\n        name=name,\n        ob_traces=ob_traces,\n        fig_kwargs=fig_kwargs,\n        env=env,\n        initial_ob=data[\"initial_ob\"],\n        fname=fname,\n        trial_starts=trial_starts_axis,\n    )\n</code></pre>"},{"location":"api/utils/#neurogym.utils.plotting.fig_","title":"fig_","text":"<pre><code>fig_(\n    ob,\n    actions,\n    gt=None,\n    rewards=None,\n    performance=None,\n    states=None,\n    legend=True,\n    ob_traces=None,\n    name=\"\",\n    fname=None,\n    fig_kwargs=None,\n    env=None,\n    initial_ob=None,\n    trial_starts=None,\n)\n</code></pre> <p>Visualize a run in a simple environment.</p> <p>Parameters:</p> Name Type Description Default <code>ob</code> <p>np array of observation (n_step, n_unit)</p> required <code>actions</code> <p>np array of action (n_step, n_unit)</p> required <code>gt</code> <p>np array of groud truth</p> <code>None</code> <code>rewards</code> <p>np array of rewards</p> <code>None</code> <code>performance</code> <p>np array of performance (if set to <code>None</code> performance plotting will be skipped)</p> <code>None</code> <code>states</code> <p>np array of network states</p> <code>None</code> <code>name</code> <p>title to show on the rewards panel and name to save figure</p> <code>''</code> <code>fname</code> <p>if != '', where to save the figure</p> <code>None</code> <code>legend</code> <p>whether to show the legend for actions panel or not</p> <code>True</code> <code>ob_traces</code> <p>None or list. If list, observations will be plot as traces, with the labels specified by ob_traces</p> <code>None</code> <code>fig_kwargs</code> <p>figure properties admitted by matplotlib.pyplot.subplots() function</p> <code>None</code> <code>env</code> <p>environment class for extra information</p> <code>None</code> <code>initial_ob</code> <p>initial observation to be used to align with actions</p> <code>None</code> <code>trial_starts</code> <p>list of trial start indices, 1-based</p> <code>None</code> Source code in <code>neurogym/utils/plotting.py</code> <pre><code>def fig_(\n    ob,\n    actions,\n    gt=None,\n    rewards=None,\n    performance=None,\n    states=None,\n    legend=True,\n    ob_traces=None,\n    name=\"\",\n    fname=None,\n    fig_kwargs=None,\n    env=None,\n    initial_ob=None,\n    trial_starts=None,\n):\n    \"\"\"Visualize a run in a simple environment.\n\n    Args:\n        ob: np array of observation (n_step, n_unit)\n        actions: np array of action (n_step, n_unit)\n        gt: np array of groud truth\n        rewards: np array of rewards\n        performance: np array of performance (if set to `None` performance plotting will be skipped)\n        states: np array of network states\n        name: title to show on the rewards panel and name to save figure\n        fname: if != '', where to save the figure\n        legend: whether to show the legend for actions panel or not\n        ob_traces: None or list.\n            If list, observations will be plot as traces, with the labels\n            specified by ob_traces\n        fig_kwargs: figure properties admitted by matplotlib.pyplot.subplots() function\n        env: environment class for extra information\n        initial_ob: initial observation to be used to align with actions\n        trial_starts: list of trial start indices, 1-based\n    \"\"\"\n    if fig_kwargs is None:\n        fig_kwargs = {}\n    ob = np.array(ob)\n    actions = np.array(actions)\n\n    if initial_ob is None:\n        initial_ob = ob[0].copy()\n\n    # Align observation with actions by inserting an initial obs from env\n    ob = np.insert(ob, 0, initial_ob, axis=0)\n    # Trim last obs to match actions\n    ob = ob[:-1]\n\n    if len(ob.shape) == 2:\n        return plot_env_1dbox(\n            ob,\n            actions,\n            gt=gt,\n            rewards=rewards,\n            performance=performance,\n            states=states,\n            legend=legend,\n            ob_traces=ob_traces,\n            name=name,\n            fname=fname,\n            fig_kwargs=fig_kwargs,\n            env=env,\n            trial_starts=trial_starts,\n        )\n    if len(ob.shape) == 4:\n        return plot_env_3dbox(ob, fname=fname, env=env)\n\n    msg = f\"{ob.shape=} not supported.\"\n    raise ValueError(msg)\n</code></pre>"},{"location":"api/utils/#neurogym.utils.plotting.plot_env_1dbox","title":"plot_env_1dbox","text":"<pre><code>plot_env_1dbox(\n    ob,\n    actions,\n    gt=None,\n    rewards=None,\n    performance=None,\n    states=None,\n    legend=True,\n    ob_traces=None,\n    name=\"\",\n    fname=None,\n    fig_kwargs=None,\n    env=None,\n    trial_starts=None,\n)\n</code></pre> <p>Plot environment with 1-D Box observation space.</p> Source code in <code>neurogym/utils/plotting.py</code> <pre><code>def plot_env_1dbox(\n    ob,\n    actions,\n    gt=None,\n    rewards=None,\n    performance=None,\n    states=None,\n    legend=True,\n    ob_traces=None,\n    name=\"\",\n    fname=None,\n    fig_kwargs=None,\n    env=None,\n    trial_starts=None,\n):\n    \"\"\"Plot environment with 1-D Box observation space.\"\"\"\n    if fig_kwargs is None:\n        fig_kwargs = {}\n    if len(ob.shape) != 2:\n        msg = \"ob has to be 2-dimensional.\"\n        raise ValueError(msg)\n    steps = np.arange(1, ob.shape[0] + 1)\n\n    n_row = 2  # observation and action\n    n_row += rewards is not None\n    n_row += performance is not None\n    n_row += states is not None\n\n    gt_colors = \"gkmcry\"\n    if not fig_kwargs:\n        fig_kwargs = {\"sharex\": True, \"figsize\": (6, n_row * 1.2)}\n    f, axes = plt.subplots(n_row, 1, **fig_kwargs)\n    i_ax = 0\n\n    # Plot observation\n    ax = axes[i_ax]\n    i_ax += 1\n    if ob_traces:\n        if len(ob_traces) != ob.shape[1]:\n            msg = f\"Please provide label for each of the {ob.shape[1]} traces in the observations.\"\n            raise ValueError(msg)\n\n        # Plot all traces first\n        for ind_tr, tr in enumerate(ob_traces):\n            ax.plot(steps, ob[:, ind_tr], label=tr)\n\n        ax.spines[\"top\"].set_visible(False)\n        ax.spines[\"right\"].set_visible(False)\n\n        # Compute ticks and labels\n        yticks = []\n        yticklabels = []\n\n        # Find fixation index (if exists)\n        fix_idx = next((i for i, tr in enumerate(ob_traces) if \"fix\" in tr.lower()), None)\n\n        if fix_idx is not None:\n            yticks.append(np.max(ob[:, fix_idx]))\n            yticklabels.append(\"Fix. Cue\")\n\n            # All other indices are stimuli\n            stim_means = [np.mean(ob[:, i]) for i in range(len(ob_traces)) if i != fix_idx]\n            if stim_means:\n                yticks.append(np.mean(stim_means))\n                yticklabels.append(\"Stimuli\")\n        else:\n            # No fixation, all are stimuli\n            yticks.append(np.mean([np.mean(ob[:, i]) for i in range(len(ob_traces))]))\n            yticklabels.append(\"Stimuli\")\n\n        if legend:\n            ax.legend(loc=\"upper right\")\n        if trial_starts is not None:\n            for t_start in trial_starts:\n                ax.axvline(t_start, linestyle=\"--\", color=\"grey\", alpha=0.7)\n        ax.set_xlim([0.5, len(steps) + 1])\n        ax.set_yticks(yticks)\n        ax.set_yticklabels(yticklabels)\n    else:\n        ax.imshow(ob.T, aspect=\"auto\", origin=\"lower\")\n        if env and hasattr(env.observation_space, \"name\"):\n            # Plot environment annotation\n            yticks = []\n            yticklabels = []\n            for key, val in env.observation_space.name.items():\n                yticks.append((np.min(val) + np.max(val)) / 2)\n                yticklabels.append(key)\n            ax.set_yticks(yticks)\n            ax.set_yticklabels(yticklabels)\n        else:\n            ax.set_yticks([])\n        ax.spines[\"top\"].set_visible(False)\n        ax.spines[\"bottom\"].set_visible(False)\n        ax.spines[\"right\"].set_visible(False)\n\n    if name:\n        ax.set_title(f\"{name} env\")\n    ax.set_ylabel(\"Obs.\")\n    # Show step numbers on x-axis\n    ax.set_xticks(np.arange(0, len(steps) + 1, 5))\n    ax.set_xticklabels(np.arange(0, len(steps) + 1, 5))\n    # Add gray background grid with white lines\n    _set_grid_style(ax)\n\n    # Plot actions\n    ax = axes[i_ax]\n    i_ax += 1\n    if len(actions.shape) &gt; 1:\n        # Changes not implemented yet\n        ax.plot(steps, actions, marker=\"+\", label=\"Actions\")\n    else:\n        ax.plot(steps, actions, marker=\"+\", label=\"Actions\")\n    if gt is not None:\n        gt = np.array(gt)\n        if len(gt.shape) &gt; 1:\n            for ind_gt in range(gt.shape[1]):\n                ax.plot(\n                    steps,\n                    gt[:, ind_gt],\n                    f\"--{gt_colors[ind_gt]}\",\n                    label=f\"Ground truth {ind_gt}\",\n                )\n        else:\n            ax.plot(steps, gt, f\"--{gt_colors[0]}\", label=\"Ground truth\")\n    if trial_starts is not None:\n        for t_start in trial_starts:\n            ax.axvline(t_start, linestyle=\"--\", color=\"grey\", alpha=0.7)\n    ax.set_xlim([0.5, len(steps) + 1])\n    ax.set_ylabel(\"Act.\")\n    ax.spines[\"top\"].set_visible(False)\n    ax.spines[\"right\"].set_visible(False)\n    if legend:\n        ax.legend(loc=\"upper right\")\n    if env and hasattr(env.action_space, \"name\"):\n        yticks = []\n        yticklabels = []\n        for key, val in env.action_space.name.items():\n            if isinstance(val, list | tuple | np.ndarray | range):\n                for v in val:\n                    yticks.append(v)\n                    yticklabels.append(f\"{key}_{v}\")\n            else:  # single int\n                yticks.append(val)\n                yticklabels.append(key)\n        ax.set_yticks(yticks)\n        ax.set_yticklabels(yticklabels)\n    # Show step numbers on x-axis\n    ax.set_xticks(np.arange(0, len(steps) + 1, 5))\n    ax.set_xticklabels(np.arange(0, len(steps) + 1, 5))\n    # Add gray background grid with white lines\n    _set_grid_style(ax)\n\n    # Plot rewards if provided\n    if rewards is not None:\n        ax = axes[i_ax]\n        i_ax += 1\n        ax.plot(steps, rewards, \"r\", label=\"Rewards\")\n        ax.set_ylabel(\"Rew.\")\n        ax.spines[\"top\"].set_visible(False)\n        ax.spines[\"right\"].set_visible(False)\n        if legend:\n            ax.legend(loc=\"upper right\")\n        if trial_starts is not None:\n            for t_start in trial_starts:\n                ax.axvline(t_start, linestyle=\"--\", color=\"grey\", alpha=0.7)\n        ax.set_xlim([0.5, len(steps) + 1])\n\n        if env and hasattr(env, \"rewards\") and env.rewards is not None:\n            yticks = []\n            yticklabels = []\n\n            if isinstance(env.rewards, dict):\n                for key, val in env.rewards.items():\n                    yticks.append(val)\n                    yticklabels.append(f\"{str(key)[:5].title()} {val:0.2f}\")\n            else:\n                for val in env.rewards:\n                    yticks.append(val)\n                    yticklabels.append(f\"{val:0.2f}\")\n\n            ax.set_yticks(yticks)\n            ax.set_yticklabels(yticklabels)\n        # Show step numbers on x-axis\n        ax.set_xticks(np.arange(0, len(steps) + 1, 5))\n        ax.set_xticklabels(np.arange(0, len(steps) + 1, 5))\n        # Add gray background grid with white lines\n        _set_grid_style(ax)\n\n    # Plot performance if provided\n    if performance is not None:\n        ax = axes[i_ax]\n        i_ax += 1\n        ax.plot(steps, performance, \"k\", label=\"Performance\")\n        ax.set_ylabel(\"Performance\")\n        performance = np.array(performance)\n        mean_perf = np.mean(performance[performance != -1])\n        ax.set_title(f\"Mean performance: {np.round(mean_perf, 2)}\")\n        ax.spines[\"top\"].set_visible(False)\n        ax.spines[\"right\"].set_visible(False)\n        if legend:\n            ax.legend(loc=\"upper right\")\n        if trial_starts is not None:\n            for t_start in trial_starts:\n                ax.axvline(t_start, linestyle=\"--\", color=\"grey\", alpha=0.7)\n        ax.set_xlim([0.5, len(steps) + 1])\n        # Add gray background grid with white lines\n        _set_grid_style(ax)\n\n    # Plot states if provided\n    if states is not None:\n        if performance is not None or rewards is not None:\n            # Show step numbers on x-axis\n            ax.set_xticks(np.arange(0, len(steps) + 1, 5))\n            ax.set_xticklabels(np.arange(0, len(steps) + 1, 5))\n        ax = axes[i_ax]\n        i_ax += 1\n        plt.imshow(states[:, int(states.shape[1] / 2) :].T, aspect=\"auto\")\n        ax.set_title(\"Activity\")\n        ax.set_ylabel(\"Neurons\")\n        ax.spines[\"top\"].set_visible(False)\n        ax.spines[\"right\"].set_visible(False)\n\n    ax.set_xlabel(\"Steps\")\n    plt.tight_layout()\n    if fname:\n        fname = str(fname)\n        if not (fname.endswith((\".png\", \".svg\"))):\n            fname += \".png\"\n        f.savefig(fname, dpi=300)\n        plt.close(f)\n    return f\n</code></pre>"},{"location":"api/utils/#neurogym.utils.plotting.plot_env_3dbox","title":"plot_env_3dbox","text":"<pre><code>plot_env_3dbox(ob, fname='', env=None) -&gt; None\n</code></pre> <p>Plot environment with 3-D Box observation space.</p> Source code in <code>neurogym/utils/plotting.py</code> <pre><code>def plot_env_3dbox(ob, fname=\"\", env=None) -&gt; None:\n    \"\"\"Plot environment with 3-D Box observation space.\"\"\"\n    ob = ob.astype(np.uint8)  # TODO: Temporary\n    fig = plt.figure()\n    ax = fig.add_axes((0.1, 0.1, 0.8, 0.8))\n    ax.axis(\"off\")\n    im = ax.imshow(ob[0], animated=True)\n\n    def animate(i, *args, **kwargs):\n        im.set_array(ob[i])\n        return (im,)\n\n    interval = env.dt if env is not None else 50\n    ani = animation.FuncAnimation(fig, animate, frames=ob.shape[0], interval=interval)\n    if fname:\n        writer = animation.writers[\"ffmpeg\"](fps=int(1000 / interval))\n        fname = str(fname)\n        if not fname.endswith(\".mp4\"):\n            fname += \".mp4\"\n        ani.save(fname, writer=writer, dpi=300)\n</code></pre>"},{"location":"api/utils/#neurogym.utils.ngym_random","title":"ngym_random","text":""},{"location":"api/utils/#neurogym.utils.ngym_random.TruncExp","title":"TruncExp","text":"<pre><code>TruncExp(vmean, vmin=0, vmax=inf)\n</code></pre> Source code in <code>neurogym/utils/ngym_random.py</code> <pre><code>def __init__(self, vmean, vmin=0, vmax=np.inf) -&gt; None:\n    self.vmean = vmean\n    self.vmin = vmin\n    self.vmax = vmax\n    self.rng = np.random.RandomState()\n</code></pre>"},{"location":"api/utils/#neurogym.utils.ngym_random.TruncExp.seed","title":"seed","text":"<pre><code>seed(seed=None) -&gt; None\n</code></pre> <p>Seed the PRNG of this space.</p> Source code in <code>neurogym/utils/ngym_random.py</code> <pre><code>def seed(self, seed=None) -&gt; None:\n    \"\"\"Seed the PRNG of this space.\"\"\"\n    self.rng = np.random.RandomState(seed)\n</code></pre>"},{"location":"api/utils/#neurogym.utils.ngym_random.trunc_exp","title":"trunc_exp","text":"<pre><code>trunc_exp(rng, vmean, vmin=0, vmax=inf)\n</code></pre> <p>Function for generating period durations.</p> Source code in <code>neurogym/utils/ngym_random.py</code> <pre><code>def trunc_exp(rng, vmean, vmin=0, vmax=np.inf):\n    \"\"\"Function for generating period durations.\"\"\"\n    if vmin &gt;= vmax:  # the &gt; is to avoid issues when making vmin as big as dt\n        return vmax\n    while True:\n        x = rng.exponential(vmean)\n        if vmin &lt;= x &lt; vmax:\n            return x\n</code></pre>"},{"location":"api/utils/#neurogym.utils.ngym_random.random_number_fn","title":"random_number_fn","text":"<pre><code>random_number_fn(dist, args, rng)\n</code></pre> <p>Return a random number generating function from a distribution.</p> Source code in <code>neurogym/utils/ngym_random.py</code> <pre><code>def random_number_fn(dist, args, rng):\n    \"\"\"Return a random number generating function from a distribution.\"\"\"\n    if dist == \"uniform\":\n        return lambda: rng.uniform(*args)\n    if dist == \"choice\":\n        return lambda: rng.choice(args)\n    if dist == \"truncated_exponential\":\n        return lambda: trunc_exp(rng, *args)\n    if dist == \"constant\":\n        return lambda: args\n    msg = f\"Unknown distribution: {dist}.\"\n    raise ValueError(msg)\n</code></pre>"},{"location":"api/utils/#neurogym.utils.ngym_random.random_number_name","title":"random_number_name","text":"<pre><code>random_number_name(dist, args)\n</code></pre> <p>Return a string explaining the dist and args.</p> Source code in <code>neurogym/utils/ngym_random.py</code> <pre><code>def random_number_name(dist, args):\n    \"\"\"Return a string explaining the dist and args.\"\"\"\n    if dist == \"uniform\":\n        return f\"{dist} between {args[0]} and {args[1]}\"\n    if dist == \"choice\":\n        return f\"{dist} within {args}\"\n    if dist == \"truncated_exponential\":\n        string = f\"truncated exponential with mean {args[0]}\"\n        if len(args) &gt; 1:\n            string += f\", min {args[1]}\"\n        if len(args) &gt; 2:\n            string += f\", max {args[2]}\"\n        return string\n    if dist == \"constant\":\n        return f\"dist{args}\"\n    msg = f\"Unknown distribution: {dist}.\"\n    raise ValueError(msg)\n</code></pre>"},{"location":"api/utils/#neurogym.utils.scheduler","title":"scheduler","text":"<p>Trial scheduler class.</p>"},{"location":"api/utils/#neurogym.utils.scheduler.BaseSchedule","title":"BaseSchedule","text":"<pre><code>BaseSchedule(n)\n</code></pre> <p>Base schedule.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <p>int, number of conditions to schedule</p> required Source code in <code>neurogym/utils/scheduler.py</code> <pre><code>def __init__(self, n) -&gt; None:\n    self.n = n\n    self.total_count = 0  # total count\n    self.count = 0  # count within a condition\n    self.i = 0  # initialize at 0\n    self.rng = np.random.RandomState()\n</code></pre>"},{"location":"api/utils/#neurogym.utils.scheduler.SequentialSchedule","title":"SequentialSchedule","text":"<pre><code>SequentialSchedule(n)\n</code></pre> <p>               Bases: <code>BaseSchedule</code></p> <p>Sequential schedules.</p> Source code in <code>neurogym/utils/scheduler.py</code> <pre><code>def __init__(self, n) -&gt; None:\n    super().__init__(n)\n</code></pre>"},{"location":"api/utils/#neurogym.utils.scheduler.RandomSchedule","title":"RandomSchedule","text":"<pre><code>RandomSchedule(n)\n</code></pre> <p>               Bases: <code>BaseSchedule</code></p> <p>Random schedules.</p> Source code in <code>neurogym/utils/scheduler.py</code> <pre><code>def __init__(self, n) -&gt; None:\n    super().__init__(n)\n</code></pre>"},{"location":"api/utils/#neurogym.utils.scheduler.SequentialBlockSchedule","title":"SequentialBlockSchedule","text":"<pre><code>SequentialBlockSchedule(n, block_lens)\n</code></pre> <p>               Bases: <code>BaseSchedule</code></p> <p>Sequential block schedules.</p> Source code in <code>neurogym/utils/scheduler.py</code> <pre><code>def __init__(self, n, block_lens) -&gt; None:\n    super().__init__(n)\n    self.block_lens = block_lens\n    if len(block_lens) != n:\n        msg = f\"{len(block_lens)=} must be equal to {n=}.\"\n        raise ValueError(msg)\n</code></pre>"},{"location":"api/utils/#neurogym.utils.scheduler.RandomBlockSchedule","title":"RandomBlockSchedule","text":"<pre><code>RandomBlockSchedule(n, block_lens)\n</code></pre> <p>               Bases: <code>BaseSchedule</code></p> <p>Random block schedules.</p> Source code in <code>neurogym/utils/scheduler.py</code> <pre><code>def __init__(self, n, block_lens) -&gt; None:\n    super().__init__(n)\n    self.block_lens = block_lens\n    if len(block_lens) != n:\n        msg = f\"{len(block_lens)=} must be equal to {n=}.\"\n        raise ValueError(msg)\n</code></pre>"},{"location":"api/utils/#neurogym.utils.spaces","title":"spaces","text":""},{"location":"api/utils/#neurogym.utils.spaces.Box","title":"Box","text":"<pre><code>Box(low, high, name=None, **kwargs)\n</code></pre> <p>               Bases: <code>Box</code></p> <p>Thin wrapper of gymnasium.spaces.Box.</p> <p>Allow the user to give names to each dimension of the Box.</p> <p>Parameters:</p> Name Type Description Default <code>low,</code> <code>(high, kwargs)</code> <p>see gymnasium.spaces.Box</p> required <code>name</code> <p>dict describing the name of different dimensions</p> <code>None</code> Example usage <p>observation_space = Box(low=0, high=1,                         name={'fixation': 0, 'stimulus': [1, 2]})</p> Source code in <code>neurogym/utils/spaces.py</code> <pre><code>def __init__(self, low, high, name=None, **kwargs) -&gt; None:\n    super().__init__(low, high, **kwargs)\n    if isinstance(name, dict):\n        self.name = name\n    elif name is not None:\n        msg = f\"{type(name)=} must be `dict` or `NoneType`.\"\n        raise TypeError(msg)\n</code></pre>"},{"location":"api/utils/#neurogym.utils.spaces.Discrete","title":"Discrete","text":"<pre><code>Discrete(n, name=None, **kwargs)\n</code></pre> <p>               Bases: <code>Discrete</code></p> <p>Thin wrapper of gymnasium.spaces.Discrete.</p> <p>Allow the user to give names to each dimension of the Discrete space.</p> <p>Parameters:</p> Name Type Description Default <code>low,</code> <code>(high, kwargs)</code> <p>see gymnasium.spaces.Box</p> required <code>name</code> <p>dict describing the name of different dimensions</p> <code>None</code> Example usage <p>observation_space = Discrete(n=3, name={'fixation': 0, 'stimulus': [1, 2]})</p> Source code in <code>neurogym/utils/spaces.py</code> <pre><code>def __init__(self, n, name=None, **kwargs) -&gt; None:\n    super().__init__(n)\n    if isinstance(name, dict):\n        self.name = name\n    elif name is not None:\n        msg = f\"{type(name)=} must be `dict` or `NoneType`.\"\n        raise TypeError(msg)\n</code></pre>"},{"location":"api/utils/#neurogym.utils.tasktools","title":"tasktools","text":""},{"location":"api/utils/#neurogym.utils.tasktools.to_map","title":"to_map","text":"<pre><code>to_map(*args)\n</code></pre> <p>Produces ordered dict from given inputs.</p> Source code in <code>neurogym/utils/tasktools.py</code> <pre><code>def to_map(*args):\n    \"\"\"Produces ordered dict from given inputs.\"\"\"\n    var_list = args[0] if isinstance(args[0], list) else args\n    od = OrderedDict()\n    for i, v in enumerate(var_list):\n        od[v] = i\n\n    return od\n</code></pre>"},{"location":"api/utils/#neurogym.utils.tasktools.get_idx","title":"get_idx","text":"<pre><code>get_idx(t, start_end)\n</code></pre> <p>Auxiliary function for defining task periods.</p> Source code in <code>neurogym/utils/tasktools.py</code> <pre><code>def get_idx(t, start_end):\n    \"\"\"Auxiliary function for defining task periods.\"\"\"\n    start, end = start_end\n    return list(np.where((start &lt;= t) &amp; (t &lt; end))[0])\n</code></pre>"},{"location":"api/utils/#neurogym.utils.tasktools.get_periods_idx","title":"get_periods_idx","text":"<pre><code>get_periods_idx(dt, periods)\n</code></pre> <p>Function for defining task periods.</p> Source code in <code>neurogym/utils/tasktools.py</code> <pre><code>def get_periods_idx(dt, periods):\n    \"\"\"Function for defining task periods.\"\"\"\n    t = np.linspace(0, periods[\"tmax\"], int(periods[\"tmax\"] / dt) + 1)\n\n    return t, {k: get_idx(t, v) for k, v in periods.items() if k != \"tmax\"}\n</code></pre>"},{"location":"api/utils/#neurogym.utils.tasktools.minmax_number","title":"minmax_number","text":"<pre><code>minmax_number(dist, args)\n</code></pre> <p>Given input to the random_number_fn function, return min and max.</p> Source code in <code>neurogym/utils/tasktools.py</code> <pre><code>def minmax_number(dist, args):\n    \"\"\"Given input to the random_number_fn function, return min and max.\"\"\"\n    if dist == \"uniform\":\n        return args[0], args[1]\n    if dist == \"choice\":\n        return np.min(args), np.max(args)\n    if dist == \"truncated_exponential\":\n        return args[1], args[2]\n    if dist == \"constant\":\n        return args, args\n    msg = f\"Unknown distribution: {dist}.\"\n    raise ValueError(msg)\n</code></pre>"},{"location":"api/utils/#neurogym.utils.tasktools.circular_dist","title":"circular_dist","text":"<pre><code>circular_dist(original_dist)\n</code></pre> <p>Get the distance in periodic boundary conditions.</p> Source code in <code>neurogym/utils/tasktools.py</code> <pre><code>def circular_dist(original_dist):\n    \"\"\"Get the distance in periodic boundary conditions.\"\"\"\n    return np.minimum(abs(original_dist), 2 * np.pi - abs(original_dist))\n</code></pre>"},{"location":"api/utils/#neurogym.utils.tasktools.correct_2AFC","title":"correct_2AFC","text":"<pre><code>correct_2AFC(perf)\n</code></pre> <p>Computes performance.</p> Source code in <code>neurogym/utils/tasktools.py</code> <pre><code>def correct_2AFC(perf):  # noqa: N802\n    \"\"\"Computes performance.\"\"\"\n    p_decision = perf.n_decision / perf.n_trials\n    p_correct = divide(perf.n_correct, perf.n_decision)\n\n    return p_decision, p_correct\n</code></pre>"},{"location":"api/wrappers/","title":"Wrappers","text":""},{"location":"api/wrappers/#neurogym.wrappers.block","title":"block","text":""},{"location":"api/wrappers/#neurogym.wrappers.block.ScheduleAttr","title":"ScheduleAttr","text":"<pre><code>ScheduleAttr(env, schedule, attr_list)\n</code></pre> <p>               Bases: <code>TrialWrapper</code></p> <p>Schedule attributes.</p> <p>Parameters:</p> Name Type Description Default <code>env</code> <p>TrialEnv object</p> required <code>schedule</code> required Source code in <code>neurogym/wrappers/block.py</code> <pre><code>def __init__(self, env, schedule, attr_list) -&gt; None:\n    super().__init__(env)\n    self.schedule = schedule\n    self.attr_list = attr_list\n</code></pre>"},{"location":"api/wrappers/#neurogym.wrappers.block.MultiEnvs","title":"MultiEnvs","text":"<pre><code>MultiEnvs(envs, env_input=False)\n</code></pre> <p>               Bases: <code>TrialWrapper</code></p> <p>Wrap multiple environments.</p> <p>Parameters:</p> Name Type Description Default <code>envs</code> <p>list of env object</p> required <code>env_input</code> <p>bool, if True, add scalar inputs indicating current envinronment. default False.</p> <code>False</code> Source code in <code>neurogym/wrappers/block.py</code> <pre><code>def __init__(self, envs, env_input=False) -&gt; None:\n    super().__init__(envs[0])\n    for env in envs:\n        env.unwrapped.set_top(self)\n    self.envs = envs\n    self.i_env = 0\n\n    self.env_input = env_input\n    if env_input:\n        env_shape = envs[0].observation_space.shape\n        if len(env_shape) &gt; 1:\n            msg = f\"Env must have 1-D Box shape but got {env_shape}.\"\n            raise ValueError(msg)\n        _have_equal_shape(envs)\n        self.observation_space: spaces.Box = spaces.Box(  # type: ignore[override]\n            -np.inf,\n            np.inf,\n            shape=(env_shape[0] + len(self.envs),),\n            dtype=self.envs[0].observation_space.dtype,\n        )\n</code></pre>"},{"location":"api/wrappers/#neurogym.wrappers.block.MultiEnvs.set_i","title":"set_i","text":"<pre><code>set_i(i) -&gt; None\n</code></pre> <p>Set the i-th environment.</p> Source code in <code>neurogym/wrappers/block.py</code> <pre><code>def set_i(self, i) -&gt; None:\n    \"\"\"Set the i-th environment.\"\"\"\n    self.i_env = i\n    self.env = self.envs[self.i_env]\n</code></pre>"},{"location":"api/wrappers/#neurogym.wrappers.block.ScheduleEnvs","title":"ScheduleEnvs","text":"<pre><code>ScheduleEnvs(envs, schedule, env_input=False)\n</code></pre> <p>               Bases: <code>TrialWrapper</code></p> <p>Schedule environments.</p> <p>Parameters:</p> Name Type Description Default <code>envs</code> <p>list of env object</p> required <code>schedule</code> <p>utils.scheduler.BaseSchedule object</p> required <code>env_input</code> <p>bool, if True, add scalar inputs indicating current environment. default False.</p> <code>False</code> Source code in <code>neurogym/wrappers/block.py</code> <pre><code>def __init__(self, envs, schedule, env_input=False) -&gt; None:\n    super().__init__(envs[0])\n    for env in envs:\n        env.unwrapped.set_top(self)\n    self.envs = envs\n    self.schedule = schedule\n    self.i_env = self.next_i_env = 0\n\n    self.env_input = env_input\n    if env_input:\n        env_shape = envs[0].observation_space.shape\n        if len(env_shape) &gt; 1:\n            msg = f\"Env must have 1-D Box shape but got {env_shape}.\"\n            raise ValueError(msg)\n        _have_equal_shape(envs)\n        self.observation_space: spaces.Box = spaces.Box(  # type: ignore[override]\n            -np.inf,\n            np.inf,\n            shape=(env_shape[0] + len(self.envs),),\n            dtype=np.float32,\n        )\n</code></pre>"},{"location":"api/wrappers/#neurogym.wrappers.block.ScheduleEnvs.reset","title":"reset","text":"<pre><code>reset(**kwargs)\n</code></pre> <p>Resets environments.</p> <p>Reset each environment in self.envs and use the scheduler to select the environment returning the initial observation. This environment is also used to set the current environment self.env.</p> Source code in <code>neurogym/wrappers/block.py</code> <pre><code>def reset(self, **kwargs):\n    # TODO: kwargs to specify the condition for new_trial\n    \"\"\"Resets environments.\n\n    Reset each environment in self.envs and use the scheduler to select the environment returning\n    the initial observation. This environment is also used to set the current environment self.env.\n    \"\"\"\n    self.schedule.reset()\n    return_i_env = self.schedule()\n\n    # first reset all the env excepted return_i_env\n    for i, env in enumerate(self.envs):\n        if i == return_i_env:\n            continue\n\n        # change the current env so that calling _top.new_trial() in env.reset() will generate a trial for the env\n        # being currently reset (and not an env that is not yet reset)\n        self.set_i(i)\n        # same env used here and in the first call to new_trial()\n        self.next_i_env = self.i_env\n\n        env.reset(**kwargs)\n\n    # then reset return_i_env and return the result\n    self.set_i(return_i_env)\n    self.next_i_env = self.i_env\n    return self.env.reset()\n</code></pre>"},{"location":"api/wrappers/#neurogym.wrappers.block.ScheduleEnvs.set_i","title":"set_i","text":"<pre><code>set_i(i) -&gt; None\n</code></pre> <p>Set the current environment to the i-th environment in the list envs.</p> Source code in <code>neurogym/wrappers/block.py</code> <pre><code>def set_i(self, i) -&gt; None:\n    \"\"\"Set the current environment to the i-th environment in the list envs.\"\"\"\n    self.i_env = i\n    self.env = self.envs[self.i_env]\n    self.schedule.i = i\n</code></pre>"},{"location":"api/wrappers/#neurogym.wrappers.block.TrialHistoryV2","title":"TrialHistoryV2","text":"<pre><code>TrialHistoryV2(env, probs=None)\n</code></pre> <p>               Bases: <code>TrialWrapper</code></p> <p>Change ground truth probability based on previous outcome.</p> <p>Parameters:</p> Name Type Description Default <code>probs</code> <p>matrix of probabilities of the current choice conditioned on the previous. Shape, num-choices x num-choices</p> <code>None</code> Source code in <code>neurogym/wrappers/block.py</code> <pre><code>def __init__(self, env, probs=None) -&gt; None:\n    super().__init__(env)\n    try:\n        self.n_ch = len(self.choices)  # max num of choices\n    except AttributeError as e:\n        msg = \"TrialHistory requires task to have attribute choices.\"\n        raise AttributeError(msg) from e\n    if probs is None:\n        probs = np.ones((self.n_ch, self.n_ch)) / self.n_ch  # uniform\n    self.probs = probs\n    if self.probs.shape != (self.n_ch, self.n_ch):\n        msg = f\"{self.probs.shape=} should be {self.n_ch, self.n_ch=}.\"\n        raise ValueError(msg)\n    self.prev_trial = self.rng.choice(self.n_ch)  # random initialization\n</code></pre>"},{"location":"api/wrappers/#neurogym.wrappers.monitor","title":"monitor","text":""},{"location":"api/wrappers/#neurogym.wrappers.monitor.Monitor","title":"Monitor","text":"<pre><code>Monitor(\n    env: TrialEnv,\n    config: Config | str | Path | None = None,\n    name: str | None = None,\n    trigger: str = \"trial\",\n    interval: int = 1000,\n    verbose: bool = True,\n    plot_create: bool = False,\n    plot_steps: int = 1000,\n    ext: str = \"png\",\n    step_fn: Callable | None = None,\n)\n</code></pre> <p>               Bases: <code>Wrapper</code></p> <p>Monitor class to log, visualize, and evaluate NeuroGym environment behavior.</p> <p>Wraps a NeuroGym TrialEnv to track actions, rewards, and performance metrics, save them to disk, and optionally generate trial visualizations. Supports logging at trial or step level, with configurable frequency and verbosity.</p> <p>Parameters:</p> Name Type Description Default <code>env</code> <code>TrialEnv</code> <p>The NeuroGym environment to wrap.</p> required <code>config</code> <code>Config | str | Path | None</code> <p>Optional configuration source (Config object, TOML file path, or dictionary).</p> <code>None</code> <code>name</code> <code>str | None</code> <p>Optional monitor name; defaults to the environment class name.</p> <code>None</code> <code>trigger</code> <code>str</code> <p>When to save data (\"trial\" or \"step\").</p> <code>'trial'</code> <code>interval</code> <code>int</code> <p>How often to save data, in number of trials or steps.</p> <code>1000</code> <code>plot_create</code> <code>bool</code> <p>Whether to generate and save visualizations of environment behavior.</p> <code>False</code> <code>plot_steps</code> <code>int</code> <p>Number of steps to visualize in each plot.</p> <code>1000</code> <code>ext</code> <code>str</code> <p>Image file extension for saved plots (e.g., \"png\").</p> <code>'png'</code> <code>step_fn</code> <code>Callable | None</code> <p>Optional custom step function to override the environment's.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Whether to log information when logging or saving data.</p> <code>True</code> <code>level</code> <p>Logging verbosity level (e.g., \"INFO\", \"DEBUG\").</p> required <code>log_trigger</code> <p>When to log progress (\"trial\" or \"step\").</p> required <code>log_interval</code> <p>How often to log, in trials or steps.</p> required <p>Attributes:</p> Name Type Description <code>config</code> <code>Config</code> <p>Final validated configuration object.</p> <code>data</code> <code>dict[str, list]</code> <p>Collected behavioral data for each completed trial.</p> <code>data_eval</code> <code>dict[str, Any]</code> <p>Evaluation data collected during policy evaluation runs.</p> <code>cum_reward</code> <p>Cumulative reward for the current trial.</p> <code>num_tr</code> <p>Number of completed trials.</p> <code>t</code> <p>Step counter (used when trigger is \"step\").</p> <code>save_dir</code> <p>Directory where data and plots are saved.</p> Source code in <code>neurogym/wrappers/monitor.py</code> <pre><code>def __init__(\n    self,\n    env: ngym.TrialEnv,\n    config: ngym.Config | str | Path | None = None,\n    name: str | None = None,\n    trigger: str = \"trial\",\n    interval: int = 1000,\n    verbose: bool = True,\n    plot_create: bool = False,\n    plot_steps: int = 1000,\n    ext: str = \"png\",\n    step_fn: Callable | None = None,\n) -&gt; None:\n    super().__init__(env)\n    self.env = env\n    self.step_fn = step_fn\n\n    cfg: ngym.Config\n    if config is None:\n        config_dict = {\n            \"env\": {\"name\": env.unwrapped.__class__.__name__},\n            \"monitor\": {\n                \"name\": name or \"Monitor\",\n                \"trigger\": trigger,\n                \"interval\": interval,\n                \"verbose\": verbose,\n                \"plot\": {\n                    \"create\": plot_create,\n                    \"step\": plot_steps,\n                    \"title\": env.unwrapped.__class__.__name__,\n                    \"ext\": ext,\n                },\n            },\n            \"local_dir\": LOCAL_DIR,\n        }\n        cfg = ngym.Config.model_validate(config_dict)\n    elif isinstance(config, (str, Path)):\n        cfg = ngym.Config(config_file=config)\n    else:\n        cfg = config  # type: ignore[arg-type]\n\n    self.config: ngym.Config = cfg\n\n    # Assign names for the environment and/or the monitor if they are empty\n    if len(self.config.env.name) == 0:\n        self.config.env.name = self.env.unwrapped.__class__.__name__\n    if len(self.config.monitor.name) == 0:\n        self.config.monitor.name = self.__class__.__name__\n\n    # data to save\n    self.data: dict[str, list] = {\"action\": [], \"reward\": [], \"cum_reward\": [], \"performance\": []}\n    # Evaluation-only data collected during the policy run (not saved to disk)\n    self.data_eval: dict[str, Any] = {}\n    self.cum_reward = 0.0\n    if self.config.monitor.trigger == \"step\":\n        self.t = 0\n    self.num_tr = 0\n\n    # Directory for saving plots\n    save_dir_name = f\"{self.config.env.name}/{ngym.utils.iso_timestamp()}\"\n    self.save_dir = ngym.utils.ensure_dir(self.config.local_dir / save_dir_name)\n\n    # Figures\n    if self.config.monitor.plot.create:\n        self.stp_counter = 0\n        self.ob_mat: list = []\n        self.act_mat: list = []\n        self.rew_mat: list = []\n        self.gt_mat: list = []\n        self.perf_mat: list = []\n\n    # Ensure the reset function is called\n    initial_ob, *_ = env.reset()\n    self.initial_ob = initial_ob\n</code></pre>"},{"location":"api/wrappers/#neurogym.wrappers.monitor.Monitor.reset","title":"reset","text":"<pre><code>reset(seed=None)\n</code></pre> <p>Reset the environment.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <p>Random seed for the environment</p> <code>None</code> <p>Returns:</p> Type Description <p>The initial observation from the environment reset</p> Source code in <code>neurogym/wrappers/monitor.py</code> <pre><code>def reset(self, seed=None):\n    \"\"\"Reset the environment.\n\n    Args:\n        seed: Random seed for the environment\n\n    Returns:\n        The initial observation from the environment reset\n    \"\"\"\n    self.cum_reward = 0\n    return super().reset(seed=seed)\n</code></pre>"},{"location":"api/wrappers/#neurogym.wrappers.monitor.Monitor.step","title":"step","text":"<pre><code>step(\n    action: Any, collect_data: bool = True\n) -&gt; tuple[Any, float, bool, bool, dict[str, Any]]\n</code></pre> <p>Execute one environment step.</p> <p>This method: 1. Takes a step in the environment 2. Collects data if sv_fig is enabled 3. Saves data when a trial completes and saving conditions are met</p> <p>Parameters:</p> Name Type Description Default <code>action</code> <code>Any</code> <p>The action to take in the environment</p> required <code>collect_data</code> <code>bool</code> <p>If True, collect and save data</p> <code>True</code> <p>Returns:</p> Type Description <code>tuple[Any, float, bool, bool, dict[str, Any]]</code> <p>Tuple of (observation, reward, terminated, truncated, info)</p> Source code in <code>neurogym/wrappers/monitor.py</code> <pre><code>def step(self, action: Any, collect_data: bool = True) -&gt; tuple[Any, float, bool, bool, dict[str, Any]]:\n    \"\"\"Execute one environment step.\n\n    This method:\n    1. Takes a step in the environment\n    2. Collects data if sv_fig is enabled\n    3. Saves data when a trial completes and saving conditions are met\n\n    Args:\n        action: The action to take in the environment\n        collect_data: If True, collect and save data\n\n    Returns:\n        Tuple of (observation, reward, terminated, truncated, info)\n    \"\"\"\n    if self.step_fn is not None:\n        obs, rew, terminated, truncated, info = self.step_fn(action)\n    else:\n        obs, rew, terminated, truncated, info = self.env.step(action)\n    self.cum_reward += rew\n    if self.config.monitor.plot.create:\n        self.store_data(obs, action, rew, info)\n    if self.config.monitor.trigger == \"step\":\n        self.t += 1\n    if info.get(\"new_trial\", False):\n        self.num_tr += 1\n        self.data[\"action\"].append(action)\n        self.data[\"reward\"].append(rew)\n        self.data[\"cum_reward\"].append(self.cum_reward)\n        self.cum_reward = 0\n        for key in info:\n            if key not in self.data:\n                self.data[key] = [info[key]]\n            else:\n                self.data[key].append(info[key])\n\n        # save data\n        save = (\n            self.t &gt;= self.config.monitor.interval\n            if self.config.monitor.trigger == \"step\"\n            else self.num_tr % self.config.monitor.interval == 0\n        )\n        if save and collect_data:\n            # Create save path with pathlib for cross-platform compatibility\n            save_path = self.save_dir / f\"trial_{self.num_tr}.npz\"\n            np.savez(save_path, **self.data)\n\n            if self.config.monitor.verbose:\n                logger.info(\"--------------------\")\n                logger.info(f\"Data saved to: {save_path}\")\n                logger.info(f\"Number of trials: {self.num_tr}\")\n                logger.info(f\"Average reward: {np.mean(self.data['reward'])}\")\n                logger.info(f\"Average performance: {np.mean(self.data['performance'])}\")\n                logger.info(\"--------------------\")\n            self.reset_data()\n            if self.config.monitor.plot.create:\n                self.stp_counter = 0\n            if self.config.monitor.trigger == \"step\":\n                self.t = 0\n    return obs, rew, terminated, truncated, info\n</code></pre>"},{"location":"api/wrappers/#neurogym.wrappers.monitor.Monitor.reset_data","title":"reset_data","text":"<pre><code>reset_data() -&gt; None\n</code></pre> <p>Reset all data containers to empty lists.</p> Source code in <code>neurogym/wrappers/monitor.py</code> <pre><code>def reset_data(self) -&gt; None:\n    \"\"\"Reset all data containers to empty lists.\"\"\"\n    for key in self.data:\n        self.data[key] = []\n</code></pre>"},{"location":"api/wrappers/#neurogym.wrappers.monitor.Monitor.store_data","title":"store_data","text":"<pre><code>store_data(\n    obs: Any, action: Any, rew: float, info: dict[str, Any]\n) -&gt; None\n</code></pre> <p>Store data for visualization figures.</p> <p>Parameters:</p> Name Type Description Default <code>obs</code> <code>Any</code> <p>Current observation</p> required <code>action</code> <code>Any</code> <p>Current action</p> required <code>rew</code> <code>float</code> <p>Current reward</p> required <code>info</code> <code>dict[str, Any]</code> <p>Info dictionary from environment</p> required Source code in <code>neurogym/wrappers/monitor.py</code> <pre><code>def store_data(self, obs: Any, action: Any, rew: float, info: dict[str, Any]) -&gt; None:\n    \"\"\"Store data for visualization figures.\n\n    Args:\n        obs: Current observation\n        action: Current action\n        rew: Current reward\n        info: Info dictionary from environment\n    \"\"\"\n    if self.stp_counter &lt;= self.config.monitor.plot.step:\n        self.ob_mat.append(obs)\n        self.act_mat.append(action)\n        self.rew_mat.append(rew)\n        if \"gt\" in info:\n            self.gt_mat.append(info[\"gt\"])\n        else:\n            self.gt_mat.append(-1)\n        if \"performance\" in info:\n            self.perf_mat.append(info[\"performance\"])\n        else:\n            self.perf_mat.append(-1)\n        self.stp_counter += 1\n    elif len(self.rew_mat) &gt; 0:\n        fname = self.save_dir / f\"task_{self.num_tr:06d}.{self.config.monitor.plot.ext}\"\n        obs_mat = np.array(self.ob_mat)\n        act_mat = np.array(self.act_mat)\n        fig_(\n            ob=obs_mat,\n            actions=act_mat,\n            gt=self.gt_mat,\n            rewards=self.rew_mat,\n            performance=self.perf_mat,\n            fname=fname,\n            name=self.config.monitor.plot.title,\n            env=self.env,\n            initial_ob=self.initial_ob,\n        )\n        self.ob_mat = []\n        self.act_mat = []\n        self.rew_mat = []\n        self.gt_mat = []\n        self.perf_mat = []\n</code></pre>"},{"location":"api/wrappers/#neurogym.wrappers.monitor.Monitor.evaluate_policy","title":"evaluate_policy","text":"<pre><code>evaluate_policy(\n    num_trials: int = 100,\n    model: Any | None = None,\n    verbose: bool = True,\n) -&gt; dict[str, float | list[float]]\n</code></pre> <p>Evaluates the average performance of the RL agent in the environment.</p> <p>This method runs the given model (or random policy if None) on the environment for a specified number of trials and collects performance metrics.</p> <p>Parameters:</p> Name Type Description Default <code>num_trials</code> <code>int</code> <p>Number of trials to run for evaluation</p> <code>100</code> <code>model</code> <code>Any | None</code> <p>The policy model to evaluate (if None, uses random actions)</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>If True, prints progress information</p> <code>True</code> <p>Returns:     dict: Dictionary containing performance metrics:         - mean_performance: Average performance (if reported by environment)         - mean_reward: Proportion of positive rewards         - performances: List of performance values for each trial         - rewards: List of rewards for each trial.</p> Source code in <code>neurogym/wrappers/monitor.py</code> <pre><code>def evaluate_policy(\n    self,\n    num_trials: int = 100,\n    model: Any | None = None,\n    verbose: bool = True,\n) -&gt; dict[str, float | list[float]]:\n    \"\"\"Evaluates the average performance of the RL agent in the environment.\n\n    This method runs the given model (or random policy if None) on the\n    environment for a specified number of trials and collects performance\n    metrics.\n\n    Args:\n        num_trials: Number of trials to run for evaluation\n        model: The policy model to evaluate (if None, uses random actions)\n        verbose: If True, prints progress information\n    Returns:\n        dict: Dictionary containing performance metrics:\n            - mean_performance: Average performance (if reported by environment)\n            - mean_reward: Proportion of positive rewards\n            - performances: List of performance values for each trial\n            - rewards: List of rewards for each trial.\n    \"\"\"\n    # Reset environment\n    obs, _ = self.env.reset()\n\n    # Initialize hidden states\n    states = None\n    episode_starts = np.array([True])\n\n    # Tracking variables\n    rewards = []\n    cum_reward = 0.0\n    cum_rewards = []\n    performances = []\n    self.data_eval = {\"action\": [], \"reward\": []}\n    # Initialize trial count\n    trial_count = 0\n\n    # Run trials\n    while trial_count &lt; num_trials:\n        if model is not None:\n            if isinstance(model.policy, RecurrentActorCriticPolicy):\n                action, states = model.predict(obs, state=states, episode_start=episode_starts, deterministic=True)\n            else:\n                action, _ = model.predict(obs, deterministic=True)\n        else:\n            action = self.env.action_space.sample()\n\n        # Use collect_data=False to avoid saving evaluation data\n        obs, reward, _, _, info = self.step(action, collect_data=False)\n        # Update episode_starts after each step\n        episode_starts = np.array([False])\n        cum_reward += reward\n\n        if info.get(\"new_trial\", False):\n            trial_count += 1\n            rewards.append(reward)\n            cum_rewards.append(cum_reward)\n            cum_reward = 0.0\n            if \"performance\" in info:\n                performances.append(info[\"performance\"])\n\n            if verbose and trial_count % 1000 == 0:  # FIXME: why is this value hardcoded?\n                logger.info(f\"Completed {trial_count}/{num_trials} trials\")\n\n            self.data_eval[\"action\"].append(action)\n            self.data_eval[\"reward\"].append(reward)\n            for key in info:\n                if key not in self.data_eval:\n                    self.data_eval[key] = [info[key]]\n                else:\n                    self.data_eval[key].append(info[key])\n\n            # Reset states at the end of each trial\n            states = None\n            episode_starts = np.array([True])\n\n    # Calculate metrics\n    performance_array = np.array([p for p in performances if p != -1])\n    reward_array = np.array(rewards)\n    cum_reward_array = np.array(cum_rewards)\n\n    # Convert lists to numpy arrays for easier downstream analysis\n    for key in self.data_eval:\n        if key != \"trial\":\n            self.data_eval[key] = np.array(self.data_eval[key])\n\n    return {\n        \"rewards\": rewards,\n        \"mean_reward\": float(np.mean(reward_array &gt; 0)) if len(reward_array) &gt; 0 else 0,\n        \"cum_rewards\": cum_rewards,\n        \"mean_cum_reward\": float(np.mean(cum_reward_array)) if len(cum_reward_array) &gt; 0 else 0,\n        \"performances\": performances,\n        \"mean_performance\": float(np.mean(performance_array)) if len(performance_array) &gt; 0 else 0,\n    }\n</code></pre>"},{"location":"api/wrappers/#neurogym.wrappers.monitor.Monitor.plot_training_history","title":"plot_training_history","text":"<pre><code>plot_training_history(\n    figsize: tuple[int, int] = (12, 6),\n    save_fig: bool = True,\n    plot_performance: bool = True,\n) -&gt; Figure | None\n</code></pre> <p>Plot rewards and performance training history from saved data files with one data point per trial.</p> <p>Parameters:</p> Name Type Description Default <code>figsize</code> <code>tuple[int, int]</code> <p>Figure size as (width, height) tuple</p> <code>(12, 6)</code> <code>save_fig</code> <code>bool</code> <p>Whether to save the figure to disk</p> <code>True</code> <code>plot_performance</code> <code>bool</code> <p>Whether to plot performance in a separate plot</p> <code>True</code> <p>Returns:     matplotlib figure object</p> Source code in <code>neurogym/wrappers/monitor.py</code> <pre><code>def plot_training_history(\n    self,\n    figsize: tuple[int, int] = (12, 6),\n    save_fig: bool = True,\n    plot_performance: bool = True,\n) -&gt; plt.Figure | None:\n    \"\"\"Plot rewards and performance training history from saved data files with one data point per trial.\n\n    Args:\n        figsize: Figure size as (width, height) tuple\n        save_fig: Whether to save the figure to disk\n        plot_performance: Whether to plot performance in a separate plot\n    Returns:\n        matplotlib figure object\n    \"\"\"\n    files = sorted(self.save_dir.glob(\"*.npz\"))\n\n    if not files:\n        logger.warning(\"No data files found matching pattern: *.npz\")\n        return None\n\n    logger.info(f\"Found {len(files)} data files\")\n\n    # Arrays to hold average values\n    avg_rewards_per_file = []\n    avg_cum_rewards_per_file = []\n    avg_performances_per_file = []\n    file_indices = []\n    total_trials = 0\n\n    for file in files:\n        data = np.load(file, allow_pickle=True)\n\n        if \"reward\" in data:\n            rewards = data[\"reward\"]\n            if len(rewards) &gt; 0:\n                avg_rewards_per_file.append(np.mean(rewards))\n                total_trials += len(rewards)\n                file_indices.append(total_trials)\n\n        if \"cum_reward\" in data:\n            cum_rewards = data[\"cum_reward\"]\n            if len(cum_rewards) &gt; 0:\n                avg_cum_rewards_per_file.append(np.mean(cum_rewards))\n\n        if \"performance\" in data:\n            perfs = data[\"performance\"]\n            if len(perfs) &gt; 0:\n                avg_performances_per_file.append(np.mean(perfs))\n\n    file_indices = [0, *file_indices]\n    avg_rewards_per_file = [0, *avg_rewards_per_file]\n    avg_cum_rewards_per_file = [0, *avg_cum_rewards_per_file]\n    avg_performances_per_file = [0, *avg_performances_per_file]\n\n    fig, axes = plt.subplots(1, 2 if plot_performance else 1, figsize=figsize)\n    if not isinstance(axes, np.ndarray):\n        axes = [axes]\n\n    # 1. Rewards and Cumulative Rewards plot\n    ax1 = axes[0]\n\n    if len(avg_rewards_per_file) == len(file_indices):\n        ax1.plot(file_indices, avg_rewards_per_file, \"o-\", color=\"blue\", label=\"Avg Reward\", linewidth=2)\n    if len(avg_cum_rewards_per_file) == len(file_indices):\n        ax1.plot(file_indices, avg_cum_rewards_per_file, \"s--\", color=\"red\", label=\"Avg Cum Reward\", linewidth=2)\n\n    ax1.set_xlabel(\"Trials\")\n    ax1.set_ylabel(\"Reward / Cumulative Reward\")\n    common_ylim = (-0.05, 1.05)\n    ax1.set_ylim(common_ylim)\n    ax1.set_title(\"Reward and Cumulative Reward per File\")\n\n    ax1.grid(True, which=\"both\", axis=\"y\", linestyle=\"--\", alpha=0.7)\n    ax1.legend(loc=\"center right\", bbox_to_anchor=(1, 0.2))\n\n    # 2. Optional: Performances plot\n    if plot_performance and len(axes) &gt; 1:\n        ax2 = axes[1]\n        if len(avg_performances_per_file) == len(file_indices):\n            ax2.plot(file_indices, avg_performances_per_file, \"o-\", color=\"green\", linewidth=2)\n        ax2.set_xlabel(\"Trials\")\n        ax2.set_ylabel(\"Average Performance (0-1)\")\n        ax2.set_ylim(common_ylim)\n        ax2.set_title(\"Average Performance per File\")\n\n        ax2.grid(True, which=\"both\", axis=\"y\", linestyle=\"--\", alpha=0.7)\n    plt.tight_layout()\n    fig.subplots_adjust(top=0.8)\n\n    for ax in fig.axes:\n        ax.spines[\"top\"].set_visible(False)\n        ax.spines[\"right\"].set_visible(False)\n\n    plt.suptitle(\n        f\"Training History for {self.config.env.name}\\n({len(files)} data files, {total_trials} total trials)\",\n        fontsize=12,\n    )\n\n    if save_fig:\n        save_path = self.config.local_dir / f\"{self.config.env.name}_training_history.png\"\n        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n        logger.info(f\"Figure saved to {save_path}\")\n\n    return fig\n</code></pre>"},{"location":"api/wrappers/#neurogym.wrappers.noise","title":"noise","text":"<p>Noise wrapper.</p> <p>Created on Thu Feb 28 15:07:21 2019</p> <p>@author: molano</p>"},{"location":"api/wrappers/#neurogym.wrappers.noise.Noise","title":"Noise","text":"<pre><code>Noise(env, std_noise=0.1)\n</code></pre> <p>               Bases: <code>Wrapper</code></p> <p>Add Gaussian noise to the observations.</p> <p>Parameters:</p> Name Type Description Default <code>std_noise</code> <p>Standard deviation of noise. (def: 0.1)</p> <code>0.1</code> <code>perf_th</code> <p>If != None, the wrapper will adjust the noise so the mean performance is not larger than perf_th. (def: None, float)</p> required <code>w</code> <p>Window used to compute the mean performance. (def: 100, int)</p> required <code>step_noise</code> <p>Step used to increment/decrease std. (def: 0.001, float)</p> required Source code in <code>neurogym/wrappers/noise.py</code> <pre><code>def __init__(self, env, std_noise=0.1) -&gt; None:\n    super().__init__(env)\n    self.env = env\n    self.std_noise = std_noise\n</code></pre>"},{"location":"api/wrappers/#neurogym.wrappers.pass_action","title":"pass_action","text":""},{"location":"api/wrappers/#neurogym.wrappers.pass_action.PassAction","title":"PassAction","text":"<pre><code>PassAction(env)\n</code></pre> <p>               Bases: <code>Wrapper</code></p> <p>Modifies observation by adding the previous action.</p> Source code in <code>neurogym/wrappers/pass_action.py</code> <pre><code>def __init__(self, env) -&gt; None:\n    super().__init__(env)\n    self.env = env\n    if isinstance(env.observation_space, spaces.Discrete):\n        env_oss = env.observation_space.n  # Number of discrete states\n        self.observation_space = spaces.Discrete(n=env_oss + 1)\n    else:\n        env_oss = env.observation_space.shape[0]\n        self.observation_space = spaces.Box(\n            -np.inf,\n            np.inf,\n            shape=(int(env_oss) + 1,),\n            dtype=np.float32,\n        )\n</code></pre>"},{"location":"api/wrappers/#neurogym.wrappers.pass_reward","title":"pass_reward","text":""},{"location":"api/wrappers/#neurogym.wrappers.pass_reward.PassReward","title":"PassReward","text":"<pre><code>PassReward(env)\n</code></pre> <p>               Bases: <code>Wrapper</code></p> <p>Modifies observation by adding the previous reward.</p> Source code in <code>neurogym/wrappers/pass_reward.py</code> <pre><code>def __init__(self, env) -&gt; None:\n    \"\"\"Modifies observation by adding the previous reward.\"\"\"\n    super().__init__(env)\n    if isinstance(env.observation_space, spaces.Discrete):\n        env_oss = env.observation_space.n  # Number of discrete states\n        self.observation_space = spaces.Discrete(n=env_oss + 1)\n    else:\n        env_oss = env.observation_space.shape[0]\n        self.observation_space = spaces.Box(\n            -np.inf,\n            np.inf,\n            shape=(int(env_oss) + 1,),\n            dtype=np.float32,\n        )\n</code></pre>"},{"location":"api/wrappers/#neurogym.wrappers.reaction_time","title":"reaction_time","text":"<p>Noise wrapper.</p> <p>Created on Thu Feb 28 15:07:21 2019</p> <p>@author: molano</p>"},{"location":"api/wrappers/#neurogym.wrappers.reaction_time.ReactionTime","title":"ReactionTime","text":"<pre><code>ReactionTime(\n    env: TrialEnv,\n    urgency: float = 0.0,\n    end_on_stimulus: bool = True,\n)\n</code></pre> <p>               Bases: <code>Wrapper</code></p> <p>Allow reaction time response.</p> <p>Modifies a given environment by allowing the network to act at any time after the fixation period. By default, the trial ends when the stimulus period ends. Optionally, the original trial structure can be preserved while still allowing early responses.</p> <p>Parameters:</p> Name Type Description Default <code>env</code> <code>TrialEnv</code> <p>The environment to wrap</p> required <code>urgency</code> <code>float</code> <p>Urgency signal added to reward at each timestep</p> <code>0.0</code> <code>end_on_stimulus</code> <code>bool</code> <p>If True (default), trial ends when stimulus ends. If False, preserves original trial timing while allowing early responses during stimulus period.</p> <code>True</code> Source code in <code>neurogym/wrappers/reaction_time.py</code> <pre><code>def __init__(self, env: ngym.TrialEnv, urgency: float = 0.0, end_on_stimulus: bool = True) -&gt; None:\n    super().__init__(env)\n    self.urgency = urgency\n    self.end_on_stimulus = end_on_stimulus\n    self.tr_dur = 0\n</code></pre>"},{"location":"api/wrappers/#neurogym.wrappers.side_bias","title":"side_bias","text":""},{"location":"api/wrappers/#neurogym.wrappers.side_bias.SideBias","title":"SideBias","text":"<pre><code>SideBias(\n    env: TrialEnv,\n    probs: list[list[float]],\n    block_dur: float | tuple[int, int] = 200,\n)\n</code></pre> <p>               Bases: <code>TrialWrapper</code></p> <p>Changes the probability of ground truth with block-wise biases.</p> <p>Parameters:</p> Name Type Description Default <code>env</code> <code>TrialEnv</code> <p>The task environment to wrap (must expose <code>choices</code>).</p> required <code>probs</code> <code>list[list[float]]</code> <p>Explicit probability matrix with shape (n_blocks, n_choices).    Each row defines the choice probabilities for one block and must sum to 1.0.    The number of columns (n_choices) must match the number of choices defined in the task    (i.e., <code>len(env.choices)</code>), so that each probability maps to a valid choice.</p> required <code>block_dur</code> <code>float | tuple[int, int]</code> <p>Duration of each block, with behavior depending on the type: - int (\u2265 1): Use a fixed number of trials per block     (e.g., block_dur=20 means each block has exactly 20 trials). - float (0 &lt; value &lt; 1): Specify a per-trial probability of switching     to a new block (e.g., block_dur=0.1 means there's a 10% chance of     switching blocks after each trial). - tuple (low, high): Draw the number of trials per block randomly from     a uniform distribution over the integer range [low, high] (inclusive).</p> <code>200</code> <p>Examples:</p> <ul> <li>probs=[[0.8, 0.2], [0.2, 0.8], [0.4, 0.6]], block_dur=200   Stay 200 trials per block, randomly switch to new block after;</li> <li>probs=[[0.8, 0.2], [0.2, 0.8], [0.4, 0.6]], block_dur=0.1   10% probability per trial to switch to new random block;</li> <li>probs=[[0.8, 0.2], [0.2, 0.8], [0.4, 0.6]], block_dur=(200, 400)   Random trials in [200, 400] range per block, then switch.</li> </ul> Source code in <code>neurogym/wrappers/side_bias.py</code> <pre><code>def __init__(self, env: ngym.TrialEnv, probs: list[list[float]], block_dur: float | tuple[int, int] = 200) -&gt; None:\n    super().__init__(env)\n\n    # Validate environment\n    if not isinstance(self.task, ngym.TrialEnv):\n        msg = \"SideBias requires the wrapped task to be a TrialEnv.\"\n        raise TypeError(msg)\n\n    try:\n        self.choices = self.task.choices  # type: ignore[attr-defined]\n    except AttributeError as e:\n        msg = \"SideBias requires task to have attribute choices.\"\n        raise AttributeError(msg) from e\n\n    # Reject non-matrix types (no automatic matrix generation)\n    if (\n        not isinstance(probs, list)\n        or not all(isinstance(row, list) for row in probs)\n        or not all(isinstance(prob, (float, int)) for row in probs for prob in row)\n    ):\n        msg = (\n            \"probs must be a 2D list of lists (matrix) with shape (n_blocks, n_choices),\"\n            \"e.g., probs = [[0.5, 0.5], [0.2, 0.8], [0.8, 0.2]] for n_blocks = 3 and n_choices = 2.\"\n        )\n        raise TypeError(msg)\n\n    # Convert to numpy array and validate\n    self.choice_prob = np.array(probs, dtype=float)\n\n    # Validate matrix dimensions\n    if self.choice_prob.ndim != 2:\n        msg = f\"probs must be a 2D matrix, got {self.choice_prob.ndim}D array.\"\n        raise ValueError(msg)\n\n    n_blocks, n_choices = self.choice_prob.shape\n\n    if n_choices != len(self.choices):\n        msg = (\n            f\"The number of choices inferred from the probability matrix ({n_choices}) \"\n            f\"does not match the number of choices defined in the task ({len(self.choices)}). \"\n            \"These must be equal to ensure each probability corresponds to a valid choice.\"\n        )\n        raise ValueError(msg)\n\n    # Validate that each row sums to 1 (with tolerance for floating point)\n    row_sums = np.sum(self.choice_prob, axis=1)\n    tolerance = 1e-10\n    if not np.allclose(row_sums, 1.0, atol=tolerance):\n        invalid_rows = np.where(~np.isclose(row_sums, 1.0, atol=tolerance))[0]\n        msg = (\n            f\"Each row in probs must sum to 1.0. \"\n            f\"Rows {invalid_rows.tolist()} have sums {row_sums[invalid_rows].tolist()}\"\n        )\n        raise ValueError(msg)\n\n    # Validate probabilities are non-negative\n    if np.any(self.choice_prob &lt; 0):\n        msg = \"All probabilities in probs must be non-negative.\"\n        raise ValueError(msg)\n\n    self.n_block = n_blocks\n    self.curr_block = self.task.rng.choice(self.n_block)\n\n    # Validate and set block_dur\n    self._validate_and_set_block_dur(block_dur)\n\n    # Initialize block state\n    self._remaining_trials, self._p_switch = self._new_block_duration()\n</code></pre>"},{"location":"api/wrappers/#neurogym.wrappers.side_bias.SideBias.new_trial","title":"new_trial","text":"<pre><code>new_trial(**kwargs)\n</code></pre> <p>Generate new trial with block-based probability biases.</p> Source code in <code>neurogym/wrappers/side_bias.py</code> <pre><code>def new_trial(self, **kwargs):\n    \"\"\"Generate new trial with block-based probability biases.\"\"\"\n    # Determine if we should switch blocks\n    if self._p_switch is not None:\n        # Probability-based switching\n        switch_block = self.task.rng.random() &lt; self._p_switch\n    else:\n        # Counter-based switching\n        self._remaining_trials -= 1\n        switch_block = self._remaining_trials &lt;= 0\n\n    if switch_block:\n        # Switch to a different random block (never same as current)\n        if self.n_block &gt; 1:\n            # Multiple blocks available - choose different one\n            available_blocks = [b for b in range(self.n_block) if b != self.curr_block]\n            self.curr_block = self.task.rng.choice(available_blocks)\n        # If only 1 block, stay in same block (edge case)\n\n        # Generate new duration/probability for the new block\n        self._remaining_trials, self._p_switch = self._new_block_duration()\n\n    # Set trial parameters based on current block\n    probs = self.choice_prob[self.curr_block]\n    kwargs[\"ground_truth\"] = self.task.rng.choice(self.choices, p=probs)\n    kwargs[\"probs\"] = probs\n\n    return self.env.new_trial(**kwargs)\n</code></pre>"},{"location":"api/tags/confidence/","title":"Confidence","text":"<p>The following tasks implement confidence paradigms:</p> <ul> <li>PostDecisionWager</li> </ul>"},{"location":"api/tags/context_dependent/","title":"Context Dependent","text":"<p>The following tasks implement context-dependent paradigms:</p> <ul> <li>ContextDecisionMaking</li> </ul>"},{"location":"api/tags/continuous_action_space/","title":"Continuous Action Space","text":"<p>The following tasks implement continuous action space paradigms:</p> <ul> <li>ReachingDelayResponse</li> </ul>"},{"location":"api/tags/delayed_response/","title":"Delayed Response","text":"<p>The following tasks implement delayed response paradigms:</p> <ul> <li>GoNogo</li> <li>IntervalDiscrimination</li> <li>PerceptualDecisionMakingDelayResponse</li> <li>PostDecisionWager</li> <li>ReachingDelayResponse</li> </ul>"},{"location":"api/tags/go_no_go/","title":"Go-No-Go","text":"<p>The following tasks implement go/no-go paradigms:</p> <ul> <li>DelayPairedAssociation</li> <li>GoNogo</li> <li>MotorTiming</li> <li>OneTwoThreeGo</li> <li>ReadySetGo</li> </ul>"},{"location":"api/tags/motor/","title":"Motor","text":"<p>The following tasks implement motor paradigms:</p> <ul> <li>Reaching1D</li> <li>Reaching1DWithSelfDistraction</li> </ul>"},{"location":"api/tags/multidimensional_action_space/","title":"Multidimensional Action Space","text":"<p>The following tasks implement multidimensional action space paradigms:</p> <ul> <li>ReachingDelayResponse</li> </ul>"},{"location":"api/tags/n_alternative/","title":"N-alternative","text":"<p>The following tasks implement n-alternative paradigms:</p> <ul> <li>Bandit</li> </ul>"},{"location":"api/tags/perceptual/","title":"Perceptual","text":"<p>The following tasks implement perceptual paradigms:</p> <ul> <li>AntiReach</li> <li>ContextDecisionMaking</li> <li>DelayComparison</li> <li>DelayMatchCategory</li> <li>DelayMatchSample</li> <li>DelayMatchSampleDistractor1D</li> <li>DelayPairedAssociation</li> <li>DualDelayMatchSample</li> <li>EconomicDecisionMaking</li> <li>HierarchicalReasoning</li> <li>MultiSensoryIntegration</li> <li>PerceptualDecisionMaking</li> <li>PerceptualDecisionMakingDelayResponse</li> <li>PostDecisionWager</li> <li>ProbabilisticReasoning</li> <li>PulseDecisionMaking</li> <li>ReachingDelayResponse</li> <li>ContextDecisionMaking</li> </ul>"},{"location":"api/tags/steps_action_space/","title":"Steps Action Space","text":"<p>The following tasks implement steps action space paradigms:</p> <ul> <li>AntiReach</li> <li>Reaching1D</li> <li>Reaching1DWithSelfDistraction</li> </ul>"},{"location":"api/tags/supervised/","title":"Supervised","text":"<p>The following tasks implement supervised paradigms:</p> <ul> <li>ContextDecisionMaking</li> <li>DelayComparison</li> <li>DelayMatchCategory</li> <li>DelayMatchSample</li> <li>DelayMatchSampleDistractor1D</li> <li>DelayPairedAssociation</li> <li>DualDelayMatchSample</li> <li>GoNogo</li> <li>HierarchicalReasoning</li> <li>IntervalDiscrimination</li> <li>MotorTiming</li> <li>MultiSensoryIntegration</li> <li>OneTwoThreeGo</li> <li>PerceptualDecisionMaking</li> <li>PerceptualDecisionMakingDelayResponse</li> <li>ProbabilisticReasoning</li> <li>PulseDecisionMaking</li> <li>ReachingDelayResponse</li> <li>ReadySetGo</li> </ul>"},{"location":"api/tags/timing/","title":"Timing","text":"<p>The following tasks implement timing paradigms:</p> <ul> <li>IntervalDiscrimination</li> <li>MotorTiming</li> <li>OneTwoThreeGo</li> <li>ReadySetGo</li> </ul>"},{"location":"api/tags/two_alternative/","title":"Two-Alternative","text":"<p>The following tasks implement two-alternative paradigms:</p> <ul> <li>ContextDecisionMaking</li> <li>DawTwoStep</li> <li>DelayComparison</li> <li>DelayMatchCategory</li> <li>DelayMatchSample</li> <li>DelayMatchSampleDistractor1D</li> <li>DualDelayMatchSample</li> <li>HierarchicalReasoning</li> <li>IntervalDiscrimination</li> <li>MultiSensoryIntegration</li> <li>PerceptualDecisionMaking</li> <li>PerceptualDecisionMakingDelayResponse</li> <li>ProbabilisticReasoning</li> <li>PulseDecisionMaking</li> </ul>"},{"location":"api/tags/value_based/","title":"Value-Based","text":"<p>The following tasks implement value-based paradigms:</p> <ul> <li>EconomicDecisionMaking</li> </ul>"},{"location":"api/tags/working_memory/","title":"Working Memory","text":"<p>The following tasks implement working memory paradigms:</p> <ul> <li>DelayComparison</li> <li>DelayMatchCategory</li> <li>DelayMatchSample</li> <li>DelayMatchSampleDistractor1D</li> <li>DelayPairedAssociation</li> <li>DualDelayMatchSample</li> <li>IntervalDiscrimination</li> </ul>"},{"location":"examples/annubes/","title":"Annubes","text":"In\u00a0[\u00a0]: Copied! <pre># ! pip install neurogym[rl]\n</pre> # ! pip install neurogym[rl] In\u00a0[\u00a0]: Copied! <pre>import neurogym as ngym\nfrom neurogym.envs.native.annubes import AnnubesEnv\nfrom stable_baselines3.common.env_checker import check_env\nfrom IPython.display import clear_output\nclear_output()\n</pre> import neurogym as ngym from neurogym.envs.native.annubes import AnnubesEnv from stable_baselines3.common.env_checker import check_env from IPython.display import clear_output clear_output() <p>Let's create an environment, check it works and visualize it.</p> In\u00a0[\u00a0]: Copied! <pre>env = AnnubesEnv()\n\n# check the custom environment and output additional warnings (if any)\ncheck_env(env)\n\n# check the environment with a random agent\nobs, info = env.reset()\nn_steps = 10\nfor _ in range(n_steps):\n    # random action\n    action = env.action_space.sample()\n    obs, reward, terminated, truncated, info = env.step(action)\n    if terminated:\n        obs, info = env.reset()\n\nprint(env.timing)\nprint(\"----------------\")\nprint(env.observation_space)\nprint(env.observation_space.name)\nprint(\"----------------\")\nprint(env.action_space)\nprint(env.action_space.name)\n</pre> env = AnnubesEnv()  # check the custom environment and output additional warnings (if any) check_env(env)  # check the environment with a random agent obs, info = env.reset() n_steps = 10 for _ in range(n_steps):     # random action     action = env.action_space.sample()     obs, reward, terminated, truncated, info = env.step(action)     if terminated:         obs, info = env.reset()  print(env.timing) print(\"----------------\") print(env.observation_space) print(env.observation_space.name) print(\"----------------\") print(env.action_space) print(env.action_space.name) In\u00a0[\u00a0]: Copied! <pre>fig = ngym.utils.plot_env(env, ob_traces=[\"fixation\", \"start\", \"v\", \"a\"], num_trials=10)\n</pre> fig = ngym.utils.plot_env(env, ob_traces=[\"fixation\", \"start\", \"v\", \"a\"], num_trials=10) In\u00a0[\u00a0]: Copied! <pre>import warnings\n\nfrom stable_baselines3 import A2C\nfrom stable_baselines3.common.vec_env import DummyVecEnv\n\nwarnings.filterwarnings(\"default\")\n\n# these values are set low for testing purposes. To get a better sense of the package, we recommend setting\n# `total_timesteps = 20000` and `log_interval = 1000`\ntotal_timesteps = 200\nlog_interval = 10\n\n# train agent\nenv = AnnubesEnv()\nenv_vec = DummyVecEnv([lambda: env])\nmodel = A2C(\"MlpPolicy\", env_vec, verbose=0)\nmodel.learn(total_timesteps=total_timesteps, log_interval=log_interval)\nenv_vec.close()\n\n# plot example trials with trained agent\ndata = ngym.utils.plot_env(\n    env, num_trials=10, ob_traces=[\"fixation\", \"start\", \"v\", \"a\"], model=model\n)\n</pre> import warnings  from stable_baselines3 import A2C from stable_baselines3.common.vec_env import DummyVecEnv  warnings.filterwarnings(\"default\")  # these values are set low for testing purposes. To get a better sense of the package, we recommend setting # `total_timesteps = 20000` and `log_interval = 1000` total_timesteps = 200 log_interval = 10  # train agent env = AnnubesEnv() env_vec = DummyVecEnv([lambda: env]) model = A2C(\"MlpPolicy\", env_vec, verbose=0) model.learn(total_timesteps=total_timesteps, log_interval=log_interval) env_vec.close()  # plot example trials with trained agent data = ngym.utils.plot_env(     env, num_trials=10, ob_traces=[\"fixation\", \"start\", \"v\", \"a\"], model=model ) In\u00a0[\u00a0]: Copied! <pre>env1 = AnnubesEnv({\"v\": 1, \"a\": 0})\nenv1_vec = DummyVecEnv([lambda: env1])\n# create a model and train it with the first environment\nmodel = A2C(\"MlpPolicy\", env1_vec, verbose=0)\nmodel.learn(total_timesteps=total_timesteps)\nenv1_vec.close()\n\n# plot example trials with trained agent\ndata = ngym.utils.plot_env(\n    env1, num_trials=10, ob_traces=[\"fixation\", \"start\", \"v\", \"a\"], model=model\n)\n</pre> env1 = AnnubesEnv({\"v\": 1, \"a\": 0}) env1_vec = DummyVecEnv([lambda: env1]) # create a model and train it with the first environment model = A2C(\"MlpPolicy\", env1_vec, verbose=0) model.learn(total_timesteps=total_timesteps) env1_vec.close()  # plot example trials with trained agent data = ngym.utils.plot_env(     env1, num_trials=10, ob_traces=[\"fixation\", \"start\", \"v\", \"a\"], model=model ) In\u00a0[\u00a0]: Copied! <pre># switch to the second environment and continue training\nenv2 = AnnubesEnv({\"v\": 0, \"a\": 1})\nenv2_vec = DummyVecEnv([lambda: env2])\n# set the model's environment to the new environment\nmodel.set_env(env2_vec)\nmodel.learn(total_timesteps=total_timesteps)\nenv2_vec.close()\n\n# plot example trials with trained agent\ndata = ngym.utils.plot_env(\n    env2, num_trials=10, ob_traces=[\"fixation\", \"start\", \"v\", \"a\"], model=model\n)\n</pre> # switch to the second environment and continue training env2 = AnnubesEnv({\"v\": 0, \"a\": 1}) env2_vec = DummyVecEnv([lambda: env2]) # set the model's environment to the new environment model.set_env(env2_vec) model.learn(total_timesteps=total_timesteps) env2_vec.close()  # plot example trials with trained agent data = ngym.utils.plot_env(     env2, num_trials=10, ob_traces=[\"fixation\", \"start\", \"v\", \"a\"], model=model ) In\u00a0[\u00a0]: Copied! <pre># Switch to the third environment and finish training\nenv3 = AnnubesEnv({\"v\": 0.5, \"a\": 0.5})\nenv3_vec = DummyVecEnv([lambda: env3])\n# set the model's environment to the new environment\nmodel.set_env(env3_vec)\nmodel.learn(total_timesteps=total_timesteps)\nenv3_vec.close()\n\n# plot example trials with trained agent\ndata = ngym.utils.plot_env(\n    env3, num_trials=10, ob_traces=[\"fixation\", \"start\", \"v\", \"a\"], model=model\n)\n</pre> # Switch to the third environment and finish training env3 = AnnubesEnv({\"v\": 0.5, \"a\": 0.5}) env3_vec = DummyVecEnv([lambda: env3]) # set the model's environment to the new environment model.set_env(env3_vec) model.learn(total_timesteps=total_timesteps) env3_vec.close()  # plot example trials with trained agent data = ngym.utils.plot_env(     env3, num_trials=10, ob_traces=[\"fixation\", \"start\", \"v\", \"a\"], model=model ) In\u00a0[\u00a0]: Copied! <pre># Save the final model after all training\nmodel.save(\"final_model\")\n</pre> # Save the final model after all training model.save(\"final_model\")"},{"location":"examples/annubes/#annubesenv-environment","title":"<code>AnnubesEnv</code> environment\u00b6","text":"<p>This notebook is a simple example of how to use the <code>AnnubesEnv</code> class to create a custom environment and use it to train a reinforcement learning agent with <code>stable_baselines3</code>.</p>"},{"location":"examples/annubes/#installation","title":"Installation\u00b6","text":"<p>Google Colab: Uncomment and execute cell below when running this notebook on google colab.</p> <p>Local: Follow these instructions when running this notebook locally.</p>"},{"location":"examples/annubes/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/annubes/#training-annubesenv","title":"Training <code>AnnubesEnv</code>\u00b6","text":""},{"location":"examples/annubes/#1-regular-training","title":"1. Regular training\u00b6","text":"<p>We can train <code>AnnubesEnv</code> using one of the models defined in <code>stable_baselines3</code>, for example <code>A2C</code>.</p>"},{"location":"examples/annubes/#2-sequential-training","title":"2. Sequential training\u00b6","text":"<p>We can also train <code>AnnubesEnv</code> using a sequential training approach. This is useful when we want to train the agent in multiple stages, each with a different environment configuration. This can be useful for:</p> <ul> <li><p>Curriculum learning: Gradually increase the difficulty of the environments. Start with simpler tasks and progressively move to more complex ones, allowing the agent to build on its previous experiences.</p> </li> <li><p>Domain randomization: Vary the environment dynamics (e.g., physics, obstacles) during training to improve the agent's robustness to changes in the environment.</p> </li> <li><p>Transfer learning: If you have access to different agents or architectures, you can use transfer learning techniques to fine-tune the model on a new environment.</p> </li> </ul> <p>In this case it is important to include all the possible observations in each environment, even if not all of them are used. This is because the model is initialized with the first environment's observation space and it is not possible to change it later.</p>"},{"location":"examples/contextdecisionmaking/","title":"Context-Dependent Decision Making Task","text":"In\u00a0[\u00a0]: Copied! <pre># Uncomment to install\n# ! pip install neurogym[rl]\n</pre> # Uncomment to install # ! pip install neurogym[rl] In\u00a0[\u00a0]: Copied! <pre>import warnings\nfrom IPython.display import clear_output\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport neurogym as ngym\nimport torch.nn as nn\nimport torch\nimport gymnasium as gym\nimport os\nfrom stable_baselines3.common.env_checker import check_env\nfrom neurogym.wrappers.monitor import Monitor\nfrom sb3_contrib import RecurrentPPO\nfrom stable_baselines3.common.vec_env import DummyVecEnv\nfrom neurogym.utils.ngym_random import TruncExp\nfrom neurogym.utils.psychometric import plot_psychometric\n\n# Suppress warnings\nwarnings.filterwarnings(\"ignore\")\nclear_output()\n</pre> import warnings from IPython.display import clear_output import matplotlib.pyplot as plt import numpy as np import neurogym as ngym import torch.nn as nn import torch import gymnasium as gym import os from stable_baselines3.common.env_checker import check_env from neurogym.wrappers.monitor import Monitor from sb3_contrib import RecurrentPPO from stable_baselines3.common.vec_env import DummyVecEnv from neurogym.utils.ngym_random import TruncExp from neurogym.utils.psychometric import plot_psychometric  # Suppress warnings warnings.filterwarnings(\"ignore\") clear_output() In\u00a0[\u00a0]: Copied! <pre># Environment parameters\n# These settings are low to speed up testing; we recommend setting EVAL_TRIALS to at least 1000 and TRAIN_TRIALS to at least 6000\nEVAL_TRIALS = 100\nTRAIN_TRIALS = 100  # Choose the desired number of trials\nuse_expl_context = True  # Use explicit context mode\n\n# These are the default values, shown here to demonstrate how they can be modified:\ndt = 100\ndim_ring = 2  # Number of choices in the ring representation\nabort = False  # Whether to allow aborting the trial if the agent does not fixate\nrewards = {\n    \"abort\": -0.1,\n    \"correct\": +1.0,\n    \"fail\": 0.0\n}\ntiming = {\n    \"fixation\": 300,\n    \"stimulus\": 750,\n    \"delay\": TruncExp(600, 300, 3000),\n    \"decision\": 100,\n}\nsigma = 1.0 # Standard deviation of the Gaussian noise in the ring representation\n\n# We can modify any of these parameters by passing them to gym.make():\nkwargs = {\n    \"use_expl_context\": use_expl_context,\n    \"dt\": dt,\n    \"dim_ring\": dim_ring,\n    \"rewards\": rewards,\n    \"timing\": timing,\n    \"sigma\": sigma,\n    \"abort\": abort,\n}\n\n# Create and wrap the environment\ntask = \"ContextDecisionMaking-v0\"\nenv = gym.make(task, **kwargs)\n\n# Check the custom environment and output additional warnings (if any)\ncheck_env(env)\n\n# Print environment specifications\nprint(\"Trial timing (in milliseconds):\")\nprint(env.timing)\n\nprint(\"\\nObservation space structure:\")\nprint(env.observation_space)\nprint(\"Observation components:\")\nprint(env.observation_space.name)\n\nprint(\"\\nAction space structure:\")\nprint(env.action_space)\nprint(\"Action mapping:\")\nprint(env.action_space.name)\n</pre> # Environment parameters # These settings are low to speed up testing; we recommend setting EVAL_TRIALS to at least 1000 and TRAIN_TRIALS to at least 6000 EVAL_TRIALS = 100 TRAIN_TRIALS = 100  # Choose the desired number of trials use_expl_context = True  # Use explicit context mode  # These are the default values, shown here to demonstrate how they can be modified: dt = 100 dim_ring = 2  # Number of choices in the ring representation abort = False  # Whether to allow aborting the trial if the agent does not fixate rewards = {     \"abort\": -0.1,     \"correct\": +1.0,     \"fail\": 0.0 } timing = {     \"fixation\": 300,     \"stimulus\": 750,     \"delay\": TruncExp(600, 300, 3000),     \"decision\": 100, } sigma = 1.0 # Standard deviation of the Gaussian noise in the ring representation  # We can modify any of these parameters by passing them to gym.make(): kwargs = {     \"use_expl_context\": use_expl_context,     \"dt\": dt,     \"dim_ring\": dim_ring,     \"rewards\": rewards,     \"timing\": timing,     \"sigma\": sigma,     \"abort\": abort, }  # Create and wrap the environment task = \"ContextDecisionMaking-v0\" env = gym.make(task, **kwargs)  # Check the custom environment and output additional warnings (if any) check_env(env)  # Print environment specifications print(\"Trial timing (in milliseconds):\") print(env.timing)  print(\"\\nObservation space structure:\") print(env.observation_space) print(\"Observation components:\") print(env.observation_space.name)  print(\"\\nAction space structure:\") print(env.action_space) print(\"Action mapping:\") print(env.action_space.name) In\u00a0[\u00a0]: Copied! <pre>obs, info = env.reset()\n\n# Visualize example trials\nfig = ngym.utils.plot_env(\n    env,\n    name='Context Decision Making',\n    ob_traces=[\n        'Fixation',\n        'Stim 1, Mod 1',  # First stimulus component of modality 1\n        'Stim 2, Mod 1',  # Second stimulus component of modality 1\n        'Stim 1, Mod 2',  # First stimulus component of modality 2\n        'Stim 2, Mod 2',  # Second stimulus component of modality 2\n        'Context 1',      # First context signal\n        'Context 2',      # Second context signal\n    ],\n    num_trials=5\n)\n\n# Evaluate performance of the environment before training\neval_monitor = Monitor(\n    env\n)\nprint(\"\\nEvaluating random policy performance...\")\nmetrics = eval_monitor.evaluate_policy(num_trials=EVAL_TRIALS)\nprint(f\"\\nRandom policy metrics ({EVAL_TRIALS:,} trials):\")\nprint(f\"Mean performance: {metrics['mean_performance']:.4f}\")\nprint(f\"Mean reward: {metrics['mean_reward']:.4f}\")\n</pre> obs, info = env.reset()  # Visualize example trials fig = ngym.utils.plot_env(     env,     name='Context Decision Making',     ob_traces=[         'Fixation',         'Stim 1, Mod 1',  # First stimulus component of modality 1         'Stim 2, Mod 1',  # Second stimulus component of modality 1         'Stim 1, Mod 2',  # First stimulus component of modality 2         'Stim 2, Mod 2',  # Second stimulus component of modality 2         'Context 1',      # First context signal         'Context 2',      # Second context signal     ],     num_trials=5 )  # Evaluate performance of the environment before training eval_monitor = Monitor(     env ) print(\"\\nEvaluating random policy performance...\") metrics = eval_monitor.evaluate_policy(num_trials=EVAL_TRIALS) print(f\"\\nRandom policy metrics ({EVAL_TRIALS:,} trials):\") print(f\"Mean performance: {metrics['mean_performance']:.4f}\") print(f\"Mean reward: {metrics['mean_reward']:.4f}\") <p>As we can see, the agent's behavior is entirely random. It does not learn to fixate or to choose the correct option based on contextual cues. As a result, its performance is also random. Through training, we expect the agent to improve by learning to respect the fixation period, use context cues to infer the relevant context, and map signal peaks to the correct choices in the ring representation. Let\u2019s move on to training the agent to see whether it can learn these key aspects of the task.</p> In\u00a0[\u00a0]: Copied! <pre># Set the number of trials to train on\ntrial_length_stats = env.trial_length_stats(num_trials=EVAL_TRIALS)\navg_timesteps = int(trial_length_stats[\"mean\"])\ntotal_timesteps = TRAIN_TRIALS * avg_timesteps\nprint(f\"Training for {TRAIN_TRIALS:,} trials \u2248 {total_timesteps:,} timesteps\")\n\n# Configure monitoring with trial-appropriate parameters\ntrials_per_figure = 10  # Show 10 trials in each figure\nsteps_per_figure = int(trials_per_figure * avg_timesteps)\n\ntrain_monitor = Monitor(\n    env,\n    trigger=\"trial\",              # Save based on completed trials\n    interval=1000,                # Save data every 1000 trials\n    plot_create=True,             # Save visualization figures\n    plot_steps=steps_per_figure,  # Number of steps to visualize on the figure\n    verbose=True,                 # Print stats when data is saved\n)\n\n# DummyVecEnv is Stable-Baselines3 wrapper that converts the environment\n# into a vectorized form (required by PPO), allowing for parallel training of multiple environments\nenv_vec = DummyVecEnv([lambda: train_monitor])\n\n# Create and train Recurrent PPO agent\n# Set n_steps to be a multiple of your average trial length\ntrials_per_batch = 64\nn_steps = int(avg_timesteps * trials_per_batch)  # Collect approximately 64 trials per update\nbatch_size = 32  # Small batch size for short episodes\npolicy_kwargs = {\n    \"lstm_hidden_size\": 128,\n    \"n_lstm_layers\": 2,\n    \"shared_lstm\": True,          # Share LSTM to reduce parameters\n    \"enable_critic_lstm\": False,  # Disable separate LSTM for critic when sharing\n}\nrl_model = RecurrentPPO(\n    \"MlpLstmPolicy\",\n    env_vec,\n    learning_rate=3e-4,\n    n_steps=n_steps,          # Align with multiple complete episodes\n    batch_size=32,            # Smaller batch size\n    ent_coef=0,               # Entropy coefficient\n    policy_kwargs=policy_kwargs,\n    verbose=1\n)\n\nrl_model.learn(total_timesteps=total_timesteps, log_interval=int(total_timesteps/10))\nenv_vec.close()\n</pre> # Set the number of trials to train on trial_length_stats = env.trial_length_stats(num_trials=EVAL_TRIALS) avg_timesteps = int(trial_length_stats[\"mean\"]) total_timesteps = TRAIN_TRIALS * avg_timesteps print(f\"Training for {TRAIN_TRIALS:,} trials \u2248 {total_timesteps:,} timesteps\")  # Configure monitoring with trial-appropriate parameters trials_per_figure = 10  # Show 10 trials in each figure steps_per_figure = int(trials_per_figure * avg_timesteps)  train_monitor = Monitor(     env,     trigger=\"trial\",              # Save based on completed trials     interval=1000,                # Save data every 1000 trials     plot_create=True,             # Save visualization figures     plot_steps=steps_per_figure,  # Number of steps to visualize on the figure     verbose=True,                 # Print stats when data is saved )  # DummyVecEnv is Stable-Baselines3 wrapper that converts the environment # into a vectorized form (required by PPO), allowing for parallel training of multiple environments env_vec = DummyVecEnv([lambda: train_monitor])  # Create and train Recurrent PPO agent # Set n_steps to be a multiple of your average trial length trials_per_batch = 64 n_steps = int(avg_timesteps * trials_per_batch)  # Collect approximately 64 trials per update batch_size = 32  # Small batch size for short episodes policy_kwargs = {     \"lstm_hidden_size\": 128,     \"n_lstm_layers\": 2,     \"shared_lstm\": True,          # Share LSTM to reduce parameters     \"enable_critic_lstm\": False,  # Disable separate LSTM for critic when sharing } rl_model = RecurrentPPO(     \"MlpLstmPolicy\",     env_vec,     learning_rate=3e-4,     n_steps=n_steps,          # Align with multiple complete episodes     batch_size=32,            # Smaller batch size     ent_coef=0,               # Entropy coefficient     policy_kwargs=policy_kwargs,     verbose=1 )  rl_model.learn(total_timesteps=total_timesteps, log_interval=int(total_timesteps/10)) env_vec.close() In\u00a0[\u00a0]: Copied! <pre># Plot example trials with trained agent\nfig = ngym.utils.plot_env(\n    env_vec,\n    name='Context Decision Making (trained)',\n    ob_traces=[\n        'Fixation',\n        'Stim 1, Mod 1',  # First stimulus component of modality 1\n        'Stim 2, Mod 1',  # Second stimulus component of modality 1\n        'Stim 1, Mod 2',  # First stimulus component of modality 2\n        'Stim 2, Mod 2',  # Second stimulus component of modality 2\n        'Context 1',      # First context signal\n        'Context 2',      # Second context signal\n    ],\n    num_trials=5,\n    model=rl_model,\n)\n</pre> # Plot example trials with trained agent fig = ngym.utils.plot_env(     env_vec,     name='Context Decision Making (trained)',     ob_traces=[         'Fixation',         'Stim 1, Mod 1',  # First stimulus component of modality 1         'Stim 2, Mod 1',  # Second stimulus component of modality 1         'Stim 1, Mod 2',  # First stimulus component of modality 2         'Stim 2, Mod 2',  # Second stimulus component of modality 2         'Context 1',      # First context signal         'Context 2',      # Second context signal     ],     num_trials=5,     model=rl_model, ) <p>After training, we visualize the agent's behavior on a few example trials. In contrast to the random agent, we should now see:</p> <ul> <li>Consistent fixation maintenance during the fixation period</li> <li>Choices that correlate with the evidence in the modality indicated by the context signal</li> <li>No systematic relationship between choices and signals from the irrelevant modality</li> <li>Performance significantly above chance level (0.5), reflecting successful context-dependent decisions</li> </ul> <p>The plot shows the trained agent's behavior across 5 example trials, allowing us to visualize how well it has learned to flexibly switch its attention between modalities based on the context signal and make appropriate choices using the ring representation.</p> In\u00a0[\u00a0]: Copied! <pre># Evaluate performance of the trained model\nprint(\"\\nEvaluating trained model performance...\")\nrl_trained_metrics = eval_monitor.evaluate_policy(num_trials=EVAL_TRIALS, model=rl_model)\nprint(f\"\\nTrained model metrics ({EVAL_TRIALS:,} trials):\")\nprint(f\"Mean performance: {rl_trained_metrics['mean_performance']:.4f}\")\nprint(f\"Mean reward: {rl_trained_metrics['mean_reward']:.4f}\")\n\nfig = train_monitor.plot_training_history(figsize=(6, 4), plot_performance=False)\n</pre> # Evaluate performance of the trained model print(\"\\nEvaluating trained model performance...\") rl_trained_metrics = eval_monitor.evaluate_policy(num_trials=EVAL_TRIALS, model=rl_model) print(f\"\\nTrained model metrics ({EVAL_TRIALS:,} trials):\") print(f\"Mean performance: {rl_trained_metrics['mean_performance']:.4f}\") print(f\"Mean reward: {rl_trained_metrics['mean_reward']:.4f}\")  fig = train_monitor.plot_training_history(figsize=(6, 4), plot_performance=False) In\u00a0[\u00a0]: Copied! <pre>if not os.getenv(\"GITHUB_ACTIONS\"):\n    # Evaluate policy and extract data\n    eval_monitor.evaluate_policy(num_trials=EVAL_TRIALS, model=rl_model)\n    data = eval_monitor.data_eval\n\n    # Extract trial-level fields\n    trials = data['trial']\n    other_choices = np.array([trial['other_choice'] == 2 for trial in trials], dtype=int)\n    coh_1 = np.array([trial['coh_1'] for trial in trials])\n    coh_2 = np.array([trial['coh_2'] for trial in trials])\n    contexts = np.array([trial['context'] for trial in trials])\n\n    # Filter out trials where action is 0 (no action taken)\n    actions_only_mask = data['action'] != 0\n    data_action = data['action'][actions_only_mask]\n    data_gt = data['gt'][actions_only_mask]\n    data_oc = other_choices[actions_only_mask]\n    coh_1 = coh_1[actions_only_mask]\n    coh_2 = coh_2[actions_only_mask]\n    contexts = contexts[actions_only_mask]\n\n    # Convert actions and ground truth to binary (0 = left, 1 = right)\n    choices = (data_action == 2).astype(int)\n    ground_truth = (data_gt == 2).astype(int)\n    data_oc = (data_oc == 2).astype(int)\n\n    # Plotting setup\n    fig, axs = plt.subplots(2, 1, figsize=(5, 5), sharex=True, sharey=True,\n                            gridspec_kw={'hspace': 0.5})\n\n    evidence_sources = [('coh_1', coh_1), ('coh_2', coh_2)]\n    context_ids = [0, 1]\n\n    # Plot psychometric curves for each context and evidence type\n    for i, context_id in enumerate(context_ids):\n        ax = axs[i]\n        title = f'Context {context_id + 1}'\n\n        for j, (label, evidence) in enumerate(evidence_sources):\n            # Filter trials matching current context\n            mask = contexts == context_id\n            ev = evidence[mask]\n            ch = choices[mask]\n\n            # Choose correct reference: ground truth or other agent\n            reference = ground_truth[mask] if context_id == j else data_oc[mask]\n\n            # Signed evidence: negative if correct answer is left\n            signed_ev = np.where(reference == 0, -ev, ev)\n\n            plot_psychometric(signed_ev, ch, ax, title=title, legend=label)\n\n    plt.tight_layout()\n    plt.show()\n</pre> if not os.getenv(\"GITHUB_ACTIONS\"):     # Evaluate policy and extract data     eval_monitor.evaluate_policy(num_trials=EVAL_TRIALS, model=rl_model)     data = eval_monitor.data_eval      # Extract trial-level fields     trials = data['trial']     other_choices = np.array([trial['other_choice'] == 2 for trial in trials], dtype=int)     coh_1 = np.array([trial['coh_1'] for trial in trials])     coh_2 = np.array([trial['coh_2'] for trial in trials])     contexts = np.array([trial['context'] for trial in trials])      # Filter out trials where action is 0 (no action taken)     actions_only_mask = data['action'] != 0     data_action = data['action'][actions_only_mask]     data_gt = data['gt'][actions_only_mask]     data_oc = other_choices[actions_only_mask]     coh_1 = coh_1[actions_only_mask]     coh_2 = coh_2[actions_only_mask]     contexts = contexts[actions_only_mask]      # Convert actions and ground truth to binary (0 = left, 1 = right)     choices = (data_action == 2).astype(int)     ground_truth = (data_gt == 2).astype(int)     data_oc = (data_oc == 2).astype(int)      # Plotting setup     fig, axs = plt.subplots(2, 1, figsize=(5, 5), sharex=True, sharey=True,                             gridspec_kw={'hspace': 0.5})      evidence_sources = [('coh_1', coh_1), ('coh_2', coh_2)]     context_ids = [0, 1]      # Plot psychometric curves for each context and evidence type     for i, context_id in enumerate(context_ids):         ax = axs[i]         title = f'Context {context_id + 1}'          for j, (label, evidence) in enumerate(evidence_sources):             # Filter trials matching current context             mask = contexts == context_id             ev = evidence[mask]             ch = choices[mask]              # Choose correct reference: ground truth or other agent             reference = ground_truth[mask] if context_id == j else data_oc[mask]              # Signed evidence: negative if correct answer is left             signed_ev = np.where(reference == 0, -ev, ev)              plot_psychometric(signed_ev, ch, ax, title=title, legend=label)      plt.tight_layout()     plt.show() In\u00a0[\u00a0]: Copied! <pre># Environment parameters\ntask = 'ContextDecisionMaking-v0'\nkwargs = {\n    \"use_expl_context\": use_expl_context,\n    \"dt\": dt,\n    \"dim_ring\": dim_ring,\n    \"rewards\": rewards,\n    \"timing\": timing,\n    \"sigma\": sigma,\n    \"abort\": abort,\n}\n\n# Set seq_len to capture 95% of trials, and multiply by 2\nseq_len = 2*int(trial_length_stats[\"percentile_95\"])\nprint(f\"Using sequence length: {seq_len}\")\n\n# Make supervised dataset\nprint(f\"Creating dataset with batch_size={batch_size}\")\ndataset = ngym.Dataset(task, env_kwargs=kwargs, batch_size=batch_size, seq_len=seq_len)\n\nenv = dataset.env\n\n# Extract dimensions from environment\nob_size = env.observation_space.shape[0]\nact_size = env.action_space.n\n\nprint(f\"Observation size: {ob_size}\")\nprint(f\"Action size: {act_size}\")\n\n# Get a batch of data\ninputs, target = dataset()\n\n# Display shapes and content\nprint(f\"Input batch shape: {inputs.shape}\")\nprint(f\"Target batch shape: {target.shape}\")\n\n# Set device\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Using device: {device}\")\n</pre> # Environment parameters task = 'ContextDecisionMaking-v0' kwargs = {     \"use_expl_context\": use_expl_context,     \"dt\": dt,     \"dim_ring\": dim_ring,     \"rewards\": rewards,     \"timing\": timing,     \"sigma\": sigma,     \"abort\": abort, }  # Set seq_len to capture 95% of trials, and multiply by 2 seq_len = 2*int(trial_length_stats[\"percentile_95\"]) print(f\"Using sequence length: {seq_len}\")  # Make supervised dataset print(f\"Creating dataset with batch_size={batch_size}\") dataset = ngym.Dataset(task, env_kwargs=kwargs, batch_size=batch_size, seq_len=seq_len)  env = dataset.env  # Extract dimensions from environment ob_size = env.observation_space.shape[0] act_size = env.action_space.n  print(f\"Observation size: {ob_size}\") print(f\"Action size: {act_size}\")  # Get a batch of data inputs, target = dataset()  # Display shapes and content print(f\"Input batch shape: {inputs.shape}\") print(f\"Target batch shape: {target.shape}\")  # Set device device = 'cuda' if torch.cuda.is_available() else 'cpu' print(f\"Using device: {device}\") In\u00a0[\u00a0]: Copied! <pre>class Net(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(Net, self).__init__()\n        self.hidden_size = hidden_size\n        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=False)\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x, hidden=None):\n        # Forward pass with hidden state tracking\n        lstm_out, hidden = self.lstm(x, hidden)\n        output = self.fc(lstm_out)\n        return output, hidden\n\n    def init_hidden(self, batch_size, device):\n        \"\"\"Initialize hidden state\"\"\"\n        h0 = torch.zeros(1, batch_size, self.hidden_size).to(device)\n        c0 = torch.zeros(1, batch_size, self.hidden_size).to(device)\n        return (h0, c0)\n\n# Create the model\nhidden_size = 128\nsl_model = Net(\n    input_size=ob_size,\n    hidden_size=hidden_size,\n    output_size=act_size,\n).to(device)\n</pre> class Net(nn.Module):     def __init__(self, input_size, hidden_size, output_size):         super(Net, self).__init__()         self.hidden_size = hidden_size         self.lstm = nn.LSTM(input_size, hidden_size, batch_first=False)         self.fc = nn.Linear(hidden_size, output_size)      def forward(self, x, hidden=None):         # Forward pass with hidden state tracking         lstm_out, hidden = self.lstm(x, hidden)         output = self.fc(lstm_out)         return output, hidden      def init_hidden(self, batch_size, device):         \"\"\"Initialize hidden state\"\"\"         h0 = torch.zeros(1, batch_size, self.hidden_size).to(device)         c0 = torch.zeros(1, batch_size, self.hidden_size).to(device)         return (h0, c0)  # Create the model hidden_size = 128 sl_model = Net(     input_size=ob_size,     hidden_size=hidden_size,     output_size=act_size, ).to(device) In\u00a0[\u00a0]: Copied! <pre># This setting is low to speed up testing, we recommend setting it to at least 1000\nEPOCHS = 100\n\n# This weighting deprioritizes class 0 while keeping classes 1 and 2 equally important,\n# aligning with the reward distribution idea from the RL setting\nclass_weights = torch.tensor([0.05, 1, 1]).to(device)\n# Define the optimizer and loss function\noptimizer = torch.optim.Adam(sl_model.parameters(), lr=0.01, weight_decay=1e-5)\ncriterion = nn.CrossEntropyLoss(weight=class_weights)\n\n# Training loop\nloss_history = []\n\nfor i in range(EPOCHS):\n    # Get a batch of data\n    inputs, targets = dataset()\n\n    # Convert to PyTorch tensors\n    inputs = torch.from_numpy(inputs).float().to(device)\n    targets = torch.from_numpy(targets).long().to(device)\n\n    # Initialize hidden state\n    hidden = sl_model.init_hidden(inputs.size(1), device)\n\n    # Zero gradients\n    optimizer.zero_grad()\n\n    # Forward pass with hidden state tracking\n    outputs, _ = sl_model(inputs, hidden)\n\n    # Reshape for CrossEntropyLoss\n    outputs_flat = outputs.reshape(-1, outputs.size(2))\n    targets_flat = targets.reshape(-1)\n\n    # Calculate loss\n    # Weight the loss to account for class imbalance (very low weight to 0s, higher weights to 1s and 2s)\n    loss = criterion(outputs_flat, targets_flat)\n\n    # Backward pass and optimize\n    loss.backward()\n    optimizer.step()\n\n    # print statistics\n    loss_history.append(loss.item())\n    if i % 50 == 0:\n        print('Epoch [{}/{}], Loss: {:.4f}'.format(i, EPOCHS, loss.item()))\n\nprint('Finished Training')\n\n# Plot the loss curve\nplt.figure(figsize=(8, 4))\nplt.plot(loss_history)\nplt.title('Training Loss')\nplt.xlabel('Iterations')\nplt.ylabel('Loss (50-iteration moving average)')\nplt.grid(True, alpha=0.3)\nplt.show()\n</pre> # This setting is low to speed up testing, we recommend setting it to at least 1000 EPOCHS = 100  # This weighting deprioritizes class 0 while keeping classes 1 and 2 equally important, # aligning with the reward distribution idea from the RL setting class_weights = torch.tensor([0.05, 1, 1]).to(device) # Define the optimizer and loss function optimizer = torch.optim.Adam(sl_model.parameters(), lr=0.01, weight_decay=1e-5) criterion = nn.CrossEntropyLoss(weight=class_weights)  # Training loop loss_history = []  for i in range(EPOCHS):     # Get a batch of data     inputs, targets = dataset()      # Convert to PyTorch tensors     inputs = torch.from_numpy(inputs).float().to(device)     targets = torch.from_numpy(targets).long().to(device)      # Initialize hidden state     hidden = sl_model.init_hidden(inputs.size(1), device)      # Zero gradients     optimizer.zero_grad()      # Forward pass with hidden state tracking     outputs, _ = sl_model(inputs, hidden)      # Reshape for CrossEntropyLoss     outputs_flat = outputs.reshape(-1, outputs.size(2))     targets_flat = targets.reshape(-1)      # Calculate loss     # Weight the loss to account for class imbalance (very low weight to 0s, higher weights to 1s and 2s)     loss = criterion(outputs_flat, targets_flat)      # Backward pass and optimize     loss.backward()     optimizer.step()      # print statistics     loss_history.append(loss.item())     if i % 50 == 0:         print('Epoch [{}/{}], Loss: {:.4f}'.format(i, EPOCHS, loss.item()))  print('Finished Training')  # Plot the loss curve plt.figure(figsize=(8, 4)) plt.plot(loss_history) plt.title('Training Loss') plt.xlabel('Iterations') plt.ylabel('Loss (50-iteration moving average)') plt.grid(True, alpha=0.3) plt.show() In\u00a0[\u00a0]: Copied! <pre>verbose = False\n# Evaluate performance of the trained model\nsl_model.eval()\n\ndata_sl = {\"action\": [], \"gt\": [], \"oc\": [], \"trial\": []}\ntotal_correct = 0\ntotal_trials = 0\n\n# Evaluate for specified number of trials\nprint(f\"Evaluating model performance across {EVAL_TRIALS} trials...\")\n\nfor i in range(EVAL_TRIALS):\n    # Generate a new trial\n    trial = env.new_trial()\n    data_sl[\"trial\"].append(trial)\n    ob, gt = env.ob, env.gt\n    data_sl[\"gt\"].append(gt[-1])\n    data_sl[\"oc\"].append(trial['other_choice'])\n    if verbose:\n        print(\"observation shape:\", ob.shape)\n        print(\"ground truth shape:\", gt.shape)\n\n    # Handle potentially variable-length trials\n    trial_length = ob.shape[0]\n\n    # Add batch dimension to observation\n    ob = ob[:, np.newaxis, :]\n\n    # Convert to tensor\n    inputs = torch.from_numpy(ob).float().to(device)\n    if verbose:\n        print(\"inputs shape:\", inputs.shape)\n\n    # Initialize hidden state for new trial\n    hidden = sl_model.init_hidden(1, device)\n\n    with torch.no_grad():\n        outputs, _ = sl_model(inputs, hidden)\n        pred_actions = torch.argmax(outputs, dim=2)\n        data_sl[\"action\"].append(pred_actions[-1, 0].cpu().numpy())\n        if verbose:\n            print(\"outputs shape:\", outputs.shape)\n            print(\"predicted actions shape:\", pred_actions.shape)\n            print(\"predicted actions:\", pred_actions)\n            print(\"ground truth:\", gt)\n\n    # Check only the decision period\n    decision_idx = trial_length - 1  # Assuming decision is at the end\n    is_correct = (gt[decision_idx] == pred_actions[decision_idx, 0].cpu().numpy())\n\n    total_correct += is_correct\n\n    if (i+1) % 100 == 0:\n        print(f\"Completed {i+1}/{EVAL_TRIALS} trials | Accuracy: {total_correct/(i+1):.4f}\")\n\n# Calculate mean performance\nsl_mean_performance = total_correct / EVAL_TRIALS\n\nfor key in data_sl:\n    if key != \"trial\":\n        data_sl[key] = np.array(data_sl[key])\n\nprint(f\"Mean performance across {EVAL_TRIALS} trials: {sl_mean_performance:.4f}\")\n</pre> verbose = False # Evaluate performance of the trained model sl_model.eval()  data_sl = {\"action\": [], \"gt\": [], \"oc\": [], \"trial\": []} total_correct = 0 total_trials = 0  # Evaluate for specified number of trials print(f\"Evaluating model performance across {EVAL_TRIALS} trials...\")  for i in range(EVAL_TRIALS):     # Generate a new trial     trial = env.new_trial()     data_sl[\"trial\"].append(trial)     ob, gt = env.ob, env.gt     data_sl[\"gt\"].append(gt[-1])     data_sl[\"oc\"].append(trial['other_choice'])     if verbose:         print(\"observation shape:\", ob.shape)         print(\"ground truth shape:\", gt.shape)      # Handle potentially variable-length trials     trial_length = ob.shape[0]      # Add batch dimension to observation     ob = ob[:, np.newaxis, :]      # Convert to tensor     inputs = torch.from_numpy(ob).float().to(device)     if verbose:         print(\"inputs shape:\", inputs.shape)      # Initialize hidden state for new trial     hidden = sl_model.init_hidden(1, device)      with torch.no_grad():         outputs, _ = sl_model(inputs, hidden)         pred_actions = torch.argmax(outputs, dim=2)         data_sl[\"action\"].append(pred_actions[-1, 0].cpu().numpy())         if verbose:             print(\"outputs shape:\", outputs.shape)             print(\"predicted actions shape:\", pred_actions.shape)             print(\"predicted actions:\", pred_actions)             print(\"ground truth:\", gt)      # Check only the decision period     decision_idx = trial_length - 1  # Assuming decision is at the end     is_correct = (gt[decision_idx] == pred_actions[decision_idx, 0].cpu().numpy())      total_correct += is_correct      if (i+1) % 100 == 0:         print(f\"Completed {i+1}/{EVAL_TRIALS} trials | Accuracy: {total_correct/(i+1):.4f}\")  # Calculate mean performance sl_mean_performance = total_correct / EVAL_TRIALS  for key in data_sl:     if key != \"trial\":         data_sl[key] = np.array(data_sl[key])  print(f\"Mean performance across {EVAL_TRIALS} trials: {sl_mean_performance:.4f}\") In\u00a0[\u00a0]: Copied! <pre>if not os.getenv(\"GITHUB_ACTIONS\"):\n    # Extract trial-level fields\n    trials = data_sl['trial']\n    other_choices = np.array([trial['other_choice'] == 2 for trial in trials], dtype=int)\n    coh_1 = np.array([trial['coh_1'] for trial in trials])\n    coh_2 = np.array([trial['coh_2'] for trial in trials])\n    contexts = np.array([trial['context'] for trial in trials])\n\n    # Filter out trials where action is 0 (no action taken)\n    actions_only_mask = data_sl['action'] != 0\n    data_action = data_sl['action'][actions_only_mask]\n    data_gt = data_sl['gt'][actions_only_mask]\n    data_oc = other_choices[actions_only_mask]\n    coh_1 = coh_1[actions_only_mask]\n    coh_2 = coh_2[actions_only_mask]\n    contexts = contexts[actions_only_mask]\n\n    # Convert actions and ground truth to binary (0 = left, 1 = right)\n    choices = (data_action == 2).astype(int)\n    ground_truth = (data_gt == 2).astype(int)\n    data_oc = (data_oc == 2).astype(int)\n\n    # Plotting setup\n    fig, axs = plt.subplots(2, 1, figsize=(5, 5), sharex=True, sharey=True,\n                            gridspec_kw={'hspace': 0.5})\n\n    evidence_sources = [('coh_1', coh_1), ('coh_2', coh_2)]\n    context_ids = [0, 1]\n\n    # Plot psychometric curves for each context and evidence type\n    for i, context_id in enumerate(context_ids):\n        ax = axs[i]\n        title = f'Context {context_id + 1}'\n\n        for j, (label, evidence) in enumerate(evidence_sources):\n            # Filter trials matching current context\n            mask = contexts == context_id\n            ev = evidence[mask]\n            ch = choices[mask]\n\n            # Choose correct reference: ground truth or other agent\n            reference = ground_truth[mask] if context_id == j else data_oc[mask]\n\n            # Signed evidence: negative if correct answer is left\n            signed_ev = np.where(reference == 0, -ev, ev)\n\n            plot_psychometric(signed_ev, ch, ax, title=title, legend=label)\n\n    plt.tight_layout()\n    plt.show()\n</pre> if not os.getenv(\"GITHUB_ACTIONS\"):     # Extract trial-level fields     trials = data_sl['trial']     other_choices = np.array([trial['other_choice'] == 2 for trial in trials], dtype=int)     coh_1 = np.array([trial['coh_1'] for trial in trials])     coh_2 = np.array([trial['coh_2'] for trial in trials])     contexts = np.array([trial['context'] for trial in trials])      # Filter out trials where action is 0 (no action taken)     actions_only_mask = data_sl['action'] != 0     data_action = data_sl['action'][actions_only_mask]     data_gt = data_sl['gt'][actions_only_mask]     data_oc = other_choices[actions_only_mask]     coh_1 = coh_1[actions_only_mask]     coh_2 = coh_2[actions_only_mask]     contexts = contexts[actions_only_mask]      # Convert actions and ground truth to binary (0 = left, 1 = right)     choices = (data_action == 2).astype(int)     ground_truth = (data_gt == 2).astype(int)     data_oc = (data_oc == 2).astype(int)      # Plotting setup     fig, axs = plt.subplots(2, 1, figsize=(5, 5), sharex=True, sharey=True,                             gridspec_kw={'hspace': 0.5})      evidence_sources = [('coh_1', coh_1), ('coh_2', coh_2)]     context_ids = [0, 1]      # Plot psychometric curves for each context and evidence type     for i, context_id in enumerate(context_ids):         ax = axs[i]         title = f'Context {context_id + 1}'          for j, (label, evidence) in enumerate(evidence_sources):             # Filter trials matching current context             mask = contexts == context_id             ev = evidence[mask]             ch = choices[mask]              # Choose correct reference: ground truth or other agent             reference = ground_truth[mask] if context_id == j else data_oc[mask]              # Signed evidence: negative if correct answer is left             signed_ev = np.where(reference == 0, -ev, ev)              plot_psychometric(signed_ev, ch, ax, title=title, legend=label)      plt.tight_layout()     plt.show() In\u00a0[\u00a0]: Copied! <pre># Print performance comparison\nrl_performance = rl_trained_metrics['mean_performance']\nsl_performance = sl_mean_performance\n\nprint(f\"RL model performance: {rl_performance:.4f}\")\nprint(f\"SL model performance: {sl_performance:.4f}\")\n</pre> # Print performance comparison rl_performance = rl_trained_metrics['mean_performance'] sl_performance = sl_mean_performance  print(f\"RL model performance: {rl_performance:.4f}\") print(f\"SL model performance: {sl_performance:.4f}\")"},{"location":"examples/contextdecisionmaking/#context-dependent-decision-making-task","title":"Context-Dependent Decision Making Task\u00b6","text":"<p>This environment implements a context-dependent perceptual decision-making task, in which the agent has to perform a perceptual decision that depends on a context that is explicitly indicated on each trial. The environment is a simplified version of the original task (Mante et al. 2013), that tests the agents' ability to flexibly switch between different contexts by making them choose between two options (left or right) based on the stimulus evidence associated with the relevant context (e.g., motion and color). The key features of the task are:</p> <ol> <li><p>The relevant context is explicitly signaled on each trial.</p> </li> <li><p>Choices are represented as angles evenly spaced around a circle. Note that the number of choices can be configured via <code>dim_ring</code>. With the default of 2 choices, this corresponds to:</p> <ul> <li>Position 1: 0\u00b0 (left choice)</li> <li>Position 2: 180\u00b0 (right choice)</li> </ul> </li> <li><p>Stimulus for each context is represented as a cosine modulation peaked at one of these positions.</p> </li> <li><p>The correct choice (ground truth) is randomly chosen on each trials (for 2 choices: left or right).</p> </li> <li><p>The stimulus evidence (coherence) of each context is also randomly chosen on each trial.</p> </li> </ol> <p>For example, if the context signal indicates modality 0, the agent must:</p> <ul> <li>Choose position 1 (left) when context 0's coherence peaks at 0\u00b0</li> <li>Choose position 2 (right) when context 0's coherence peaks at 180\u00b0</li> </ul> <p>In this notebook, we will:</p> <ol> <li>Train an agent on the task using supervised learning and reinforcement learning with Stable-Baselines3.</li> <li>Compare the behavior of agents trained with supervised learning and reinforcement learning.</li> </ol>"},{"location":"examples/contextdecisionmaking/#0-install-dependencies","title":"0. Install Dependencies\u00b6","text":"<p>To begin, install the <code>neurogym</code> package. This will automatically install all required dependencies, including Stable-Baselines3.</p> <p>For detailed instructions on how to install <code>neurogym</code> within a conda environment or in editable mode, refer to the installation instructions.</p>"},{"location":"examples/contextdecisionmaking/#1-training-an-agent-on-the-context-dependent-decision-making-task","title":"1. Training an Agent on the Context-Dependent Decision Making Task\u00b6","text":""},{"location":"examples/contextdecisionmaking/#11-environment-setup-and-initial-agent-behavior","title":"1.1 Environment Setup and Initial Agent Behavior\u00b6","text":"<p>Let's now create and explore the environment using the <code>ContextDecisionMaking</code> class from neurogym. We'll use the default configuration for explicit context mode (<code>use_expl_context = True</code>) which includes:</p> <ul> <li><code>dim_ring = 2</code>: Two possible choices (left/right) represented at 0\u00b0 and 180\u00b0. Note that the ring architecture can support any number of choices, making it suitable for more complex decision-making scenarios. The environment will provide context signals indicating which modality is relevant for each trial, allowing the agent to flexibly adapt its decision strategy.</li> <li><code>timing = {'fixation': 300, 'stimulus': 750, 'delay': ~600, 'decision': 100}</code> (in milliseconds).</li> <li><code>rewards = {'abort': -0.1, 'correct': +1.0}</code>; abort is a penalty applied when the agent fails to fixate. The task allows the trial to be aborted if fixation does not occur, which is where the name of this penalty comes from.</li> <li><code>sigma = 1.0</code>: Standard deviation of the noise added to the inputs.</li> </ul>"},{"location":"examples/contextdecisionmaking/#111-import-libraries","title":"1.1.1 Import Libraries\u00b6","text":""},{"location":"examples/contextdecisionmaking/#112-environment-setup","title":"1.1.2 Environment Setup\u00b6","text":""},{"location":"examples/contextdecisionmaking/#113-random-agent-behavior","title":"1.1.3 Random Agent Behavior\u00b6","text":"<p>Let's now plot the behavior of a random agent on the task. The agent will randomly choose between the two options (left/right, blue line), and we will visualize its behavior over 5 trials. We will also plot the reward received by the agent at each time step, as well as the performance on each trial. Note that performance is only defined at the end of a trial: it is 1 if the agent made the correct choice, and 0 otherwise.</p> <p>To keep track of the agent's behavior, we will use the <code>Monitor</code> wrapper, which monitors training by:</p> <ul> <li>Tracking and saving behavioral data (rewards, actions, observations) every <code>sv_per</code> steps.</li> <li>Generating visualization figures during training if<code> sv_fig=True</code>.</li> <li>Providing progress information if <code>verbose=True</code>.</li> </ul> <p>Here, we\u2019ll use the wrapper solely to compute the agent\u2019s performance, but later it will help us assess learning and save intermediate results.</p>"},{"location":"examples/contextdecisionmaking/#12-training-and-evaluating-the-agent","title":"1.2 Training and Evaluating the Agent\u00b6","text":"<p>We will now train the agent using Stable-Baselines3\u2019s implementation of PPO (Proximal Policy Optimization), a widely used reinforcement learning algorithm known for its stability and efficiency.</p> <p>To support recurrent policies, we will use RecurrentPPO, which extends PPO with recurrent neural networks, specifically LSTMs.</p>"},{"location":"examples/contextdecisionmaking/#121-training-the-agent","title":"1.2.1 Training the Agent\u00b6","text":""},{"location":"examples/contextdecisionmaking/#122-plot-the-behavior-of-the-trained-agent","title":"1.2.2 Plot the Behavior of the Trained Agent\u00b6","text":""},{"location":"examples/contextdecisionmaking/#123-evaluate-the-agents-performance","title":"1.2.3 Evaluate the Agent's Performance\u00b6","text":""},{"location":"examples/contextdecisionmaking/#124-plot-the-agents-psychometric-curves","title":"1.2.4 Plot the Agent's Psychometric Curves\u00b6","text":""},{"location":"examples/contextdecisionmaking/#2-learning-the-task-as-a-supervised-problem","title":"2. Learning the Task as a Supervised Problem\u00b6","text":"<p>We will now train the agent using supervised learning. NeuroGym provides functionality to generate a dataset directly from the environment, allowing us to sample batches of inputs and corresponding labels for training.</p>"},{"location":"examples/contextdecisionmaking/#21-converting-the-environment-to-a-supervised-dataset","title":"2.1 Converting the Environment to a Supervised Dataset\u00b6","text":""},{"location":"examples/contextdecisionmaking/#211-dataset-setup","title":"2.1.1 Dataset Setup\u00b6","text":""},{"location":"examples/contextdecisionmaking/#212-model-setup","title":"2.1.2 Model Setup\u00b6","text":""},{"location":"examples/contextdecisionmaking/#22-training-and-evaluating-a-neural-network-model","title":"2.2 Training and Evaluating a Neural Network Model\u00b6","text":""},{"location":"examples/contextdecisionmaking/#221-training-the-model","title":"2.2.1 Training the Model\u00b6","text":""},{"location":"examples/contextdecisionmaking/#222-evaluate-the-models-performance","title":"2.2.2 Evaluate the Model's Performance\u00b6","text":""},{"location":"examples/contextdecisionmaking/#223-plot-the-models-psychometric-curves","title":"2.2.3 Plot the Model's Psychometric Curves\u00b6","text":""},{"location":"examples/contextdecisionmaking/#224-comparing-rl-and-sl-approaches","title":"2.2.4 Comparing RL and SL Approaches\u00b6","text":""},{"location":"examples/demo/","title":"Simple demo notebook","text":"<p>NeuroGym is a comprehensive toolkit that allows training any network model on many established neuroscience tasks using Reinforcement Learning techniques. It includes working memory tasks, value-based decision tasks and context-dependent perceptual categorization tasks.</p> <p>In this notebook we first show how to install the relevant toolbox.</p> <p>We then show how to access the available tasks and their relevant information.</p> <p>Finally we train a feedforward neural network (MLP policy) on the Random Dots Motion task using the A2C algorithm Mnih et al. 2016 implemented in the stable-baselines3 toolbox, and plot the results.</p> <p>You can easily change the code to train a network on any other available task or using a different algorithm (e.g. ACER, PPO2).</p> In\u00a0[\u00a0]: Copied! <pre># ! pip install neurogym[rl]\n</pre> # ! pip install neurogym[rl] In\u00a0[\u00a0]: Copied! <pre>import warnings\nwarnings.filterwarnings('ignore')\nimport gymnasium as gym\nfrom neurogym.utils import info, plotting\nfrom IPython.display import clear_output\nclear_output()\n\ninfo.all_tasks()\n</pre> import warnings warnings.filterwarnings('ignore') import gymnasium as gym from neurogym.utils import info, plotting from IPython.display import clear_output clear_output()  info.all_tasks() In\u00a0[\u00a0]: Copied! <pre>task = 'GoNogo-v0'\nenv = gym.make(task)\nprint(env)\nfig = plotting.plot_env(\n    env,\n    num_steps=100,\n    # def_act=0,\n    ob_traces=['Fixation cue', 'NoGo', 'Go'],\n    # fig_kwargs={'figsize': (12, 12)}\n    )\n</pre> task = 'GoNogo-v0' env = gym.make(task) print(env) fig = plotting.plot_env(     env,     num_steps=100,     # def_act=0,     ob_traces=['Fixation cue', 'NoGo', 'Go'],     # fig_kwargs={'figsize': (12, 12)}     ) In\u00a0[\u00a0]: Copied! <pre>info.all_wrappers()\n</pre> info.all_wrappers() In\u00a0[\u00a0]: Copied! <pre>info.info_wrapper('TrialHistoryV2-v0')\n</pre> info.info_wrapper('TrialHistoryV2-v0') In\u00a0[\u00a0]: Copied! <pre>import warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nfrom neurogym.wrappers import monitor, TrialHistoryV2\nfrom stable_baselines3.common.vec_env import DummyVecEnv\nfrom stable_baselines3 import A2C  # ACER, PPO2\n# task paremters\ntiming = {'fixation': ('constant', 300),\n          'stimulus': ('constant', 700),\n          'decision': ('constant', 300)}\nkwargs = {'dt': 100, 'timing': timing}\n# wrapper parameters\nn_ch = 2\np = 0.8\nnum_blocks = 2\nprobs = np.array([[p, 1-p], [1-p, p]])  # repeating block\n\n# Build the task\nenv = gym.make(task, **kwargs)\n\n# Apply the wrapper.\nenv = TrialHistoryV2(env, probs=probs)\nenv = monitor.Monitor(env, config=\"config.toml\")\n</pre> import warnings warnings.filterwarnings('ignore') import numpy as np from neurogym.wrappers import monitor, TrialHistoryV2 from stable_baselines3.common.vec_env import DummyVecEnv from stable_baselines3 import A2C  # ACER, PPO2 # task paremters timing = {'fixation': ('constant', 300),           'stimulus': ('constant', 700),           'decision': ('constant', 300)} kwargs = {'dt': 100, 'timing': timing} # wrapper parameters n_ch = 2 p = 0.8 num_blocks = 2 probs = np.array([[p, 1-p], [1-p, p]])  # repeating block  # Build the task env = gym.make(task, **kwargs)  # Apply the wrapper. env = TrialHistoryV2(env, probs=probs) env = monitor.Monitor(env, config=\"config.toml\") In\u00a0[\u00a0]: Copied! <pre># the env is now wrapped automatically when passing it to the constructor\nmodel = A2C(\"MlpPolicy\", env, verbose=1, policy_kwargs={'net_arch': [64, 64]})\nmodel.learn(total_timesteps=env.config.agent.training.value)\nenv.close()\n</pre> # the env is now wrapped automatically when passing it to the constructor model = A2C(\"MlpPolicy\", env, verbose=1, policy_kwargs={'net_arch': [64, 64]}) model.learn(total_timesteps=env.config.agent.training.value) env.close() In\u00a0[\u00a0]: Copied! <pre># Create task\nenv = gym.make(task, **kwargs)\n# Apply the wrapper\nenv = TrialHistoryV2(env, probs=probs)\nenv = DummyVecEnv([lambda: env])\nfig = plotting.plot_env(\n    env,\n    num_steps=100,\n    # def_act=0,\n    ob_traces=['Fixation cue', 'NoGo', 'Go'],\n    # fig_kwargs={'figsize': (12, 12)},\n    model=model\n)\n</pre> # Create task env = gym.make(task, **kwargs) # Apply the wrapper env = TrialHistoryV2(env, probs=probs) env = DummyVecEnv([lambda: env]) fig = plotting.plot_env(     env,     num_steps=100,     # def_act=0,     ob_traces=['Fixation cue', 'NoGo', 'Go'],     # fig_kwargs={'figsize': (12, 12)},     model=model )"},{"location":"examples/demo/#exploring-neurogym-tasks","title":"Exploring NeuroGym Tasks\u00b6","text":""},{"location":"examples/demo/#installation","title":"Installation\u00b6","text":"<p>Google Colab: Uncomment and execute cell below when running this notebook on google colab.</p> <p>Local: Follow these instructions when running this notebook locally.</p>"},{"location":"examples/demo/#explore-tasks","title":"Explore tasks\u00b6","text":""},{"location":"examples/demo/#visualize-a-single-task","title":"Visualize a single task\u00b6","text":""},{"location":"examples/demo/#explore-wrappers","title":"Explore wrappers\u00b6","text":""},{"location":"examples/demo/#train-a-network","title":"Train a network\u00b6","text":"<p>Here, we train a simple neural network on the task at hand. We use a configuration file to load the parameters for the monitor. You can refer to the documentation for more information about how to use the configuration system.</p>"},{"location":"examples/demo/#visualize-the-results","title":"Visualize the results\u00b6","text":""},{"location":"examples/perceptualdecisionmaking/","title":"Perceptual Decision Making Task","text":"In\u00a0[\u00a0]: Copied! <pre># Uncomment to install\n# ! pip install neurogym[rl]\n</pre> # Uncomment to install # ! pip install neurogym[rl] In\u00a0[\u00a0]: Copied! <pre>import warnings\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport neurogym as ngym\nimport gymnasium as gym\nfrom neurogym.wrappers.monitor import Monitor\nfrom sb3_contrib import RecurrentPPO\nfrom stable_baselines3.common.vec_env import DummyVecEnv\nfrom pathlib import Path\nfrom neurogym.utils.ngym_random import TruncExp\nfrom neurogym.wrappers.reaction_time import ReactionTime\nfrom neurogym.wrappers.pass_action import PassAction\nfrom neurogym.wrappers.pass_reward import PassReward\nfrom neurogym.wrappers.side_bias import SideBias\nfrom neurogym.utils.psychometric import plot_psychometric\n\n# Suppress warnings\nwarnings.filterwarnings(\"ignore\")\n</pre> import warnings import matplotlib.pyplot as plt import numpy as np import os import neurogym as ngym import gymnasium as gym from neurogym.wrappers.monitor import Monitor from sb3_contrib import RecurrentPPO from stable_baselines3.common.vec_env import DummyVecEnv from pathlib import Path from neurogym.utils.ngym_random import TruncExp from neurogym.wrappers.reaction_time import ReactionTime from neurogym.wrappers.pass_action import PassAction from neurogym.wrappers.pass_reward import PassReward from neurogym.wrappers.side_bias import SideBias from neurogym.utils.psychometric import plot_psychometric  # Suppress warnings warnings.filterwarnings(\"ignore\") In\u00a0[\u00a0]: Copied! <pre># Environment parameters\n# These settings are low to speed up testing; we recommend setting EVAL_TRIALS to at least 1000\nEVAL_TRIALS = 100\ndt = 100\ndim_ring = 2  # Number of choices in the ring representation\nabort = False  # Whether to allow aborting the trial if the agent does not fixate\nrewards = {\n    \"abort\": -0.1,\n    \"correct\": +1.0,\n    \"fail\": 0.0\n}\ntiming = {\n    \"fixation\": TruncExp(600, 400, 700),\n    \"stimulus\": 2000,\n    \"delay\": 0,\n    \"decision\": 100,\n}\nsigma = 1.0 # Standard deviation of the Gaussian noise in the ring representation\n\nkwargs = {\n    \"dt\": dt,\n    \"dim_ring\": dim_ring,\n    \"rewards\": rewards,\n    \"timing\": timing,\n    \"sigma\": sigma,\n    \"abort\": abort,\n}\nblock_dur = (20, 100) # Extremes of the block duration in milliseconds\nprobs = [[0.2, 0.8], [0.8, 0.2]] # Probabilities of choosing left or right in the two blocks\n\n# Create and wrap the environment\ntask = \"PerceptualDecisionMaking-v0\"\nenv = gym.make(task, **kwargs)\nenv = ReactionTime(env, end_on_stimulus=True)\nenv = PassReward(env)\nenv = PassAction(env)\nenv = SideBias(env, probs=probs, block_dur=block_dur)\n\n# Print environment specifications\nprint(\"Trial timing (in milliseconds):\")\nprint(env.timing)\n\nprint(\"\\nObservation space structure:\")\nprint(env.observation_space)\n\nprint(\"\\nAction space structure:\")\nprint(env.action_space)\nprint(\"Action mapping:\")\nprint(env.action_space.name)\n</pre> # Environment parameters # These settings are low to speed up testing; we recommend setting EVAL_TRIALS to at least 1000 EVAL_TRIALS = 100 dt = 100 dim_ring = 2  # Number of choices in the ring representation abort = False  # Whether to allow aborting the trial if the agent does not fixate rewards = {     \"abort\": -0.1,     \"correct\": +1.0,     \"fail\": 0.0 } timing = {     \"fixation\": TruncExp(600, 400, 700),     \"stimulus\": 2000,     \"delay\": 0,     \"decision\": 100, } sigma = 1.0 # Standard deviation of the Gaussian noise in the ring representation  kwargs = {     \"dt\": dt,     \"dim_ring\": dim_ring,     \"rewards\": rewards,     \"timing\": timing,     \"sigma\": sigma,     \"abort\": abort, } block_dur = (20, 100) # Extremes of the block duration in milliseconds probs = [[0.2, 0.8], [0.8, 0.2]] # Probabilities of choosing left or right in the two blocks  # Create and wrap the environment task = \"PerceptualDecisionMaking-v0\" env = gym.make(task, **kwargs) env = ReactionTime(env, end_on_stimulus=True) env = PassReward(env) env = PassAction(env) env = SideBias(env, probs=probs, block_dur=block_dur)  # Print environment specifications print(\"Trial timing (in milliseconds):\") print(env.timing)  print(\"\\nObservation space structure:\") print(env.observation_space)  print(\"\\nAction space structure:\") print(env.action_space) print(\"Action mapping:\") print(env.action_space.name) In\u00a0[\u00a0]: Copied! <pre># Visualize example trials\nfig = ngym.utils.plot_env(\n    env,\n    name='Perceptual Decision Making',\n    ob_traces=[\n        'Fixation',\n        'Stim 1',\n        'Stim 2',\n        'PassReward', # Reward for the previous action\n        'PassAction' # Action taken in the previous step\n    ],\n    num_trials=5,\n)\n\n# Evaluate performance of the environment before training\neval_monitor = Monitor(\n    env\n)\nprint(\"\\nEvaluating random policy performance...\")\nmetrics = eval_monitor.evaluate_policy(num_trials=EVAL_TRIALS)\nprint(f\"\\nRandom policy metrics ({EVAL_TRIALS:,} trials):\")\nprint(f\"Mean performance: {metrics['mean_performance']:.4f}\")\nprint(f\"Mean reward: {metrics['mean_reward']:.4f}\")\n</pre> # Visualize example trials fig = ngym.utils.plot_env(     env,     name='Perceptual Decision Making',     ob_traces=[         'Fixation',         'Stim 1',         'Stim 2',         'PassReward', # Reward for the previous action         'PassAction' # Action taken in the previous step     ],     num_trials=5, )  # Evaluate performance of the environment before training eval_monitor = Monitor(     env ) print(\"\\nEvaluating random policy performance...\") metrics = eval_monitor.evaluate_policy(num_trials=EVAL_TRIALS) print(f\"\\nRandom policy metrics ({EVAL_TRIALS:,} trials):\") print(f\"Mean performance: {metrics['mean_performance']:.4f}\") print(f\"Mean reward: {metrics['mean_reward']:.4f}\") <p>As we can see, the agent's behavior is entirely random. Through training, we expect the agent to improve by learning to respect the fixation period, and map signal peaks to the correct choices in the ring representation. Let\u2019s move on to training the agent to see whether it can learn these key aspects of the task.</p> In\u00a0[\u00a0]: Copied! <pre># Set the number of trials to train on\n# These settings are low to speed up testing; we recommend setting TRAIN_TRIALS to at least 10000 and `interval` in Monitor to 1000\navg_timesteps = 7 # Observed\nTRAIN_TRIALS = 100  # Choose the desired number of trials\ntotal_timesteps = TRAIN_TRIALS * avg_timesteps\nprint(f\"Training for {TRAIN_TRIALS:,} trials \u2248 {total_timesteps:,} timesteps\")\n\n# Configure monitoring with trial-appropriate parameters\ntrials_per_figure = 10  # Show 10 trials in each figure\nsteps_per_figure = int(trials_per_figure * avg_timesteps)\n\ntrain_monitor = Monitor(\n    env,\n    trigger=\"trial\",              # Save based on completed trials\n    interval=100,                # Save data every 100 trials\n    plot_create=True,             # Save visualization figures\n    plot_steps=steps_per_figure,  # Number of steps to visualize on the figure\n    verbose=True,                 # Print stats when data is saved\n)\n\n# DummyVecEnv is Stable-Baselines3 wrapper that converts the environment\n# into a vectorized form (required by PPO), allowing for parallel training of multiple environments\nenv_vec = DummyVecEnv([lambda: train_monitor])\n\n# Create and train Recurrent PPO agent\n# Set n_steps to be a multiple of your average trial length\ntrials_per_batch = 64\nn_steps = int(avg_timesteps * trials_per_batch)  # Collect approximately 64 trials per update\nbatch_size = 32  # Small batch size for short episodes\npolicy_kwargs = {\n    \"lstm_hidden_size\": 128,      # Small LSTM for short sequences\n    \"n_lstm_layers\": 2,           # Single layer is sufficient\n    \"shared_lstm\": True,          # Share LSTM to reduce parameters\n    \"enable_critic_lstm\": False,  # Disable separate LSTM for critic when sharing\n}\nrl_model = RecurrentPPO(\n    \"MlpLstmPolicy\",\n    env_vec,\n    learning_rate=3e-4,       # Learning rate for the optimizer\n    n_steps=n_steps,          # Align with multiple complete episodes\n    batch_size=32,            # Smaller batch size\n    ent_coef=0.0,             # Entropy coefficient for exploration\n    policy_kwargs=policy_kwargs,\n    verbose=1\n)\n\nrl_model.learn(total_timesteps=total_timesteps, log_interval=int(total_timesteps/10))\nenv_vec.close()\n</pre> # Set the number of trials to train on # These settings are low to speed up testing; we recommend setting TRAIN_TRIALS to at least 10000 and `interval` in Monitor to 1000 avg_timesteps = 7 # Observed TRAIN_TRIALS = 100  # Choose the desired number of trials total_timesteps = TRAIN_TRIALS * avg_timesteps print(f\"Training for {TRAIN_TRIALS:,} trials \u2248 {total_timesteps:,} timesteps\")  # Configure monitoring with trial-appropriate parameters trials_per_figure = 10  # Show 10 trials in each figure steps_per_figure = int(trials_per_figure * avg_timesteps)  train_monitor = Monitor(     env,     trigger=\"trial\",              # Save based on completed trials     interval=100,                # Save data every 100 trials     plot_create=True,             # Save visualization figures     plot_steps=steps_per_figure,  # Number of steps to visualize on the figure     verbose=True,                 # Print stats when data is saved )  # DummyVecEnv is Stable-Baselines3 wrapper that converts the environment # into a vectorized form (required by PPO), allowing for parallel training of multiple environments env_vec = DummyVecEnv([lambda: train_monitor])  # Create and train Recurrent PPO agent # Set n_steps to be a multiple of your average trial length trials_per_batch = 64 n_steps = int(avg_timesteps * trials_per_batch)  # Collect approximately 64 trials per update batch_size = 32  # Small batch size for short episodes policy_kwargs = {     \"lstm_hidden_size\": 128,      # Small LSTM for short sequences     \"n_lstm_layers\": 2,           # Single layer is sufficient     \"shared_lstm\": True,          # Share LSTM to reduce parameters     \"enable_critic_lstm\": False,  # Disable separate LSTM for critic when sharing } rl_model = RecurrentPPO(     \"MlpLstmPolicy\",     env_vec,     learning_rate=3e-4,       # Learning rate for the optimizer     n_steps=n_steps,          # Align with multiple complete episodes     batch_size=32,            # Smaller batch size     ent_coef=0.0,             # Entropy coefficient for exploration     policy_kwargs=policy_kwargs,     verbose=1 )  rl_model.learn(total_timesteps=total_timesteps, log_interval=int(total_timesteps/10)) env_vec.close() In\u00a0[\u00a0]: Copied! <pre># Plot example trials with trained agent\nfig = ngym.utils.plot_env(\n    env_vec,\n    name='Perceptual Decision Making (trained)',\n    ob_traces=[\n        'Fixation',\n        'Stim 1',\n        'Stim 2',\n        'PassReward',\n        'PassAction'\n    ],\n    num_trials=5,\n    model=rl_model,\n)\n</pre> # Plot example trials with trained agent fig = ngym.utils.plot_env(     env_vec,     name='Perceptual Decision Making (trained)',     ob_traces=[         'Fixation',         'Stim 1',         'Stim 2',         'PassReward',         'PassAction'     ],     num_trials=5,     model=rl_model, ) <p>After training, we visualize the agent's behavior on a few example trials. In contrast to the random agent, we should now see:</p> <ul> <li>Consistent fixation maintenance during the fixation period</li> <li>Choices that correlate with the evidence strength of the stimulus</li> <li>Performance significantly above chance level (0.5), reflecting successful context-dependent decisions</li> </ul> <p>The plot shows the trained agent's behavior across 5 example trials, allowing us to visualize how well it has learned to make appropriate choices using the ring representation.</p> In\u00a0[\u00a0]: Copied! <pre># Evaluate performance of the last trained model\nprint(\"\\nEvaluating trained model performance...\")\nrl_trained_metrics = eval_monitor.evaluate_policy(num_trials=EVAL_TRIALS, model=rl_model)\nprint(f\"\\nTrained model metrics ({EVAL_TRIALS:,} trials):\")\nprint(f\"Mean performance: {rl_trained_metrics['mean_performance']:.4f}\")\nprint(f\"Mean reward: {rl_trained_metrics['mean_reward']:.4f}\")\n\nfig = train_monitor.plot_training_history(figsize=(6, 4), plot_performance=False)\n</pre> # Evaluate performance of the last trained model print(\"\\nEvaluating trained model performance...\") rl_trained_metrics = eval_monitor.evaluate_policy(num_trials=EVAL_TRIALS, model=rl_model) print(f\"\\nTrained model metrics ({EVAL_TRIALS:,} trials):\") print(f\"Mean performance: {rl_trained_metrics['mean_performance']:.4f}\") print(f\"Mean reward: {rl_trained_metrics['mean_reward']:.4f}\")  fig = train_monitor.plot_training_history(figsize=(6, 4), plot_performance=False) In\u00a0[\u00a0]: Copied! <pre>if not os.getenv(\"GITHUB_ACTIONS\"):\n    # Evaluate policy and extract data\n    eval_monitor.evaluate_policy(num_trials=EVAL_TRIALS, model=rl_model)\n    data = eval_monitor.data_eval\n\n    # Extract trial-level fields\n    trials = data['trial']\n    coh = np.array([t['coh'] for t in trials])\n    block = np.array([np.array_equal(t['probs'], np.array(probs[1])) for t in trials]).astype(int) # block 1 is 0, block 2 is 1\n\n    # Filter out trials where action is 0 (no action taken)\n    actions_only_mask = data['action'] != 0\n    coh = coh[actions_only_mask]\n    block = block[actions_only_mask]\n    data_action = data['action'][actions_only_mask]\n    data_gt = data['gt'][actions_only_mask]\n\n    # Convert actions and ground truth to binary (0 = left, 1 = right)\n    ch = (data_action == 2).astype(int)\n    gt = (data_gt == 2).astype(int)\n\n    # Plotting setup\n    fig, ax = plt.subplots(figsize=(3, 3))\n\n    # Plot psychometric curves for each block\n    for i, blk in enumerate(range(len(probs))):\n        # Filter trials matching current block\n        mask = block == blk\n        ev = coh[mask]\n        ch_m = ch[mask]\n        ref = gt[mask]\n\n        # Signed evidence: negative if correct answer is left\n        sig_ev = np.where(ref == 0, -ev, ev)\n\n        plot_psychometric(sig_ev, ch_m, ax, legend=f'Block {blk+1}')\n</pre> if not os.getenv(\"GITHUB_ACTIONS\"):     # Evaluate policy and extract data     eval_monitor.evaluate_policy(num_trials=EVAL_TRIALS, model=rl_model)     data = eval_monitor.data_eval      # Extract trial-level fields     trials = data['trial']     coh = np.array([t['coh'] for t in trials])     block = np.array([np.array_equal(t['probs'], np.array(probs[1])) for t in trials]).astype(int) # block 1 is 0, block 2 is 1      # Filter out trials where action is 0 (no action taken)     actions_only_mask = data['action'] != 0     coh = coh[actions_only_mask]     block = block[actions_only_mask]     data_action = data['action'][actions_only_mask]     data_gt = data['gt'][actions_only_mask]      # Convert actions and ground truth to binary (0 = left, 1 = right)     ch = (data_action == 2).astype(int)     gt = (data_gt == 2).astype(int)      # Plotting setup     fig, ax = plt.subplots(figsize=(3, 3))      # Plot psychometric curves for each block     for i, blk in enumerate(range(len(probs))):         # Filter trials matching current block         mask = block == blk         ev = coh[mask]         ch_m = ch[mask]         ref = gt[mask]          # Signed evidence: negative if correct answer is left         sig_ev = np.where(ref == 0, -ev, ev)          plot_psychometric(sig_ev, ch_m, ax, legend=f'Block {blk+1}')"},{"location":"examples/perceptualdecisionmaking/#perceptual-decision-making-task","title":"Perceptual Decision Making Task\u00b6","text":"<p>This environment implements a two-alternative forced choice perceptual decision-making task, where the agent must integrate noisy sensory evidence over time to make accurate decisions. The task is based on classic motion discrimination experiments (Britten et al. 1992) and has been adapted for studying neural mechanisms of decision-making in computational models. The key features of the task are:</p> <ol> <li><p>On each trial, a noisy stimulus appears on either the left or right side of the visual field with varying coherence levels (evidence strength).</p> </li> <li><p>Choices are represented as angles evenly spaced around a circle. With the default of 2 choices (<code>dim_ring=2</code>), this corresponds to:</p> <ul> <li>Position 1: 0\u00b0 (left choice)</li> <li>Position 2: 180\u00b0 (right choice)</li> </ul> </li> <li><p>The stimulus is presented as a cosine modulation with additive Gaussian noise, requiring the agent to integrate evidence over time to overcome noise and make accurate decisions.</p> </li> <li><p>The agent can respond at any time after stimulus onset.</p> </li> <li><p>The environment includes blocks where one side is more likely than the other, and augments observations with previous actions and rewards.</p> </li> </ol> <p>In this notebook, we will:</p> <ol> <li>Train an agent on the task using reinforcement learning with Stable-Baselines3.</li> <li>Analyze the agent's psychometric curves and compare performance across different coherence levels and block contexts.</li> </ol>"},{"location":"examples/perceptualdecisionmaking/#0-install-dependencies","title":"0. Install Dependencies\u00b6","text":"<p>To begin, install the <code>neurogym</code> package. This will automatically install all required dependencies, including Stable-Baselines3.</p> <p>For detailed instructions on how to install <code>neurogym</code> within a conda environment or in editable mode, refer to the installation instructions.</p>"},{"location":"examples/perceptualdecisionmaking/#1-training-an-agent-on-the-perceptual-decision-making-task","title":"1. Training an Agent on the Perceptual Decision Making Task\u00b6","text":""},{"location":"examples/perceptualdecisionmaking/#11-environment-setup-and-initial-agent-behavior","title":"1.1 Environment Setup and Initial Agent Behavior\u00b6","text":"<p>Let's now create and explore the environment using the <code>PerceptualDecisionMaking</code> class from neurogym. We'll use the default configuration which includes:</p> <ul> <li><code>dim_ring = 2</code>: Two possible choices (left/right) represented at 0\u00b0 and 180\u00b0. Note that the ring architecture can support any number of choices, making it suitable for more complex decision-making scenarios.</li> <li><code>timing = {'fixation': ~600, 'stimulus': 2000, 'delay': 0, 'decision': 100}</code> (in milliseconds).</li> <li><code>rewards = {'abort': -0.1, 'correct': +1.0, 'fail': 0.0}</code>; abort is a penalty applied when the agent fails to fixate. The task allows the trial to be aborted if fixation does not occur, which is where the name of this penalty comes from.</li> <li><code>sigma = 1.0</code>: Standard deviation of the noise added to the inputs.</li> </ul> <p>In this notebook, several wrappers are used to modify the environment's behavior:</p> <ul> <li><code>ReactionTime</code> wrapper allows the agent to respond at any time after stimulus onset.</li> <li><code>SideBias</code> wrapper introduces blocks where one side is more likely than the other. It uses two key parameters:<ul> <li><code>probs = [[0.2, 0.8], [0.8, 0.2]]</code>: Probability matrices defining the likelihood of each choice (only two choices in this example) in different blocks</li> <li><code>block_dur = (20, 100)</code>: Block duration randomly sampled between 20-100 trials, determining how long each bias condition persists</li> </ul> </li> <li><code>PassAction</code> and <code>PassReward</code> wrappers augment the observations with the previous step's action and reward, respectively, enabling the agent to use recent history in decision-making.</li> </ul>"},{"location":"examples/perceptualdecisionmaking/#111-import-libraries","title":"1.1.1 Import Libraries\u00b6","text":""},{"location":"examples/perceptualdecisionmaking/#112-environment-setup","title":"1.1.2 Environment Setup\u00b6","text":""},{"location":"examples/perceptualdecisionmaking/#113-random-agent-behavior","title":"1.1.3 Random Agent Behavior\u00b6","text":"<p>Let's now plot the behavior of a random agent on the task. The agent will randomly choose between the two options (left/right), and we will visualize its behavior over 5 trials. We will also plot the reward received by the agent at each time step, as well as the performance on each trial. Note that performance is only defined at the end of a trial: it is 1 if the agent made the correct choice, and 0 otherwise.</p> <p>To keep track of the agent's behavior, we will use the <code>Monitor</code> wrapper, which monitors training by:</p> <ul> <li>Tracking and saving behavioral data (rewards, actions, observations) every <code>sv_per</code> steps.</li> <li>Generating visualization figures during training if<code> sv_fig=True</code>.</li> <li>Providing progress information if <code>verbose=True</code>.</li> </ul> <p>Here, we\u2019ll use the wrapper solely to compute the agent\u2019s performance, but later it will help us assess learning and save intermediate results.</p>"},{"location":"examples/perceptualdecisionmaking/#12-training-and-evaluating-the-agent","title":"1.2 Training and Evaluating the Agent\u00b6","text":"<p>We will now train the agent using Stable-Baselines3\u2019s implementation of PPO (Proximal Policy Optimization), a widely used reinforcement learning algorithm known for its stability and efficiency.</p> <p>To support recurrent policies, we will use RecurrentPPO, which extends PPO with recurrent neural networks, specifically LSTMs.</p>"},{"location":"examples/perceptualdecisionmaking/#121-training-the-agent","title":"1.2.1 Training the Agent\u00b6","text":""},{"location":"examples/perceptualdecisionmaking/#122-plot-the-behavior-of-the-trained-agent","title":"1.2.2 Plot the Behavior of the Trained Agent\u00b6","text":""},{"location":"examples/perceptualdecisionmaking/#123-evaluate-the-agents-performance","title":"1.2.3 Evaluate the Agent's Performance\u00b6","text":""},{"location":"examples/perceptualdecisionmaking/#124-plot-the-agents-psychometric-curves","title":"1.2.4 Plot the Agent's Psychometric Curves\u00b6","text":""},{"location":"examples/reinforcement_learning/","title":"NeuroGym with RL","text":"In\u00a0[\u00a0]: Copied! <pre># ! pip install neurogym[rl]\n</pre> # ! pip install neurogym[rl] In\u00a0[\u00a0]: Copied! <pre>import gymnasium as gym\nimport neurogym as ngym\nfrom neurogym.wrappers import pass_reward\nimport warnings\nfrom IPython.display import clear_output\nclear_output()\nwarnings.filterwarnings('ignore')\n</pre> import gymnasium as gym import neurogym as ngym from neurogym.wrappers import pass_reward import warnings from IPython.display import clear_output clear_output() warnings.filterwarnings('ignore') <p>here we build the Random Dots Motion task, specifying the duration of each trial period (fixation, stimulus, decision) and wrapp it with the pass-reward wrapper which appends the previous reward to the observation. We then plot the structure of the task in a figure that shows:</p> <ol> <li>The observations received by the agent (top panel).</li> <li>The actions taken by a random agent and the correct action at each timestep (second panel).</li> <li>The rewards provided by the environment at each timestep (third panel).</li> <li>The performance of the agent at each trial (bottom panel).</li> </ol> In\u00a0[\u00a0]: Copied! <pre># Task name\nname = 'PerceptualDecisionMaking-v0'\n# task specification (here we only specify the duration of the different trial periods)\ntiming = {\n    'fixation': ('constant', 300),\n    'stimulus': ('constant', 500),\n    'decision': ('constant', 300),\n}\nkwargs = {'dt': 100, 'timing': timing}\n# build task\nenv = gym.make(name, **kwargs)\n# print task properties\nprint(env)\n# wrapp task with pass-reward wrapper\nenv = pass_reward.PassReward(env)\n# plot example trials with random agent\n_ = ngym.utils.plot_env(\n    env,\n    fig_kwargs={'figsize': (12, 12)},\n    num_steps=100,\n    ob_traces=['Fixation cue', 'Stim 1', 'Stim 2', 'Previous reward'],\n)\n</pre> # Task name name = 'PerceptualDecisionMaking-v0' # task specification (here we only specify the duration of the different trial periods) timing = {     'fixation': ('constant', 300),     'stimulus': ('constant', 500),     'decision': ('constant', 300), } kwargs = {'dt': 100, 'timing': timing} # build task env = gym.make(name, **kwargs) # print task properties print(env) # wrapp task with pass-reward wrapper env = pass_reward.PassReward(env) # plot example trials with random agent _ = ngym.utils.plot_env(     env,     fig_kwargs={'figsize': (12, 12)},     num_steps=100,     ob_traces=['Fixation cue', 'Stim 1', 'Stim 2', 'Previous reward'], ) In\u00a0[\u00a0]: Copied! <pre># these values are set low for testing purposes. To get a better sense of the package, we recommend setting\n# `total_timesteps = 100_000`\ntotal_timesteps = 500\nlog_interval = 500\n\nfrom sb3_contrib import RecurrentPPO\nfrom stable_baselines3.common.vec_env import DummyVecEnv\n\nenv = DummyVecEnv([lambda: env])\n\nmodel = RecurrentPPO(\n    policy=\"MlpLstmPolicy\",\n    env=env,\n    verbose=1,\n)\nmodel.learn(total_timesteps=total_timesteps, log_interval=log_interval)\nenv.close()\n</pre> # these values are set low for testing purposes. To get a better sense of the package, we recommend setting # `total_timesteps = 100_000` total_timesteps = 500 log_interval = 500  from sb3_contrib import RecurrentPPO from stable_baselines3.common.vec_env import DummyVecEnv  env = DummyVecEnv([lambda: env])  model = RecurrentPPO(     policy=\"MlpLstmPolicy\",     env=env,     verbose=1, ) model.learn(total_timesteps=total_timesteps, log_interval=log_interval) env.close() In\u00a0[\u00a0]: Copied! <pre>env = gym.make(name, **kwargs)\n# print task properties\nprint(env)\n# wrapp task with pass-reward wrapper\nenv = pass_reward.PassReward(env)\nenv = DummyVecEnv([lambda: env])\n\n# plot example trials with random agent\n_ = ngym.utils.plot_env(\n    env,\n    fig_kwargs={'figsize': (12, 12)},\n    num_steps=100,\n    ob_traces=['Fixation cue', 'Stim 1', 'Stim 2', 'Previous reward'],\n    model=model,\n)\n</pre> env = gym.make(name, **kwargs) # print task properties print(env) # wrapp task with pass-reward wrapper env = pass_reward.PassReward(env) env = DummyVecEnv([lambda: env])  # plot example trials with random agent _ = ngym.utils.plot_env(     env,     fig_kwargs={'figsize': (12, 12)},     num_steps=100,     ob_traces=['Fixation cue', 'Stim 1', 'Stim 2', 'Previous reward'],     model=model, )"},{"location":"examples/reinforcement_learning/#neurogym-with-reinforcement-learning-stable-baselines3","title":"Neurogym with Reinforcement Learning (stable-baselines3)\u00b6","text":"<p>NeuroGym is a toolkit that allows training any network model on many established neuroscience tasks techniques such as standard Supervised Learning or Reinforcement Learning (RL). In this notebook we will use RL to train an LSTM network on the classical Random Dots Motion (RDM) task (Britten et al. 1992).</p> <p>We first show how to install the relevant toolboxes. We then show how build the task of interest (in the example the RDM task), wrapp it with the pass-reward wrapper in one line and visualize the structure of the final task. Finally we train an LSTM network on the task using the A2C algorithm Mnih et al. 2016 implemented in the stable-baselines3 toolbox, and plot the results.</p> <p>It is straightforward to change the code to train a network on any other available task or using a different RL algorithm (e.g. ACER, PPO2).</p>"},{"location":"examples/reinforcement_learning/#installation","title":"Installation\u00b6","text":"<p>Google Colab: Uncomment and execute cell below when running this notebook on google colab.</p> <p>Local: Follow these instructions when running this notebook locally.</p>"},{"location":"examples/reinforcement_learning/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/reinforcement_learning/#task","title":"Task\u00b6","text":""},{"location":"examples/reinforcement_learning/#train-a-network","title":"Train a network\u00b6","text":""},{"location":"examples/reinforcement_learning/#visualize-results","title":"Visualize results\u00b6","text":""},{"location":"examples/supervised_learning_keras/","title":"NeuroGym with Keras","text":"In\u00a0[\u00a0]: Copied! <pre># ! pip install neurogym\n</pre> # ! pip install neurogym  In\u00a0[\u00a0]: Copied! <pre>import warnings\nfrom IPython.display import clear_output\n\nimport numpy as np\nimport neurogym as ngym\nfrom neurogym.utils import plotting\n\n# note that some system will show a warning that the lines below cannot be resolved/have missing imports.\n# if you've installed the current package as instructed, these imports will work fine nonetheless.\nfrom keras.models import Model\nfrom keras.layers import Dense, LSTM, TimeDistributed, Input\n\nclear_output()\nwarnings.filterwarnings('ignore')\n</pre> import warnings from IPython.display import clear_output  import numpy as np import neurogym as ngym from neurogym.utils import plotting  # note that some system will show a warning that the lines below cannot be resolved/have missing imports. # if you've installed the current package as instructed, these imports will work fine nonetheless. from keras.models import Model from keras.layers import Dense, LSTM, TimeDistributed, Input  clear_output() warnings.filterwarnings('ignore')  In\u00a0[\u00a0]: Copied! <pre># This settings is low to speed up testing; we recommend setting it to at least 2000\nsteps_per_epoch = 100\n\n\n# Environment\ntask = 'PerceptualDecisionMaking-v0'\nkwargs = {'dt': 100}\nseq_len = 100\n\n# Make supervised dataset\ndataset = ngym.Dataset(task, env_kwargs=kwargs, batch_size=16,seq_len=seq_len)\nenv = dataset.env\nobs_size = env.observation_space.shape[0]\nact_size = env.action_space.n\n\n# Model\nnum_h = 64\n# from https://www.tensorflow.org/guide/keras/rnn\nxin = Input(batch_shape=(None, None, obs_size), dtype='float32')\nseq = LSTM(num_h, return_sequences=True)(xin)\nmlp = TimeDistributed(Dense(act_size, activation='softmax'))(seq)\nmodel = Model(inputs=xin, outputs=mlp)\nmodel.summary()\nmodel.compile(optimizer='Adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Train network\ndata_generator = (dataset() for _ in range(steps_per_epoch))\nhistory = model.fit(data_generator, steps_per_epoch=steps_per_epoch)\n</pre> # This settings is low to speed up testing; we recommend setting it to at least 2000 steps_per_epoch = 100   # Environment task = 'PerceptualDecisionMaking-v0' kwargs = {'dt': 100} seq_len = 100  # Make supervised dataset dataset = ngym.Dataset(task, env_kwargs=kwargs, batch_size=16,seq_len=seq_len) env = dataset.env obs_size = env.observation_space.shape[0] act_size = env.action_space.n  # Model num_h = 64 # from https://www.tensorflow.org/guide/keras/rnn xin = Input(batch_shape=(None, None, obs_size), dtype='float32') seq = LSTM(num_h, return_sequences=True)(xin) mlp = TimeDistributed(Dense(act_size, activation='softmax'))(seq) model = Model(inputs=xin, outputs=mlp) model.summary() model.compile(optimizer='Adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])  # Train network data_generator = (dataset() for _ in range(steps_per_epoch)) history = model.fit(data_generator, steps_per_epoch=steps_per_epoch) In\u00a0[\u00a0]: Copied! <pre># n_trials is set to a low number to speed up testing; we recommend setting it to at least 200\nn_trials = 50\n\nperf = 0\nfor i in range(n_trials):\n    env.new_trial()\n    obs, gt = env.ob, env.gt\n    obs = obs[:, np.newaxis, :]\n\n    action_pred = model.predict(obs, verbose=0)\n    action_pred = np.argmax(action_pred, axis=-1)\n    perf += gt[-1] == action_pred[-1, 0]\n\n    if (i+1) % 10 == 0:\n        print(f\"Completed trial {i+1}/{n_trials}\")\n\nperf /= n_trials\nprint(f\"Performance: {perf} after {i+1} trials\")\n</pre> # n_trials is set to a low number to speed up testing; we recommend setting it to at least 200 n_trials = 50  perf = 0 for i in range(n_trials):     env.new_trial()     obs, gt = env.ob, env.gt     obs = obs[:, np.newaxis, :]      action_pred = model.predict(obs, verbose=0)     action_pred = np.argmax(action_pred, axis=-1)     perf += gt[-1] == action_pred[-1, 0]      if (i+1) % 10 == 0:         print(f\"Completed trial {i+1}/{n_trials}\")  perf /= n_trials print(f\"Performance: {perf} after {i+1} trials\") In\u00a0[\u00a0]: Copied! <pre>obs = np.squeeze(obs, axis=1)  # remove the sequence dimension for plotting\naction_pred = np.squeeze(action_pred, axis=1)  # remove the sequence dimension for plotting\n\n_ = ngym.utils.plotting.fig_(obs, action_pred, gt)\n</pre> obs = np.squeeze(obs, axis=1)  # remove the sequence dimension for plotting action_pred = np.squeeze(action_pred, axis=1)  # remove the sequence dimension for plotting  _ = ngym.utils.plotting.fig_(obs, action_pred, gt)"},{"location":"examples/supervised_learning_keras/#neurogym-with-supervised-learning-keras","title":"Neurogym with Supervised Learning (keras)\u00b6","text":"<p>NeuroGym is a comprehensive toolkit that allows training any network model on many established neuroscience tasks using Reinforcement Learning techniques. It includes working memory tasks, value-based decision tasks and context-dependent perceptual categorization tasks.</p> <p>In this notebook we first show how to install the relevant toolbox.</p> <p>We then show how to access the available tasks and their relevant information.</p> <p>Finally we train an LSTM network on the Random Dots Motion task using standard supervised learning techniques (with Keras), and plot the results.</p>"},{"location":"examples/supervised_learning_keras/#installation","title":"Installation\u00b6","text":"<p>Google Colab: Uncomment and execute cell below when running this notebook on google colab.</p> <p>Local: Follow these instructions and then run <code>pip install tensorflow</code> when running this notebook locally.</p> <p>NOTE: tensorflow is pre-installed in Google Colab, but not typically part of the neurogym library.</p>"},{"location":"examples/supervised_learning_keras/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/supervised_learning_keras/#task-network-and-training","title":"Task, network, and training\u00b6","text":""},{"location":"examples/supervised_learning_keras/#analysis","title":"Analysis\u00b6","text":""},{"location":"examples/supervised_learning_pytorch/","title":"NeuroGym with PyTorch","text":"In\u00a0[\u00a0]: Copied! <pre># ! pip install neurogym\n</pre> # ! pip install neurogym In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport torch\nimport torch.nn as nn\n\nimport neurogym as ngym\n\nimport warnings\nfrom IPython.display import clear_output\nclear_output()\nwarnings.filterwarnings('ignore')\n</pre> import numpy as np import torch import torch.nn as nn  import neurogym as ngym  import warnings from IPython.display import clear_output clear_output() warnings.filterwarnings('ignore')  In\u00a0[\u00a0]: Copied! <pre># Environment\ntask = 'PerceptualDecisionMaking-v0'\nkwargs = {'dt': 100}\nseq_len = 100\n\n# Make supervised dataset\ndataset = ngym.Dataset(task, env_kwargs=kwargs, batch_size=16,\n                       seq_len=seq_len)\nenv = dataset.env\nob_size = env.observation_space.shape[0]\nact_size = env.action_space.n\n</pre> # Environment task = 'PerceptualDecisionMaking-v0' kwargs = {'dt': 100} seq_len = 100  # Make supervised dataset dataset = ngym.Dataset(task, env_kwargs=kwargs, batch_size=16,                        seq_len=seq_len) env = dataset.env ob_size = env.observation_space.shape[0] act_size = env.action_space.n In\u00a0[\u00a0]: Copied! <pre># This settings is low to speed up testing; we recommend setting it to at least 2000\nEPOCHS = 200\n\nclass Net(nn.Module):\n    def __init__(self, num_h):\n        super(Net, self).__init__()\n        self.lstm = nn.LSTM(ob_size, num_h)\n        self.linear = nn.Linear(num_h, act_size)\n\n    def forward(self, x):\n        out, hidden = self.lstm(x)\n        x = self.linear(out)\n        return x\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nnet = Net(num_h=64).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(net.parameters(), lr=1e-2)\n\nrunning_loss = 0.0\nfor i in range(EPOCHS):\n    inputs, labels = dataset()\n    inputs = torch.from_numpy(inputs).type(torch.float).to(device)\n    labels = torch.from_numpy(labels.flatten()).type(torch.long).to(device)\n\n    # zero the parameter gradients\n    optimizer.zero_grad()\n\n    # forward + backward + optimize\n    outputs = net(inputs)\n\n    loss = criterion(outputs.view(-1, act_size), labels)\n    loss.backward()\n    optimizer.step()\n\n    # print statistics\n    running_loss += loss.item()\n    if (i+1) % 100 == 0:\n        print('{:d} loss: {:0.5f}'.format(i+1, running_loss / 100))\n        running_loss = 0.0\n\nprint('Finished Training')\n</pre> # This settings is low to speed up testing; we recommend setting it to at least 2000 EPOCHS = 200  class Net(nn.Module):     def __init__(self, num_h):         super(Net, self).__init__()         self.lstm = nn.LSTM(ob_size, num_h)         self.linear = nn.Linear(num_h, act_size)      def forward(self, x):         out, hidden = self.lstm(x)         x = self.linear(out)         return x  device = 'cuda' if torch.cuda.is_available() else 'cpu' net = Net(num_h=64).to(device) criterion = nn.CrossEntropyLoss() optimizer = torch.optim.Adam(net.parameters(), lr=1e-2)  running_loss = 0.0 for i in range(EPOCHS):     inputs, labels = dataset()     inputs = torch.from_numpy(inputs).type(torch.float).to(device)     labels = torch.from_numpy(labels.flatten()).type(torch.long).to(device)      # zero the parameter gradients     optimizer.zero_grad()      # forward + backward + optimize     outputs = net(inputs)      loss = criterion(outputs.view(-1, act_size), labels)     loss.backward()     optimizer.step()      # print statistics     running_loss += loss.item()     if (i+1) % 100 == 0:         print('{:d} loss: {:0.5f}'.format(i+1, running_loss / 100))         running_loss = 0.0  print('Finished Training') In\u00a0[\u00a0]: Copied! <pre>perf = 0\nnum_trial = 200\nfor i in range(num_trial):\n    env.new_trial()\n    ob, gt = env.ob, env.gt\n    ob = ob[:, np.newaxis, :]  # Add batch axis\n    inputs = torch.from_numpy(ob).type(torch.float).to(device)\n\n    action_pred = net(inputs)\n    action_pred = action_pred.detach().numpy()\n    action_pred = np.argmax(action_pred, axis=-1)\n    perf += gt[-1] == action_pred[-1, 0]\n\nperf /= num_trial\nprint('Average performance in {:d} trials'.format(num_trial))\nprint(perf)\n</pre> perf = 0 num_trial = 200 for i in range(num_trial):     env.new_trial()     ob, gt = env.ob, env.gt     ob = ob[:, np.newaxis, :]  # Add batch axis     inputs = torch.from_numpy(ob).type(torch.float).to(device)      action_pred = net(inputs)     action_pred = action_pred.detach().numpy()     action_pred = np.argmax(action_pred, axis=-1)     perf += gt[-1] == action_pred[-1, 0]  perf /= num_trial print('Average performance in {:d} trials'.format(num_trial)) print(perf)"},{"location":"examples/supervised_learning_pytorch/#neurogym-with-supervised-learning-pytorch","title":"Neurogym with Supervised Learning (pytorch)\u00b6","text":"<p>Pytorch-based example code for training a RNN on a perceptual decision-making task.</p>"},{"location":"examples/supervised_learning_pytorch/#installation","title":"Installation\u00b6","text":"<p>Google Colab: Uncomment and execute cell below when running this notebook on google colab.</p> <p>Local: Follow these instructions when running this notebook locally.</p>"},{"location":"examples/supervised_learning_pytorch/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/supervised_learning_pytorch/#dataset","title":"Dataset\u00b6","text":""},{"location":"examples/supervised_learning_pytorch/#network-and-training","title":"Network and Training\u00b6","text":""},{"location":"examples/supervised_learning_pytorch/#analysis","title":"Analysis\u00b6","text":""},{"location":"examples/template/","title":"Example template for contributing new tasks","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"Example template for contributing new tasks.\"\"\"  # noqa: INP001\n</pre> \"\"\"Example template for contributing new tasks.\"\"\"  # noqa: INP001 In\u00a0[\u00a0]: Copied! <pre>import numpy as np\n</pre> import numpy as np In\u00a0[\u00a0]: Copied! <pre>import neurogym as ngym\nfrom neurogym import spaces\nfrom neurogym.utils.logging import logger\n</pre> import neurogym as ngym from neurogym import spaces from neurogym.utils.logging import logger In\u00a0[\u00a0]: Copied! <pre>class YourTask(ngym.TrialEnv):\n    def __init__(self, dt=100, rewards=None, timing=None, sigma=1) -&gt; None:\n        super().__init__(dt=dt)\n        # Possible decisions at the end of the trial\n        self.choices = [1, 2]  # e.g. [left, right]\n        self.sigma = sigma / np.sqrt(self.dt)  # Input noise\n\n        # Optional rewards dictionary\n        self.rewards = {\"abort\": -0.1, \"correct\": +1.0, \"fail\": 0.0}\n        if rewards:\n            self.rewards.update(rewards)\n\n        # Optional timing dictionary\n        # if provided, self.add_period can infer timing directly\n        self.timing = {\"fixation\": 100, \"stimulus\": 2000, \"delay\": 0, \"decision\": 100}\n        if timing:\n            self.timing.update(timing)\n\n        # Similar to gymnasium envs, define observations_space and action_space\n        # Optional annotation of the observation space\n        name = {\"fixation\": 0, \"stimulus\": [1, 2]}\n        self.observation_space = spaces.Box(\n            -np.inf,\n            np.inf,\n            shape=(3,),\n            dtype=np.float32,\n            name=name,\n        )\n        # Optional annotation of the action space\n        name = {\"fixation\": 0, \"choice\": [1, 2]}\n        self.action_space = spaces.Discrete(3, name=name)\n\n    def _new_trial(self, **kwargs):\n        \"\"\"Called internally to generate a next trial.\n\n        Typically, you need to\n            set trial: a dictionary of trial information\n            run self.add_period():\n                will add time periods to the trial\n                accesible through dict self.start_t and self.end_t\n            run self.add_ob():\n                will add observation to np array self.ob\n            run self.set_groundtruth():\n                will set groundtruth to np array self.gt\n\n        Returns:\n            trial: dictionary of trial information\n        \"\"\"\n        # Setting trial information\n        trial = {\"ground_truth\": self.rng.choice(self.choices)}\n        trial.update(kwargs)  # allows wrappers to modify the trial\n        ground_truth = trial[\"ground_truth\"]\n\n        # Adding periods sequentially\n        self.add_period([\"fixation\", \"stimulus\", \"delay\", \"decision\"])\n\n        # Setting observations, default all 0\n        # Setting fixation cue to 1 before decision period\n        self.add_ob(1, where=\"fixation\")\n        self.set_ob(0, \"decision\", where=\"fixation\")\n        # Set the stimulus\n        stim = [0, 0, 0]\n        stim[ground_truth] = 1\n        self.add_ob(stim, \"stimulus\")\n        # adding gaussian noise to stimulus with std = self.sigma\n        self.add_randn(0, self.sigma, \"stimulus\", where=\"stimulus\")\n\n        # Setting ground-truth value for supervised learning\n        self.set_groundtruth(ground_truth, \"decision\")\n\n        return trial\n\n    def _step(self, action):\n        \"\"\"Called internally to process one step.\n\n        Receives an action and returns:\n        a new observation, obs\n        reward associated with the action, reward\n        a boolean variable indicating whether the experiment has terminated, terminated\n            See more at https://gymnasium.farama.org/tutorials/gymnasium_basics/handling_time_limits/#termination\n        a boolean variable indicating whether the experiment has been truncated, truncated\n            See more at https://gymnasium.farama.org/tutorials/gymnasium_basics/handling_time_limits/#truncation\n        a dictionary with extra information:\n            ground truth correct response, info['gt']\n            boolean indicating the end of the trial, info['new_trial'].\n        \"\"\"\n        terminated = False\n        truncated = False\n        # rewards\n        reward = 0\n        gt = self.gt_now\n        # Example structure\n        if not self.in_period(\"decision\"):\n            if action != 0:  # if fixation break\n                reward = self.rewards[\"abort\"]\n        elif action != 0:\n            terminated = True\n            reward = self.rewards[\"correct\"] if action == gt else self.rewards[\"fail\"]\n\n        return (\n            self.ob_now,\n            reward,\n            terminated,\n            truncated,\n            {\"new_trial\": terminated, \"gt\": gt},\n        )\n</pre> class YourTask(ngym.TrialEnv):     def __init__(self, dt=100, rewards=None, timing=None, sigma=1) -&gt; None:         super().__init__(dt=dt)         # Possible decisions at the end of the trial         self.choices = [1, 2]  # e.g. [left, right]         self.sigma = sigma / np.sqrt(self.dt)  # Input noise          # Optional rewards dictionary         self.rewards = {\"abort\": -0.1, \"correct\": +1.0, \"fail\": 0.0}         if rewards:             self.rewards.update(rewards)          # Optional timing dictionary         # if provided, self.add_period can infer timing directly         self.timing = {\"fixation\": 100, \"stimulus\": 2000, \"delay\": 0, \"decision\": 100}         if timing:             self.timing.update(timing)          # Similar to gymnasium envs, define observations_space and action_space         # Optional annotation of the observation space         name = {\"fixation\": 0, \"stimulus\": [1, 2]}         self.observation_space = spaces.Box(             -np.inf,             np.inf,             shape=(3,),             dtype=np.float32,             name=name,         )         # Optional annotation of the action space         name = {\"fixation\": 0, \"choice\": [1, 2]}         self.action_space = spaces.Discrete(3, name=name)      def _new_trial(self, **kwargs):         \"\"\"Called internally to generate a next trial.          Typically, you need to             set trial: a dictionary of trial information             run self.add_period():                 will add time periods to the trial                 accesible through dict self.start_t and self.end_t             run self.add_ob():                 will add observation to np array self.ob             run self.set_groundtruth():                 will set groundtruth to np array self.gt          Returns:             trial: dictionary of trial information         \"\"\"         # Setting trial information         trial = {\"ground_truth\": self.rng.choice(self.choices)}         trial.update(kwargs)  # allows wrappers to modify the trial         ground_truth = trial[\"ground_truth\"]          # Adding periods sequentially         self.add_period([\"fixation\", \"stimulus\", \"delay\", \"decision\"])          # Setting observations, default all 0         # Setting fixation cue to 1 before decision period         self.add_ob(1, where=\"fixation\")         self.set_ob(0, \"decision\", where=\"fixation\")         # Set the stimulus         stim = [0, 0, 0]         stim[ground_truth] = 1         self.add_ob(stim, \"stimulus\")         # adding gaussian noise to stimulus with std = self.sigma         self.add_randn(0, self.sigma, \"stimulus\", where=\"stimulus\")          # Setting ground-truth value for supervised learning         self.set_groundtruth(ground_truth, \"decision\")          return trial      def _step(self, action):         \"\"\"Called internally to process one step.          Receives an action and returns:         a new observation, obs         reward associated with the action, reward         a boolean variable indicating whether the experiment has terminated, terminated             See more at https://gymnasium.farama.org/tutorials/gymnasium_basics/handling_time_limits/#termination         a boolean variable indicating whether the experiment has been truncated, truncated             See more at https://gymnasium.farama.org/tutorials/gymnasium_basics/handling_time_limits/#truncation         a dictionary with extra information:             ground truth correct response, info['gt']             boolean indicating the end of the trial, info['new_trial'].         \"\"\"         terminated = False         truncated = False         # rewards         reward = 0         gt = self.gt_now         # Example structure         if not self.in_period(\"decision\"):             if action != 0:  # if fixation break                 reward = self.rewards[\"abort\"]         elif action != 0:             terminated = True             reward = self.rewards[\"correct\"] if action == gt else self.rewards[\"fail\"]          return (             self.ob_now,             reward,             terminated,             truncated,             {\"new_trial\": terminated, \"gt\": gt},         ) In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n    # Instantiate the task\n    env = YourTask()\n    trial = env.new_trial()\n    logger.info(f\"Trial info: {trial}\")\n    logger.info(f\"Trial observation shape: {env.ob.shape}\")\n    logger.info(f\"Trial action shape: {env.gt.shape}\")\n    env.reset()\n    ob, reward, terminated, truncated, info = env.step(env.action_space.sample())\n    logger.info(f\"Single time step observation shape: {ob.shape}\")\n</pre> if __name__ == \"__main__\":     # Instantiate the task     env = YourTask()     trial = env.new_trial()     logger.info(f\"Trial info: {trial}\")     logger.info(f\"Trial observation shape: {env.ob.shape}\")     logger.info(f\"Trial action shape: {env.gt.shape}\")     env.reset()     ob, reward, terminated, truncated, info = env.step(env.action_space.sample())     logger.info(f\"Single time step observation shape: {ob.shape}\")"},{"location":"examples/understanding_neurogym_task/","title":"Custom tasks","text":"In\u00a0[\u00a0]: Copied! <pre># ! pip install neurogym\n</pre> # ! pip install neurogym  <p>Neurogym tasks follow basic Gymnasium tasks format. Gymnasium is a maintained fork of OpenAI\u2019s Gym library. Each task is defined as a Python class, inheriting from the <code>gymnasium.Env</code> class.</p> <p>In this section we describe basic structure for an gymnasium task.</p> <p>In the <code>__init__</code> method, it is necessary to define two attributes, <code>self.observation_space</code> and <code>self.action_space</code> which describe the kind of spaces used by observations (network inputs) and actions (network outputs).</p> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport gymnasium as gym\n\n\nclass MyEnv(gym.Env):\n    def __init__(self):\n        super().__init__()  # Python boilerplate to initialize base class\n\n        # A two-dimensional box with minimum and maximum value set by low and high\n        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(2,))\n\n        # A discrete space with 3 possible values (0, 1, 2)\n        self.action_space = gym.spaces.Discrete(3)\n\n\n# Instantiate an environment\nenv = MyEnv()\nprint(\"Sample random observation value:\", env.observation_space.sample())\nprint(\"Sample random action value:\", env.action_space.sample())\n</pre> import numpy as np import gymnasium as gym   class MyEnv(gym.Env):     def __init__(self):         super().__init__()  # Python boilerplate to initialize base class          # A two-dimensional box with minimum and maximum value set by low and high         self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(2,))          # A discrete space with 3 possible values (0, 1, 2)         self.action_space = gym.spaces.Discrete(3)   # Instantiate an environment env = MyEnv() print(\"Sample random observation value:\", env.observation_space.sample()) print(\"Sample random action value:\", env.action_space.sample()) <p>Another key method that needs to be defined is the <code>step</code> method, which updates the environment and outputs observations and rewards after receiving the agent's action.</p> <p>The <code>step</code> method takes <code>action</code> as inputs, and outputs the agent's next observation <code>observation</code>, a scalar reward received by the agent <code>reward</code>, a boolean describing whether the environment needs to be reset <code>done</code>, and a dictionary holding any additional information <code>info</code>.</p> <p>If the environment is described by internal states, the <code>reset</code> method would reset these internal states. This method returns an initial observation <code>observation</code>.</p> In\u00a0[\u00a0]: Copied! <pre>class MyEnv(gym.Env):\n    def __init__(self):\n        super().__init__()  # Python boilerplate to initialize base class\n        self.observation_space = gym.spaces.Box(low=-10.0, high=10.0, shape=(1,))\n        self.action_space = gym.spaces.Discrete(3)\n\n    def step(self, action):\n        ob = self.observation_space.sample()  # random sampling\n        reward = 1.0  # reward\n        terminated = False  # never ending\n        truncated = False\n        info = {}  # empty dictionary\n        return ob, reward, terminated, truncated, info\n\n    def reset(self):\n        ob = self.observation_space.sample()\n        return ob, {}\n</pre> class MyEnv(gym.Env):     def __init__(self):         super().__init__()  # Python boilerplate to initialize base class         self.observation_space = gym.spaces.Box(low=-10.0, high=10.0, shape=(1,))         self.action_space = gym.spaces.Discrete(3)      def step(self, action):         ob = self.observation_space.sample()  # random sampling         reward = 1.0  # reward         terminated = False  # never ending         truncated = False         info = {}  # empty dictionary         return ob, reward, terminated, truncated, info      def reset(self):         ob = self.observation_space.sample()         return ob, {} <p>Below we define a simple task where actions move an agent along a one-dimensional line. The reward is determined by the agent's location on this line.</p> In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\n\n\ndef get_reward(x):\n    return np.sin(x) * np.exp(-np.abs(x) / 3)\n\n\nxs = np.linspace(-10, 10, 100)\nplt.plot(xs, get_reward(xs))\nplt.xlabel(\"State value (observation)\")\nplt.ylabel(\"Reward\")\n</pre> import matplotlib.pyplot as plt   def get_reward(x):     return np.sin(x) * np.exp(-np.abs(x) / 3)   xs = np.linspace(-10, 10, 100) plt.plot(xs, get_reward(xs)) plt.xlabel(\"State value (observation)\") plt.ylabel(\"Reward\") In\u00a0[\u00a0]: Copied! <pre>class MyEnv(gym.Env):\n    def __init__(self):\n        # A one-dimensional box with minimum and maximum value set by low and high\n        self.observation_space = gym.spaces.Box(low=-10.0, high=10.0, shape=(1,))\n\n        # A discrete space with 3 possible values (0, 1, 2)\n        self.action_space = gym.spaces.Discrete(3)\n\n        self.state = 0.0\n\n    def step(self, action):\n        # Actions 0, 1, 2 correspond to state change of -0.1, 0, +0.1\n        self.state += (action - 1.0) * 0.1\n        self.state = np.clip(self.state, -10, 10)\n\n        ob = self.state  # observation\n        reward = get_reward(self.state)  # reward\n        terminated = False  # never ending\n        truncated = False\n        info = {}  # empty dictionary\n        return ob, reward, terminated, truncated, info\n\n    def reset(self):\n        # Re-initialize state\n        self.state = self.observation_space.sample()\n        return self.state, {}\n</pre> class MyEnv(gym.Env):     def __init__(self):         # A one-dimensional box with minimum and maximum value set by low and high         self.observation_space = gym.spaces.Box(low=-10.0, high=10.0, shape=(1,))          # A discrete space with 3 possible values (0, 1, 2)         self.action_space = gym.spaces.Discrete(3)          self.state = 0.0      def step(self, action):         # Actions 0, 1, 2 correspond to state change of -0.1, 0, +0.1         self.state += (action - 1.0) * 0.1         self.state = np.clip(self.state, -10, 10)          ob = self.state  # observation         reward = get_reward(self.state)  # reward         terminated = False  # never ending         truncated = False         info = {}  # empty dictionary         return ob, reward, terminated, truncated, info      def reset(self):         # Re-initialize state         self.state = self.observation_space.sample()         return self.state, {} <p>An agent can interact with the environment iteratively.</p> In\u00a0[\u00a0]: Copied! <pre>env = MyEnv()\nob, _ = env.reset()\nob_log = list()\nreward_log = list()\nfor i in range(10000):\n    action = env.action_space.sample()  # A random agent\n    ob, reward, terminated, truncated, info = env.step(action)\n    ob_log.append(ob)\n    reward_log.append(reward)\n\nplt.plot(ob_log, reward_log)\n</pre> env = MyEnv() ob, _ = env.reset() ob_log = list() reward_log = list() for i in range(10000):     action = env.action_space.sample()  # A random agent     ob, reward, terminated, truncated, info = env.step(action)     ob_log.append(ob)     reward_log.append(reward)  plt.plot(ob_log, reward_log) In\u00a0[\u00a0]: Copied! <pre>import neurogym as ngym\nfrom neurogym import TrialEnv\nfrom IPython.display import clear_output\nclear_output()\n\nclass MyTrialEnv(TrialEnv):\n    def __init__(self):\n        super().__init__()\n        self.observation_space = gym.spaces.Box(low=-1.0, high=1.0, shape=(1,))\n        self.action_space = gym.spaces.Discrete(2)\n\n        self.next_ob = np.random.uniform(-1, 1, size=(1,))\n\n    def _new_trial(self):\n        ob = self.next_ob  # observation previously computed\n        # Sample observation for the next trial\n        self.next_ob = np.random.uniform(-1, 1, size=(1,))\n\n        trial = dict()\n        # Ground-truth is 1 if ob &gt; 0, else 0\n        trial[\"ground_truth\"] = (ob &gt; 0) * 1.0\n\n        return trial\n\n    def _step(self, action):\n        ob = self.next_ob\n        # If action equals to ground_truth, reward=1, otherwise 0\n        reward = (action == self.trial[\"ground_truth\"]) * 1.0\n        terminated = False\n        truncated = False\n        info = {\"new_trial\": True}\n        return ob, reward, terminated, truncated, info\n</pre> import neurogym as ngym from neurogym import TrialEnv from IPython.display import clear_output clear_output()  class MyTrialEnv(TrialEnv):     def __init__(self):         super().__init__()         self.observation_space = gym.spaces.Box(low=-1.0, high=1.0, shape=(1,))         self.action_space = gym.spaces.Discrete(2)          self.next_ob = np.random.uniform(-1, 1, size=(1,))      def _new_trial(self):         ob = self.next_ob  # observation previously computed         # Sample observation for the next trial         self.next_ob = np.random.uniform(-1, 1, size=(1,))          trial = dict()         # Ground-truth is 1 if ob &gt; 0, else 0         trial[\"ground_truth\"] = (ob &gt; 0) * 1.0          return trial      def _step(self, action):         ob = self.next_ob         # If action equals to ground_truth, reward=1, otherwise 0         reward = (action == self.trial[\"ground_truth\"]) * 1.0         terminated = False         truncated = False         info = {\"new_trial\": True}         return ob, reward, terminated, truncated, info In\u00a0[\u00a0]: Copied! <pre>env = MyTrialEnv()\nob, _ = env.reset()\n\nprint(\"Trial\", 0)\nprint(\"Received observation\", ob)\n\nfor i in range(5):\n    action = env.action_space.sample()  # A random agent\n    print(\"Selected action\", action)\n    ob, reward, terminated, truncated, info = env.step(action)\n    print(\"Received reward\", reward)\n    print(\"Trial\", i + 1)\n    print(\"Received observation\", ob)\n</pre> env = MyTrialEnv() ob, _ = env.reset()  print(\"Trial\", 0) print(\"Received observation\", ob)  for i in range(5):     action = env.action_space.sample()  # A random agent     print(\"Selected action\", action)     ob, reward, terminated, truncated, info = env.step(action)     print(\"Received reward\", reward)     print(\"Trial\", i + 1)     print(\"Received observation\", ob) In\u00a0[\u00a0]: Copied! <pre>class MyDecisionEnv(TrialEnv):\n    def __init__(self, dt=100, timing=None):\n        super().__init__(dt=dt)  # dt is passed to base task\n\n        # Setting default task timing\n        self.timing = {\"stimulus\": 500, \"decision\": 500}\n        # Update timing if provided externally\n        if timing:\n            self.timing.update(timing)\n\n        self.observation_space = gym.spaces.Box(low=-1.0, high=1.0, shape=(1,))\n        self.action_space = gym.spaces.Discrete(2)\n\n    def _new_trial(self):\n        # Setting time periods for this trial\n        periods = [\"stimulus\", \"decision\"]\n        # Will add stimulus and decision periods sequentially using self.timing info\n        self.add_period(periods)\n\n        # Sample observation for the next trial\n        stimulus = np.random.uniform(-1, 1, size=(1,))\n\n        trial = dict()\n        trial[\"stimulus\"] = stimulus\n        # Ground-truth is 1 if stimulus &gt; 0, else 0\n        trial[\"ground_truth\"] = (stimulus &gt; 0) * 1.0\n\n        return trial\n\n    def _step(self, action):\n        # Check if the current time step is in stimulus period\n        if self.in_period(\"stimulus\"):\n            ob = np.array([self.trial[\"stimulus\"]])\n            reward = 0.0  # no reward\n        else:\n            ob = np.array([0.0])  # no observation\n            # If action equals to ground_truth, reward=1, otherwise 0\n            reward = (action == self.trial[\"ground_truth\"]) * 1.0\n\n        terminated = False\n        truncated = False\n        # By default, the trial is not ended\n        info = {\"new_trial\": False}\n        return ob, reward, terminated, truncated, info\n</pre> class MyDecisionEnv(TrialEnv):     def __init__(self, dt=100, timing=None):         super().__init__(dt=dt)  # dt is passed to base task          # Setting default task timing         self.timing = {\"stimulus\": 500, \"decision\": 500}         # Update timing if provided externally         if timing:             self.timing.update(timing)          self.observation_space = gym.spaces.Box(low=-1.0, high=1.0, shape=(1,))         self.action_space = gym.spaces.Discrete(2)      def _new_trial(self):         # Setting time periods for this trial         periods = [\"stimulus\", \"decision\"]         # Will add stimulus and decision periods sequentially using self.timing info         self.add_period(periods)          # Sample observation for the next trial         stimulus = np.random.uniform(-1, 1, size=(1,))          trial = dict()         trial[\"stimulus\"] = stimulus         # Ground-truth is 1 if stimulus &gt; 0, else 0         trial[\"ground_truth\"] = (stimulus &gt; 0) * 1.0          return trial      def _step(self, action):         # Check if the current time step is in stimulus period         if self.in_period(\"stimulus\"):             ob = np.array([self.trial[\"stimulus\"]])             reward = 0.0  # no reward         else:             ob = np.array([0.0])  # no observation             # If action equals to ground_truth, reward=1, otherwise 0             reward = (action == self.trial[\"ground_truth\"]) * 1.0          terminated = False         truncated = False         # By default, the trial is not ended         info = {\"new_trial\": False}         return ob, reward, terminated, truncated, info <p>Running the environment with a random agent and plotting the agent's observation, action, and rewards</p> In\u00a0[\u00a0]: Copied! <pre># Logging\nlog = {\"ob\": [], \"gt\": [], \"action\": [], \"reward\": []}\n\nenv = MyDecisionEnv(dt=100)\nob, _ = env.reset()\nlog[\"ob\"].append(float(ob))\nlog[\"gt\"].append(float(ob &gt; 0))\nfor i in range(30):\n    action = env.action_space.sample()  # A random agent\n    ob, reward, terminated, truncated, info = env.step(action)\n\n    log[\"action\"].append(float(action))\n    log[\"ob\"].append(float(ob))\n    log[\"gt\"].append(float(ob &gt; 0))\n    log[\"reward\"].append(float(reward))\n\nlog[\"ob\"] = log[\"ob\"][:-1]  # exclude last observation\nlog[\"gt\"] = log[\"gt\"][:-1]  # exclude last observation\n# Visualize\nf, axes = plt.subplots(len(log), 1, sharex=True)\nfor ax, key in zip(axes, log):\n    ax.plot(log[key], \".-\")\n    ax.set_ylabel(key)\n</pre> # Logging log = {\"ob\": [], \"gt\": [], \"action\": [], \"reward\": []}  env = MyDecisionEnv(dt=100) ob, _ = env.reset() log[\"ob\"].append(float(ob)) log[\"gt\"].append(float(ob &gt; 0)) for i in range(30):     action = env.action_space.sample()  # A random agent     ob, reward, terminated, truncated, info = env.step(action)      log[\"action\"].append(float(action))     log[\"ob\"].append(float(ob))     log[\"gt\"].append(float(ob &gt; 0))     log[\"reward\"].append(float(reward))  log[\"ob\"] = log[\"ob\"][:-1]  # exclude last observation log[\"gt\"] = log[\"gt\"][:-1]  # exclude last observation # Visualize f, axes = plt.subplots(len(log), 1, sharex=True) for ax, key in zip(axes, log):     ax.plot(log[key], \".-\")     ax.set_ylabel(key) In\u00a0[\u00a0]: Copied! <pre>class MyDecisionEnv(TrialEnv):\n    def __init__(self, dt=100, timing=None):\n        super().__init__(dt=dt)  # dt is passed to base task\n\n        # Setting default task timing\n        self.timing = {\"stimulus\": 500, \"decision\": 500}\n        # Update timing if provided externally\n        if timing:\n            self.timing.update(timing)\n\n        # Here we use ngym.spaces, which allows setting name of each dimension\n        name = {\"fixation\": 0, \"stimulus\": 1}\n        self.observation_space = ngym.spaces.Box(\n            low=-1.0, high=1.0, shape=(2,), name=name\n        )\n        name = {\"fixation\": 0, \"choice\": [1, 2]}\n        self.action_space = ngym.spaces.Discrete(3, name=name)\n\n    def _new_trial(self):\n        # Setting time periods for this trial\n        periods = [\"stimulus\", \"decision\"]\n        # Will add stimulus and decision periods sequentially using self.timing info\n        self.add_period(periods)\n\n        # Sample observation for the next trial\n        stimulus = np.random.uniform(-1, 1, size=(1,))\n\n        # Add value 1 to stimulus period at fixation location\n        self.add_ob(1, period=\"stimulus\", where=\"fixation\")\n        # Add value stimulus to stimulus period at stimulus location\n        self.add_ob(stimulus, period=\"stimulus\", where=\"stimulus\")\n\n        # Set ground_truth\n        groundtruth = int(stimulus &gt; 0)\n        self.set_groundtruth(groundtruth, period=\"decision\", where=\"choice\")\n\n        trial = dict()\n        trial[\"stimulus\"] = stimulus\n        trial[\"ground_truth\"] = groundtruth\n\n        return trial\n\n    def _step(self, action):\n        # self.ob_now and self.gt_now correspond to\n        # current step observation and groundtruth\n\n        # If action equals to ground_truth, reward=1, otherwise 0\n        reward = (action == self.gt_now) * 1.0\n\n        terminated = False\n        truncated = False\n        # By default, the trial is not ended\n        info = {\"new_trial\": False}\n        return self.ob_now, reward, terminated, truncated, info\n</pre> class MyDecisionEnv(TrialEnv):     def __init__(self, dt=100, timing=None):         super().__init__(dt=dt)  # dt is passed to base task          # Setting default task timing         self.timing = {\"stimulus\": 500, \"decision\": 500}         # Update timing if provided externally         if timing:             self.timing.update(timing)          # Here we use ngym.spaces, which allows setting name of each dimension         name = {\"fixation\": 0, \"stimulus\": 1}         self.observation_space = ngym.spaces.Box(             low=-1.0, high=1.0, shape=(2,), name=name         )         name = {\"fixation\": 0, \"choice\": [1, 2]}         self.action_space = ngym.spaces.Discrete(3, name=name)      def _new_trial(self):         # Setting time periods for this trial         periods = [\"stimulus\", \"decision\"]         # Will add stimulus and decision periods sequentially using self.timing info         self.add_period(periods)          # Sample observation for the next trial         stimulus = np.random.uniform(-1, 1, size=(1,))          # Add value 1 to stimulus period at fixation location         self.add_ob(1, period=\"stimulus\", where=\"fixation\")         # Add value stimulus to stimulus period at stimulus location         self.add_ob(stimulus, period=\"stimulus\", where=\"stimulus\")          # Set ground_truth         groundtruth = int(stimulus &gt; 0)         self.set_groundtruth(groundtruth, period=\"decision\", where=\"choice\")          trial = dict()         trial[\"stimulus\"] = stimulus         trial[\"ground_truth\"] = groundtruth          return trial      def _step(self, action):         # self.ob_now and self.gt_now correspond to         # current step observation and groundtruth          # If action equals to ground_truth, reward=1, otherwise 0         reward = (action == self.gt_now) * 1.0          terminated = False         truncated = False         # By default, the trial is not ended         info = {\"new_trial\": False}         return self.ob_now, reward, terminated, truncated, info <p>Sampling one trial. The trial observation and ground-truth can be used for supervised learning.</p> In\u00a0[\u00a0]: Copied! <pre>env = MyDecisionEnv()\nenv.reset()\n\ntrial = env.new_trial()\nob, gt = env.ob, env.gt\n\nprint(\"Trial information\", trial)\nprint(\"Observation shape is (N_time, N_unit) =\", ob.shape)\nprint(\"Groundtruth shape is (N_time,) =\", gt.shape)\n</pre> env = MyDecisionEnv() env.reset()  trial = env.new_trial() ob, gt = env.ob, env.gt  print(\"Trial information\", trial) print(\"Observation shape is (N_time, N_unit) =\", ob.shape) print(\"Groundtruth shape is (N_time,) =\", gt.shape) <p>Visualizing the environment with a helper function.</p> In\u00a0[\u00a0]: Copied! <pre># Run the environment for 2 trials using a random agent.\nfig = ngym.utils.plot_env(\n    env,\n    ob_traces=[\"stimulus\", \"fixation\"],\n    num_trials=2,\n)\n</pre> # Run the environment for 2 trials using a random agent. fig = ngym.utils.plot_env(     env,     ob_traces=[\"stimulus\", \"fixation\"],     num_trials=2, ) In\u00a0[\u00a0]: Copied! <pre>class PerceptualDecisionMaking(ngym.TrialEnv):\n    \"\"\"Two-alternative forced choice task in which the subject has to\n    integrate two stimuli to decide which one is higher on average.\n\n    Args:\n        stim_scale: Controls the difficulty of the experiment. (def: 1., float)\n        sigma: float, input noise level\n        dim_ring: int, dimension of ring input and output\n    \"\"\"\n\n    metadata = {\n        \"paper_link\": \"https://www.jneurosci.org/content/12/12/4745\",\n        \"paper_name\": \"\"\"The analysis of visual motion: a comparison of\n        neuronal and psychophysical performance\"\"\",\n        \"tags\": [\"perceptual\", \"two-alternative\", \"supervised\"],\n    }\n\n    def __init__(\n        self, dt=100, rewards=None, timing=None, stim_scale=1.0, sigma=1.0, dim_ring=2\n    ):\n        super().__init__(dt=dt)\n        # The strength of evidence, modulated by stim_scale\n        self.cohs = np.array([0, 6.4, 12.8, 25.6, 51.2]) * stim_scale\n        self.sigma = sigma / np.sqrt(self.dt)  # Input noise\n\n        # Rewards\n        self.rewards = {\"abort\": -0.1, \"correct\": +1.0, \"fail\": 0.0}\n        if rewards:\n            self.rewards.update(rewards)\n\n        self.timing = {\"fixation\": 100, \"stimulus\": 2000, \"delay\": 0, \"decision\": 100}\n        if timing:\n            self.timing.update(timing)\n\n        self.abort = False\n\n        self.theta = np.linspace(0, 2 * np.pi, dim_ring + 1)[:-1]\n        self.choices = np.arange(dim_ring)\n\n        name = {\"fixation\": 0, \"stimulus\": range(1, dim_ring + 1)}\n        self.observation_space = ngym.spaces.Box(\n            -np.inf, np.inf, shape=(1 + dim_ring,), dtype=np.float32, name=name\n        )\n        name = {\"fixation\": 0, \"choice\": range(1, dim_ring + 1)}\n        self.action_space = ngym.spaces.Discrete(1 + dim_ring, name=name)\n\n    def _new_trial(self, **kwargs):\n        # Trial info\n        trial = {\n            \"ground_truth\": self.rng.choice(self.choices),\n            \"coh\": self.rng.choice(self.cohs),\n        }\n        trial.update(kwargs)\n\n        coh = trial[\"coh\"]\n        ground_truth = trial[\"ground_truth\"]\n        stim_theta = self.theta[ground_truth]\n\n        # Periods\n        self.add_period([\"fixation\", \"stimulus\", \"delay\", \"decision\"])\n\n        # Observations\n        self.add_ob(1, period=[\"fixation\", \"stimulus\", \"delay\"], where=\"fixation\")\n        stim = np.cos(self.theta - stim_theta) * (coh / 200) + 0.5\n        self.add_ob(stim, \"stimulus\", where=\"stimulus\")\n        self.add_randn(0, self.sigma, \"stimulus\", where=\"stimulus\")\n\n        # Ground truth\n        self.set_groundtruth(ground_truth, period=\"decision\", where=\"choice\")\n\n        return trial\n\n    def _step(self, action):\n        new_trial = False\n        terminated = False\n        truncated = False\n        # rewards\n        reward = 0\n        gt = self.gt_now\n        # observations\n        if self.in_period(\"fixation\"):\n            if action != 0:  # action = 0 means fixating\n                new_trial = self.abort\n                reward += self.rewards[\"abort\"]\n        elif self.in_period(\"decision\"):\n            if action != 0:\n                new_trial = True\n                if action == gt:\n                    reward += self.rewards[\"correct\"]\n                    self.performance = 1\n                else:\n                    reward += self.rewards[\"fail\"]\n\n        return (\n            self.ob_now,\n            reward,\n            terminated,\n            truncated,\n            {\"new_trial\": new_trial, \"gt\": gt},\n        )\n</pre> class PerceptualDecisionMaking(ngym.TrialEnv):     \"\"\"Two-alternative forced choice task in which the subject has to     integrate two stimuli to decide which one is higher on average.      Args:         stim_scale: Controls the difficulty of the experiment. (def: 1., float)         sigma: float, input noise level         dim_ring: int, dimension of ring input and output     \"\"\"      metadata = {         \"paper_link\": \"https://www.jneurosci.org/content/12/12/4745\",         \"paper_name\": \"\"\"The analysis of visual motion: a comparison of         neuronal and psychophysical performance\"\"\",         \"tags\": [\"perceptual\", \"two-alternative\", \"supervised\"],     }      def __init__(         self, dt=100, rewards=None, timing=None, stim_scale=1.0, sigma=1.0, dim_ring=2     ):         super().__init__(dt=dt)         # The strength of evidence, modulated by stim_scale         self.cohs = np.array([0, 6.4, 12.8, 25.6, 51.2]) * stim_scale         self.sigma = sigma / np.sqrt(self.dt)  # Input noise          # Rewards         self.rewards = {\"abort\": -0.1, \"correct\": +1.0, \"fail\": 0.0}         if rewards:             self.rewards.update(rewards)          self.timing = {\"fixation\": 100, \"stimulus\": 2000, \"delay\": 0, \"decision\": 100}         if timing:             self.timing.update(timing)          self.abort = False          self.theta = np.linspace(0, 2 * np.pi, dim_ring + 1)[:-1]         self.choices = np.arange(dim_ring)          name = {\"fixation\": 0, \"stimulus\": range(1, dim_ring + 1)}         self.observation_space = ngym.spaces.Box(             -np.inf, np.inf, shape=(1 + dim_ring,), dtype=np.float32, name=name         )         name = {\"fixation\": 0, \"choice\": range(1, dim_ring + 1)}         self.action_space = ngym.spaces.Discrete(1 + dim_ring, name=name)      def _new_trial(self, **kwargs):         # Trial info         trial = {             \"ground_truth\": self.rng.choice(self.choices),             \"coh\": self.rng.choice(self.cohs),         }         trial.update(kwargs)          coh = trial[\"coh\"]         ground_truth = trial[\"ground_truth\"]         stim_theta = self.theta[ground_truth]          # Periods         self.add_period([\"fixation\", \"stimulus\", \"delay\", \"decision\"])          # Observations         self.add_ob(1, period=[\"fixation\", \"stimulus\", \"delay\"], where=\"fixation\")         stim = np.cos(self.theta - stim_theta) * (coh / 200) + 0.5         self.add_ob(stim, \"stimulus\", where=\"stimulus\")         self.add_randn(0, self.sigma, \"stimulus\", where=\"stimulus\")          # Ground truth         self.set_groundtruth(ground_truth, period=\"decision\", where=\"choice\")          return trial      def _step(self, action):         new_trial = False         terminated = False         truncated = False         # rewards         reward = 0         gt = self.gt_now         # observations         if self.in_period(\"fixation\"):             if action != 0:  # action = 0 means fixating                 new_trial = self.abort                 reward += self.rewards[\"abort\"]         elif self.in_period(\"decision\"):             if action != 0:                 new_trial = True                 if action == gt:                     reward += self.rewards[\"correct\"]                     self.performance = 1                 else:                     reward += self.rewards[\"fail\"]          return (             self.ob_now,             reward,             terminated,             truncated,             {\"new_trial\": new_trial, \"gt\": gt},         ) In\u00a0[\u00a0]: Copied! <pre>env = PerceptualDecisionMaking()\nfig = ngym.utils.plot_env(\n    env,\n    ob_traces=[\"Stim1\", \"Stim2\", \"fixation\"],\n    num_trials=2,\n)\n</pre> env = PerceptualDecisionMaking() fig = ngym.utils.plot_env(     env,     ob_traces=[\"Stim1\", \"Stim2\", \"fixation\"],     num_trials=2, )"},{"location":"examples/understanding_neurogym_task/#understanding-neurogym-tasks","title":"Understanding NeuroGym Tasks\u00b6","text":"<p>This is a tutorial for understanding NeuroGym task structure. Here we will go through</p> <ol> <li>Defining a basic gymnasium task</li> <li>Defining a basic trial-based neurogym task</li> <li>Adding observation and ground truth in NeuroGym tasks</li> </ol>"},{"location":"examples/understanding_neurogym_task/#installation","title":"Installation\u00b6","text":"<p>Google Colab: Uncomment and execute cell below when running this notebook on google colab.</p> <p>Local: Follow these instructions when running this notebook locally.</p>"},{"location":"examples/understanding_neurogym_task/#gymnasium-tasks","title":"Gymnasium tasks\u00b6","text":""},{"location":"examples/understanding_neurogym_task/#visualize-results","title":"Visualize results\u00b6","text":""},{"location":"examples/understanding_neurogym_task/#trial-based-neurogym-tasks","title":"Trial-based Neurogym Tasks\u00b6","text":"<p>Many neuroscience and cognitive science tasks have trial structure. <code>neurogym.TrialEnv</code> provides a class for common trial-based tasks. Its main difference from <code>gymnasium.Env</code> is the <code>_new_trial()</code> method that generates abstract information about a new trial, and optionally, the observation and ground-truth output. Additionally, users provide a <code>_step()</code> method instead of <code>step()</code>.</p> <p>The <code>_new_trial()</code> method takes any key-word arguments (<code>**kwargs</code>), and outputs a dictionary <code>trial</code> containing relevant information about this trial. This dictionary is accesible during <code>_step</code> as <code>self.trial</code>.</p> <p>Here we define a simple task where the agent needs to make a binary decision on every trial based on its observation. Each trial is only one time step.</p>"},{"location":"examples/understanding_neurogym_task/#including-time-period-and-observation-in-trial-based-tasks","title":"Including time, period, and observation in trial-based tasks\u00b6","text":"<p>Most neuroscience and cognitive science tasks follow additional temporal structures that are incorporated into <code>neurogym.TrialEnv</code>. These tasks typically</p> <ol> <li>Are described in real time instead of discrete time steps. For example, the task can last 3 seconds.</li> <li>Contain multiple time periods in each trial, such as a stimulus period and a response period.</li> </ol> <p>To include these features, neurogym tasks typically support setting the time length of each step in <code>dt</code> (in ms), and the time length of each time period in <code>timing</code>.</p> <p>For example, consider the following binary decision-making task with a 500ms stimulus period, followed by a 500ms decision period. The periods are added to each trial through <code>self.add_period()</code> in <code>self._new_trial()</code>. During <code>_step()</code>, you can check which period the task is currently in with <code>self.in_period(period_name)</code>.</p>"},{"location":"examples/understanding_neurogym_task/#setting-observation-and-ground-truth-at-the-beginning-of-each-trial","title":"Setting observation and ground-truth at the beginning of each trial\u00b6","text":"<p>In many tasks, the observation and ground-truth are pre-determined for each trial, and can be set in <code>self._new_trial()</code>. The generated observation and ground-truth can then be used as inputs and targets for supervised learning.</p> <p>Observation and ground_truth can be set in <code>self._new_trial()</code> with the <code>self.add_ob()</code> and <code>self.set_groundtruth</code> methods. Users can specify the period and location of the observation using their names. For example, <code>self.add_ob(1, period='stimulus', where='fixation')</code>.</p> <p>This allows the users to access the observation and groundtruth of the entire trial with <code>self.ob</code> and <code>self.gt</code>, and access their values with <code>self.ob_now</code> and <code>self.gt_now</code>.</p>"},{"location":"examples/understanding_neurogym_task/#an-example-perceptual-decision-making-task","title":"An example perceptual decision-making task\u00b6","text":"<p>Using the above style, we can define a simple perceptual decision-making task (the PerceptualDecisionMaking task from neurogym).</p>"}]}