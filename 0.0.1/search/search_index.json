{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to NeuroGym","text":"<p>NeuroGym is a curated collection of neuroscience tasks with a common interface. The goal is to facilitate training of neural network models on neuroscience tasks.</p> <p>NeuroGym inherits from the machine learning toolkit Gymnasium, a maintained fork of OpenAI\u2019s Gym library. It allows a wide range of well established machine learning algorithms to be easily trained on behavioral paradigms relevant for the neuroscience community. NeuroGym also incorporates several properties and functions (e.g. continuous-time and trial-based tasks) that are important for neuroscience applications. The toolkit also includes various modifier functions that allow easy configuration of new tasks.</p> <p></p> <p>\ud83d\udc1b Bugs reports and \u2b50 features requests here</p> <p>\ud83d\udd27 Pull Requests</p> <p>For more details about how to contribute, see the contribution guidelines.</p>"},{"location":"code_of_conduct/","title":"Code of Conduct","text":""},{"location":"code_of_conduct/#our-pledge","title":"Our Pledge","text":"<p>In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p>"},{"location":"code_of_conduct/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to creating a positive environment include:</p> <ul> <li>Using welcoming and inclusive language</li> <li>Being respectful of differing viewpoints and experiences</li> <li>Gracefully accepting constructive criticism</li> <li>Focusing on what is best for the community</li> <li>Showing empathy towards other community members</li> </ul> <p>Examples of unacceptable behavior by participants include:</p> <ul> <li>The use of sexualized language or imagery and unwelcome sexual attention or   advances</li> <li>Trolling, insulting/derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or electronic   address, without explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"code_of_conduct/#our-responsibilities","title":"Our Responsibilities","text":"<p>Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.</p> <p>Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.</p>"},{"location":"code_of_conduct/#scope","title":"Scope","text":"<p>This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.</p>"},{"location":"code_of_conduct/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at g.crocioni@esciencecenter.nl. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.</p> <p>Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.</p>"},{"location":"code_of_conduct/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html</p>"},{"location":"contributing/","title":"Contributing","text":"<p>We welcome any kind of contribution to our software, from simple comment or question to a full fledged pull request. Please read and follow our Code of Conduct.</p> <p>A contribution can be one of the following cases:</p> <ol> <li>you have a question;</li> <li>you think you may have found a bug (including unexpected behavior);</li> <li>you want to make some kind of change to the code base (e.g. to fix a bug, to add a new feature, to update documentation);</li> <li>you want to make a new release of the code base.</li> </ol> <p>The sections below outline the steps in each case.</p>"},{"location":"contributing/#you-have-a-question","title":"You have a question","text":"<ol> <li>use the search functionality here to see if someone already filed the same issue;</li> <li>if your issue search did not yield any relevant results, make a new issue;</li> <li>apply the \"Question\" label; apply other labels when relevant.</li> </ol>"},{"location":"contributing/#you-think-you-may-have-found-a-bug","title":"You think you may have found a bug","text":"<ol> <li>use the search functionality here to see if someone already filed the same issue;</li> <li>if your issue search did not yield any relevant results, make a new issue, making sure to provide enough information to the rest of the community to understand the cause and context of the problem. Depending on the issue, you may want to include:</li> <li>the SHA hashcode of the commit that is causing your problem;</li> <li>some identifying information (name and version number) for dependencies you're using;</li> <li>information about the operating system;</li> <li>apply relevant labels to the newly created issue.</li> </ol>"},{"location":"contributing/#you-want-to-make-some-kind-of-change-to-the-code-base","title":"You want to make some kind of change to the code base","text":"<ol> <li>(important) announce your plan to the rest of the community before you start working. This announcement should be in the form of a (new) issue;</li> <li>(important) wait until some kind of consensus is reached about your idea being a good idea;</li> <li>if needed, fork the repository to your own Github profile and create your own feature branch off of the latest master commit. While working on your feature branch, make sure to stay up to date with the master branch by pulling in changes, possibly from the 'upstream' repository (follow the instructions here and here);</li> <li>make sure the existing tests still work by running <code>pytest</code>;</li> <li>add your own tests (if necessary);</li> <li>update or expand the documentation;</li> <li>update the <code>CHANGELOG.md</code> file with change;</li> <li>push your feature branch to (your fork of) the annubes repository on GitHub;</li> <li>create the pull request, e.g. following the instructions here.</li> </ol> <p>In case you feel like you've made a valuable contribution, but you don't know how to write or run tests for it, or how to generate the documentation: don't let this discourage you from making the pull request; we can help you! Just go ahead and submit the pull request, but keep in mind that you might be asked to append additional commits to your pull request.</p>"},{"location":"installation/","title":"Installation","text":"<p>Create and activate a virtual environment to install the current package, e.g. using conda (please refer to their site for questions about creating the environment):</p> <pre><code>conda activate # ensures you are in the base environment\nconda create -n neurogym python=3.11\nconda activate neurogym\n</code></pre> <p>Then install neurogym as follows:</p> <pre><code>git clone https://github.com/neurogym/neurogym.git\ncd neurogym\npip install -e .\n</code></pre>"},{"location":"installation/#psychopy-installation","title":"Psychopy installation","text":"<p>If you need psychopy for your project, additionally run</p> <pre><code>pip install psychopy\"\n</code></pre>"},{"location":"license/","title":"License","text":"<p>Apache License</p> <p>Copyright 2024, Giulia Crocioni, Dani L. Bodor, The Netherlands eScience Center</p> <p>Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at</p> <p>http://www.apache.org/licenses/LICENSE-2.0</p> <p>Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p>"},{"location":"neurogym/","title":"NeuroGym","text":""},{"location":"neurogym/#tasks","title":"Tasks","text":"<p>Currently implemented tasks can be found here.</p>"},{"location":"neurogym/#wrappers","title":"Wrappers","text":"<p>Wrappers (see list) are short scripts that allow introducing modifications the original tasks. For instance, the Random Dots Motion task can be transformed into a reaction time task by passing it through the reaction_time wrapper. Alternatively, the combine wrapper allows training an agent in two different tasks simultaneously.</p>"},{"location":"neurogym/#examples","title":"Examples","text":"<p>NeuroGym is compatible with most packages that use gymnasium. In this example jupyter notebook we show how to train a neural network with reinforcement learning algorithms using the Stable-Baselines3 toolbox.</p>"},{"location":"neurogym/#custom-tasks","title":"Custom tasks","text":"<p>Creating custom new tasks should be easy. You can contribute tasks using the regular gymnasium format. If your task has a trial/period structure, this template provides the basic structure that we recommend a task to have:</p> <pre><code>from gymnasium import spaces\nimport neurogym as ngym\n\nclass YourTask(ngym.PeriodEnv):\n    metadata = {}\n\n    def __init__(self, dt=100, timing=None, extra_input_param=None):\n        super().__init__(dt=dt)\n\n\n    def new_trial(self, **kwargs):\n        \"\"\"\n        new_trial() is called when a trial ends to generate the next trial.\n        Here you have to set:\n        The trial periods: fixation, stimulus...\n        Optionally, you can set:\n        The ground truth: the correct answer for the created trial.\n        \"\"\"\n\n    def _step(self, action):\n        \"\"\"\n        _step receives an action and returns:\n            a new observation, obs\n            reward associated with the action, reward\n            a boolean variable indicating whether the experiment has terminated, terminated\n                See more at https://gymnasium.farama.org/tutorials/gymnasium_basics/handling_time_limits/#termination\n            a boolean variable indicating whether the experiment has been truncated, truncated\n                See more at https://gymnasium.farama.org/tutorials/gymnasium_basics/handling_time_limits/#truncation\n            a dictionary with extra information:\n                ground truth correct response, info['gt']\n                boolean indicating the end of the trial, info['new_trial']\n        \"\"\"\n\n        return obs, reward, terminated, truncated, {'new_trial': new_trial, 'gt': gt}\n</code></pre>"},{"location":"api/core/","title":"Core","text":""},{"location":"api/core/#neurogym.core.TrialEnv","title":"TrialEnv","text":"<pre><code>TrialEnv(dt=100, num_trials_before_reset=10000000, r_tmax=0)\n</code></pre> <p>               Bases: <code>BaseEnv</code></p> <p>The main Neurogym class for trial-based envs.</p> Source code in <code>neurogym/core.py</code> <pre><code>def __init__(self, dt=100, num_trials_before_reset=10000000, r_tmax=0) -&gt; None:\n    super().__init__(dt=dt)\n    self.r_tmax = r_tmax\n    self.num_tr = 0\n    self.num_tr_exp = num_trials_before_reset\n    self.trial: dict | None = None\n    self._ob_built = False\n    self._gt_built = False\n    self._has_gt = False  # check if the task ever defined gt\n\n    self._default_ob_value = None  # default to 0\n\n    # For optional periods\n    self.timing: dict = {}\n    self.start_t: dict = {}\n    self.end_t: dict = {}\n    self.start_ind: dict = {}\n    self.end_ind: dict = {}\n    self._tmax = 0  # Length of each trial\n\n    self._top = self\n    self._duration: dict = {}\n</code></pre>"},{"location":"api/core/#neurogym.core.TrialEnv.seed","title":"seed","text":"<pre><code>seed(seed=None)\n</code></pre> <p>Set random seed.</p> Source code in <code>neurogym/core.py</code> <pre><code>def seed(self, seed=None):\n    \"\"\"Set random seed.\"\"\"\n    self.rng = np.random.RandomState(seed)\n    if hasattr(self, \"action_space\") and self.action_space is not None:\n        self.action_space.seed(seed)\n    for val in self.timing.values():\n        with contextlib.suppress(AttributeError):\n            val.seed(seed)\n    return [seed]\n</code></pre>"},{"location":"api/core/#neurogym.core.TrialEnv.post_step","title":"post_step","text":"<pre><code>post_step(ob, reward, terminated, truncated, info)\n</code></pre> <p>Optional task-specific wrapper applied at the end of step.</p> <p>It allows to modify ob online (e.g. provide a specific observation for different actions made by the agent)</p> Source code in <code>neurogym/core.py</code> <pre><code>def post_step(self, ob, reward, terminated, truncated, info):\n    \"\"\"Optional task-specific wrapper applied at the end of step.\n\n    It allows to modify ob online (e.g. provide a specific observation for different actions made by the agent)\n    \"\"\"\n    return ob, reward, terminated, truncated, info\n</code></pre>"},{"location":"api/core/#neurogym.core.TrialEnv.new_trial","title":"new_trial","text":"<pre><code>new_trial(**kwargs)\n</code></pre> <p>Public interface for starting a new trial.</p> <p>Returns:</p> Name Type Description <code>trial</code> <p>dict of trial information. Available to step function as self.trial</p> Source code in <code>neurogym/core.py</code> <pre><code>def new_trial(self, **kwargs):\n    \"\"\"Public interface for starting a new trial.\n\n    Returns:\n        trial: dict of trial information. Available to step function as\n            self.trial\n    \"\"\"\n    # Reset for next trial\n    self._tmax = 0  # reset, self.tmax not reset so it can be used in step\n    self._ob_built = False\n    self._gt_built = False\n    trial = self._new_trial(**kwargs)\n    self.trial = trial\n    self.num_tr += 1  # Increment trial count\n    self._has_gt = self._gt_built\n    return trial\n</code></pre>"},{"location":"api/core/#neurogym.core.TrialEnv.step","title":"step","text":"<pre><code>step(action)\n</code></pre> <p>Public interface for the environment.</p> Source code in <code>neurogym/core.py</code> <pre><code>def step(self, action):\n    \"\"\"Public interface for the environment.\"\"\"\n    ob, reward, terminated, truncated, info = self._step(action)\n\n    if \"new_trial\" not in info:\n        info[\"new_trial\"] = False\n\n    if self._has_gt and \"gt\" not in info:\n        # If gt is built, default gt to gt_now\n        # must run before incrementing t\n        info[\"gt\"] = self.gt_now\n\n    self.t += self.dt  # increment within trial time count\n    self.t_ind += 1\n\n    if self.t + self.dt &gt; self.tmax and not info[\"new_trial\"]:\n        info[\"new_trial\"] = True\n        reward += self.r_tmax\n\n    # TODO: new_trial happens after step, so trial indx precedes obs change\n    if info[\"new_trial\"]:\n        info[\"performance\"] = self.performance\n        self.t = self.t_ind = 0  # Reset within trial time count\n        trial = self._top.new_trial()\n        self.performance = 0\n        info[\"trial\"] = trial\n    if ob is OBNOW:\n        ob = self.ob[self.t_ind]\n    return self.post_step(ob, reward, terminated, truncated, info)\n</code></pre>"},{"location":"api/core/#neurogym.core.TrialEnv.reset","title":"reset","text":"<pre><code>reset(seed=None, options=None)\n</code></pre> <p>Reset the environment.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <p>random seed, overwrites self.seed if not None</p> <code>None</code> <code>options</code> <p>additional options used to reset the env. Can include 'step_fn' and 'no_step'. <code>step_fn</code> can be a function or None. If function, overwrite original <code>self.step</code> method. <code>no_step</code> is a bool. If True, no step is taken and observation randomly sampled. It defaults to False.</p> <code>None</code> Source code in <code>neurogym/core.py</code> <pre><code>def reset(self, seed=None, options=None):\n    \"\"\"Reset the environment.\n\n    Args:\n        seed: random seed, overwrites self.seed if not None\n        options: additional options used to reset the env.\n            Can include 'step_fn' and 'no_step'.\n            `step_fn` can be a function or None. If function, overwrite original\n            `self.step` method.\n            `no_step` is a bool. If True, no step is taken and observation randomly\n            sampled. It defaults to False.\n    \"\"\"\n    super().reset(seed=seed)\n\n    self.num_tr = 0\n    self.t = self.t_ind = 0\n\n    step_fn = options.get(\"step_fn\") if options else None\n    no_step = options.get(\"no_step\", False) if options else False\n\n    self._top.new_trial()\n\n    # have to also call step() to get the initial ob since some wrappers modify step() but not new_trial()\n    self.action_space.seed(0)\n    if no_step:\n        return self.observation_space.sample(), {}\n    if step_fn is None:\n        ob, _, _, _, _ = self._top.step(self.action_space.sample())\n    else:\n        ob, _, _, _, _ = step_fn(self.action_space.sample())\n    return ob, {}\n</code></pre>"},{"location":"api/core/#neurogym.core.TrialEnv.render","title":"render","text":"<pre><code>render(mode='human') -&gt; None\n</code></pre> <p>Plots relevant variables/parameters.</p> Source code in <code>neurogym/core.py</code> <pre><code>def render(self, mode=\"human\") -&gt; None:\n    \"\"\"Plots relevant variables/parameters.\"\"\"\n</code></pre>"},{"location":"api/core/#neurogym.core.TrialEnv.set_top","title":"set_top","text":"<pre><code>set_top(wrapper) -&gt; None\n</code></pre> <p>Set top to be wrapper.</p> Source code in <code>neurogym/core.py</code> <pre><code>def set_top(self, wrapper) -&gt; None:\n    \"\"\"Set top to be wrapper.\"\"\"\n    self._top = wrapper\n</code></pre>"},{"location":"api/core/#neurogym.core.TrialEnv.add_period","title":"add_period","text":"<pre><code>add_period(period, duration=None, before=None, after=None, last_period=False) -&gt; None\n</code></pre> <p>Add an period.</p> <p>Parameters:</p> Name Type Description Default <code>period</code> <p>string or list of strings, name of the period</p> required <code>duration</code> <p>float or None, duration of the period if None, inferred from timing_fn</p> <code>None</code> <code>before</code> <p>(optional) str, name of period that this period is before</p> <code>None</code> <code>after</code> <p>(optional) str, name of period that this period is after or float, time of period start</p> <code>None</code> <code>last_period</code> <p>bool, default False. If True, then this is last period will generate self.tmax, self.tind, and self.ob</p> <code>False</code> Source code in <code>neurogym/core.py</code> <pre><code>def add_period(\n    self,\n    period,\n    duration=None,\n    before=None,\n    after=None,\n    last_period=False,\n) -&gt; None:\n    \"\"\"Add an period.\n\n    Args:\n        period: string or list of strings, name of the period\n        duration: float or None, duration of the period\n            if None, inferred from timing_fn\n        before: (optional) str, name of period that this period is before\n        after: (optional) str, name of period that this period is after\n            or float, time of period start\n        last_period: bool, default False. If True, then this is last period\n            will generate self.tmax, self.tind, and self.ob\n    \"\"\"\n    if self._ob_built:\n        msg = \"Cannot add period after ob is built, i.e. after running add_ob.\"\n        raise InvalidOperationError(msg)\n    if isinstance(period, str):\n        pass\n    else:\n        if duration is None:\n            duration = [None] * len(period)\n        elif len(duration) != len(period):\n            msg = f\"{len(duration)=} and {len(period)=} must be the same.\"\n            raise ValueError(msg)\n\n        # Recursively calling itself\n        self.add_period(period[0], duration=duration[0], after=after)\n        for i in range(1, len(period)):\n            is_last = (i == len(period) - 1) and last_period\n            self.add_period(\n                period[i],\n                duration=duration[i],\n                after=period[i - 1],\n                last_period=is_last,\n            )\n        return\n\n    if duration is None:\n        duration = self.sample_time(period)\n    self._duration[period] = duration\n\n    if after is not None:\n        start = self.end_t[after] if isinstance(after, str) else after\n    elif before is not None:\n        start = self.start_t[before] - duration\n    else:\n        start = 0  # default start with 0\n\n    self.start_t[period] = start\n    self.end_t[period] = start + duration\n    self.start_ind[period] = int(start / self.dt)\n    self.end_ind[period] = int((start + duration) / self.dt)\n\n    self._tmax = max(self._tmax, start + duration)\n    self.tmax = int(self._tmax / self.dt) * self.dt\n</code></pre>"},{"location":"api/core/#neurogym.core.TrialEnv.view_ob","title":"view_ob","text":"<pre><code>view_ob(period=None)\n</code></pre> <p>View observation of an period.</p> Source code in <code>neurogym/core.py</code> <pre><code>def view_ob(self, period=None):\n    \"\"\"View observation of an period.\"\"\"\n    if not self._ob_built:\n        self._init_ob()\n\n    if period is None:\n        return self.ob\n    return self.ob[self.start_ind[period] : self.end_ind[period]]\n</code></pre>"},{"location":"api/core/#neurogym.core.TrialEnv.add_ob","title":"add_ob","text":"<pre><code>add_ob(value, period=None, where=None) -&gt; None\n</code></pre> <p>Add value to observation.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <p>array-like (ob_space.shape, ...)</p> required <code>period</code> <p>string, must be name of an added period</p> <code>None</code> <code>where</code> <p>string or np array, location of stimulus to be added</p> <code>None</code> Source code in <code>neurogym/core.py</code> <pre><code>def add_ob(self, value, period=None, where=None) -&gt; None:\n    \"\"\"Add value to observation.\n\n    Args:\n        value: array-like (ob_space.shape, ...)\n        period: string, must be name of an added period\n        where: string or np array, location of stimulus to be added\n    \"\"\"\n    self._add_ob(value, period, where, reset=False)\n</code></pre>"},{"location":"api/core/#neurogym.core.TrialEnv.set_groundtruth","title":"set_groundtruth","text":"<pre><code>set_groundtruth(value, period=None, where=None) -&gt; None\n</code></pre> <p>Set groundtruth value.</p> Source code in <code>neurogym/core.py</code> <pre><code>def set_groundtruth(self, value, period=None, where=None) -&gt; None:\n    \"\"\"Set groundtruth value.\"\"\"\n    if not self._gt_built:\n        self._init_gt()\n\n    if where is not None:\n        # TODO: Only works for Discrete action_space, make it work for Box\n        value = self.action_space.name[where][value]  # type: ignore[attr-defined]\n    if isinstance(period, str):\n        self.gt[self.start_ind[period] : self.end_ind[period]] = value\n    elif period is None:\n        self.gt[:] = value\n    else:\n        for p in period:\n            self.set_groundtruth(value, p)\n</code></pre>"},{"location":"api/core/#neurogym.core.TrialEnv.view_groundtruth","title":"view_groundtruth","text":"<pre><code>view_groundtruth(period)\n</code></pre> <p>View observation of an period.</p> Source code in <code>neurogym/core.py</code> <pre><code>def view_groundtruth(self, period):\n    \"\"\"View observation of an period.\"\"\"\n    if not self._gt_built:\n        self._init_gt()\n    return self.gt[self.start_ind[period] : self.end_ind[period]]\n</code></pre>"},{"location":"api/core/#neurogym.core.TrialEnv.in_period","title":"in_period","text":"<pre><code>in_period(period, t=None)\n</code></pre> <p>Check if current time or time t is in period.</p> Source code in <code>neurogym/core.py</code> <pre><code>def in_period(self, period, t=None):\n    \"\"\"Check if current time or time t is in period.\"\"\"\n    if t is None:\n        t = self.t  # Default\n    return self.start_t[period] &lt;= t &lt; self.end_t[period]\n</code></pre>"},{"location":"api/core/#neurogym.core.BaseEnv","title":"BaseEnv","text":"<pre><code>BaseEnv(dt=100)\n</code></pre> <p>               Bases: <code>Env</code></p> <p>The base Neurogym class to include dt.</p> Source code in <code>neurogym/core.py</code> <pre><code>def __init__(self, dt=100) -&gt; None:\n    super().__init__()\n    self.dt = dt\n    self.t = self.t_ind = 0\n    self.tmax = 10000  # maximum time steps\n    self.performance = 0\n    self.rewards: dict = {}\n    self.rng = np.random.RandomState()\n</code></pre>"},{"location":"api/core/#neurogym.core.BaseEnv.seed","title":"seed","text":"<pre><code>seed(seed=None)\n</code></pre> <p>Set random seed.</p> Source code in <code>neurogym/core.py</code> <pre><code>def seed(self, seed=None):\n    \"\"\"Set random seed.\"\"\"\n    self.rng = np.random.RandomState(seed)\n    if self.action_space is not None:\n        self.action_space.seed(seed)\n    return [seed]\n</code></pre>"},{"location":"api/envs/","title":"Environments","text":""},{"location":"api/envs/#neurogym.envs.annubes","title":"annubes","text":""},{"location":"api/envs/#neurogym.envs.annubes.AnnubesEnv","title":"AnnubesEnv","text":"<pre><code>AnnubesEnv(session: dict[str, float] | None = None, stim_intensities: list[float] | None = None, stim_time: int = 1000, catch_prob: float = 0.5, max_sequential: int | None = None, fix_intensity: float = 0, fix_time: Any = 500, iti: Any = 0, dt: int = 100, tau: int = 100, output_behavior: list[float] | None = None, noise_std: float = 0.01, rewards: dict[str, float] | None = None, random_seed: int | None = None)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>General class for the Annubes type of tasks.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>dict[str, float] | None</code> <p>Configuration of the trials that can appear during a session. It is given by a dictionary representing the ratio (values) of the different trials (keys) within the task. Trials with a single modality (e.g., a visual trial) must be represented by single characters, while trials with multiple modalities (e.g., an audiovisual trial) are represented by the character combination of those trials. Note that values are read relative to each other, such that e.g. <code>{\"v\": 0.25, \"a\": 0.75}</code> is equivalent to <code>{\"v\": 1, \"a\": 3}</code>. Defaults to {\"v\": 0.5, \"a\": 0.5}.</p> <code>None</code> <code>stim_intensities</code> <code>list[float] | None</code> <p>List of possible intensity values of each stimulus, when the stimulus is present. Note that when the stimulus is not present, the intensity is set to 0. Defaults to [0.8, 0.9, 1].</p> <code>None</code> <code>stim_time</code> <code>int</code> <p>Duration of each stimulus in ms. Defaults to 1000.</p> <code>1000</code> <code>catch_prob</code> <code>float</code> <p>Probability of catch trials in the session. Must be between 0 and 1 (inclusive). Defaults to 0.5.</p> <code>0.5</code> <code>max_sequential</code> <code>int | None</code> <p>Maximum number of sequential trials of the same modality. It applies only to the modalities defined in <code>session</code>, i.e., it does not apply to catch trials.  Defaults to None (no maximum).</p> <code>None</code> <code>fix_intensity</code> <code>float</code> <p>Intensity of input signal during fixation. Defaults to 0.</p> <code>0</code> <code>fix_time</code> <code>Any</code> <p>Fixation time specification. Can be one of the following: - A number (int or float): Fixed duration in milliseconds. - A callable: Function that returns the duration when called. - A list of numbers: Random choice from the list. - A tuple specifying a distribution:     - (\"uniform\", (min, max)): Uniform distribution between min and max.     - (\"choice\", [options]): Random choice from the given options.     - (\"truncated_exponential\", [parameters]): Truncated exponential distribution.     - (\"constant\", value): Always returns the given value.     - (\"until\", end_time): Sets duration to reach the specified end time. The final duration is rounded down to the nearest multiple of the simulation timestep (dt). Note that the duration of each input and output signal is increased by this time. Defaults to 500.</p> <code>500</code> <code>iti</code> <code>Any</code> <p>Inter-trial interval, or time window between sequential trials, in ms. Same format as <code>fix_time</code>. Defaults to 0.</p> <code>0</code> <code>dt</code> <code>int</code> <p>Time step in ms. Defaults to 100.</p> <code>100</code> <code>tau</code> <code>int</code> <p>Time constant in ms. Defaults to 100.</p> <code>100</code> <code>output_behavior</code> <code>list[float] | None</code> <p>List of possible intensity values of the behavioral output. Currently only the smallest and largest value of this list are used. Defaults to [0, 1].</p> <code>None</code> <code>noise_std</code> <code>float</code> <p>Standard deviation of the input noise. Defaults to 0.01.</p> <code>0.01</code> <code>rewards</code> <code>dict[str, float] | None</code> <p>Dictionary of rewards for different outcomes. The keys are \"abort\", \"correct\", and \"fail\". Defaults to {\"abort\": -0.1, \"correct\": +1.0, \"fail\": 0.0}.</p> <code>None</code> <code>random_seed</code> <code>int | None</code> <p>Seed for numpy's random number generator (rng). If an int is given, it will be used as the seed for <code>np.random.default_rng()</code>. Defaults to None (i.e. the initial state itself is random).</p> <code>None</code> Source code in <code>neurogym/envs/annubes.py</code> <pre><code>def __init__(\n    self,\n    session: dict[str, float] | None = None,\n    stim_intensities: list[float] | None = None,\n    stim_time: int = 1000,\n    catch_prob: float = 0.5,\n    max_sequential: int | None = None,\n    fix_intensity: float = 0,\n    fix_time: Any = 500,\n    iti: Any = 0,\n    dt: int = 100,\n    tau: int = 100,\n    output_behavior: list[float] | None = None,\n    noise_std: float = 0.01,\n    rewards: dict[str, float] | None = None,\n    random_seed: int | None = None,\n):\n    if session is None:\n        session = {\"v\": 0.5, \"a\": 0.5}\n    if output_behavior is None:\n        output_behavior = [0, 1]\n    if stim_intensities is None:\n        stim_intensities = [0.8, 0.9, 1.0]\n    if session is None:\n        session = {\"v\": 0.5, \"a\": 0.5}\n    super().__init__(dt=dt)\n    self.session = {i: session[i] / sum(session.values()) for i in session}\n    self.stim_intensities = stim_intensities\n    self.stim_time = stim_time\n    self.catch_prob = catch_prob\n    self.max_sequential = max_sequential\n    self.sequential_count = 1\n    self.last_modality: str | None = None\n    self.fix_intensity = fix_intensity\n    self.fix_time = fix_time\n    self.iti = iti\n    self.dt = dt\n    self.tau = tau\n    self.output_behavior = output_behavior\n    self.noise_std = noise_std\n    self.random_seed = random_seed\n    alpha = dt / self.tau\n    self.noise_factor = self.noise_std * np.sqrt(2 * alpha) / alpha\n    # Set random state\n    if random_seed is None:\n        rng = np.random.default_rng(random_seed)\n        self._random_seed = rng.integers(2**32)\n    else:\n        self._random_seed = random_seed\n    self._rng = np.random.default_rng(self._random_seed)\n    # Rewards\n    if rewards is None:\n        self.rewards = {\"abort\": -0.1, \"correct\": +1.0, \"fail\": 0.0}\n    else:\n        self.rewards = rewards\n    self.timing = {\"fixation\": self.fix_time, \"stimulus\": self.stim_time, \"iti\": self.iti}\n    # Set the name of each input dimension\n    obs_space_name = {\"fixation\": 0, \"start\": 1, **{trial: i for i, trial in enumerate(session, 2)}}\n    self.observation_space = ngym.spaces.Box(low=0.0, high=1.0, shape=(len(obs_space_name),), name=obs_space_name)\n    # Set the name of each action value\n    self.action_space = ngym.spaces.Discrete(\n        n=len(self.output_behavior),\n        name={\"fixation\": self.fix_intensity, \"choice\": self.output_behavior[1:]},\n    )\n</code></pre>"},{"location":"api/envs/#neurogym.envs.antireach","title":"antireach","text":"<p>Anti-reach or anti-saccade task.</p>"},{"location":"api/envs/#neurogym.envs.antireach.AntiReach","title":"AntiReach","text":"<pre><code>AntiReach(dt=100, anti=True, rewards=None, timing=None, dim_ring=32)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>Anti-response task.</p> <p>During the fixation period, the agent fixates on a fixation point. During the following stimulus period, the agent is then shown a stimulus away from the fixation point. Finally, the agent needs to respond in the opposite direction of the stimulus during the decision period.</p> <p>Parameters:</p> Name Type Description Default <code>anti</code> <p>bool, if True, requires an anti-response. If False, requires a pro-response, i.e. response towards the stimulus.</p> <code>True</code> Source code in <code>neurogym/envs/antireach.py</code> <pre><code>def __init__(self, dt=100, anti=True, rewards=None, timing=None, dim_ring=32) -&gt; None:\n    super().__init__(dt=dt)\n\n    self.anti = anti\n\n    # Rewards\n    self.rewards = {\"abort\": -0.1, \"correct\": +1.0, \"fail\": 0.0}\n    if rewards:\n        self.rewards.update(rewards)\n\n    self.timing = {\"fixation\": 500, \"stimulus\": 500, \"delay\": 0, \"decision\": 500}\n    if timing:\n        self.timing.update(timing)\n\n    self.abort = False\n\n    # action and observation spaces\n    self.dim_ring = dim_ring\n    self.theta = np.arange(0, 2 * np.pi, 2 * np.pi / dim_ring)\n    self.choices = np.arange(dim_ring)\n\n    name = {\"fixation\": 0, \"stimulus\": range(1, dim_ring + 1)}\n    self.observation_space = spaces.Box(\n        -np.inf,\n        np.inf,\n        shape=(1 + dim_ring,),\n        dtype=np.float32,\n        name=name,\n    )\n\n    name = {\"fixation\": 0, \"choice\": range(1, dim_ring + 1)}\n    self.action_space = spaces.Discrete(1 + dim_ring, name=name)\n</code></pre>"},{"location":"api/envs/#neurogym.envs.bandit","title":"bandit","text":"<p>Multi-arm Bandit task.</p>"},{"location":"api/envs/#neurogym.envs.bandit.Bandit","title":"Bandit","text":"<pre><code>Bandit(dt: int = 100, n: int = 2, p: tuple[float, ...] | list[float] = (0.5, 0.5), rewards: None | list[float] | ndarray = None, timing: None | dict = None)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>Multi-arm bandit task.</p> <p>On each trial, the agent is presented with multiple choices. Each option produces a reward of a certain magnitude given a certain probability.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>int, the number of choices (arms)</p> <code>2</code> <code>p</code> <code>tuple[float, ...] | list[float]</code> <p>tuple of length n, describes the probability of each arm leading to reward</p> <code>(0.5, 0.5)</code> <code>rewards</code> <code>None | list[float] | ndarray</code> <p>tuple of length n, describe the reward magnitude of each option when rewarded</p> <code>None</code> Source code in <code>neurogym/envs/bandit.py</code> <pre><code>def __init__(\n    self,\n    dt: int = 100,\n    n: int = 2,\n    p: tuple[float, ...] | list[float] = (0.5, 0.5),\n    rewards: None | list[float] | np.ndarray = None,\n    timing: None | dict = None,\n) -&gt; None:\n    super().__init__(dt=dt)\n    if timing is not None:\n        print(\"Warning: Bandit task does not require timing variable.\")\n\n    self.n = n\n    self._p = np.array(p)  # Reward probabilities\n\n    if rewards is not None:\n        self._rewards = np.array(rewards)\n    else:\n        self._rewards = np.ones(n)  # 1 for every arm\n\n    self.observation_space = spaces.Box(\n        -np.inf,\n        np.inf,\n        shape=(1,),\n        dtype=np.float32,\n    )\n    self.action_space = spaces.Discrete(n)\n</code></pre>"},{"location":"api/envs/#neurogym.envs.contextdecisionmaking","title":"contextdecisionmaking","text":""},{"location":"api/envs/#neurogym.envs.contextdecisionmaking.SingleContextDecisionMaking","title":"SingleContextDecisionMaking","text":"<pre><code>SingleContextDecisionMaking(dt=100, context=0, rewards=None, timing=None, sigma=1.0, dim_ring=2)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>Context-dependent decision-making task.</p> <p>The agent simultaneously receives stimulus inputs from two modalities ( for example, a colored random dot motion pattern with color and motion modalities). The agent needs to make a perceptual decision based on only one of the two modalities, while ignoring the other. The agent reports its decision during the decision period, with an optional delay period in between the stimulus period and the decision period. The relevant modality is not explicitly signaled.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <p>int, 0 or 1 for the two context (rules). If 0, need to focus on modality 0 (the first one)</p> <code>0</code> Source code in <code>neurogym/envs/contextdecisionmaking.py</code> <pre><code>def __init__(\n    self,\n    dt=100,\n    context=0,\n    rewards=None,\n    timing=None,\n    sigma=1.0,\n    dim_ring=2,\n) -&gt; None:\n    super().__init__(dt=dt)\n\n    # trial conditions\n    self.cohs = [5, 15, 50]\n    self.sigma = sigma / np.sqrt(self.dt)  # Input noise\n    self.context = context\n\n    # Rewards\n    self.rewards = {\"abort\": -0.1, \"correct\": +1.0}\n    if rewards:\n        self.rewards.update(rewards)\n\n    self.timing = {\n        \"fixation\": 300,\n        # 'target': 350, # noqa: ERA001\n        \"stimulus\": 750,\n        \"delay\": ngym.random.TruncExp(600, 300, 3000),\n        \"decision\": 100,\n    }\n    if timing:\n        self.timing.update(timing)\n\n    self.abort = False\n\n    # set action and observation space\n    self.theta = np.linspace(0, 2 * np.pi, dim_ring + 1)[:-1]\n    self.choices = np.arange(dim_ring)\n\n    name = {\n        \"fixation\": 0,\n        \"stimulus_mod1\": range(1, dim_ring + 1),\n        \"stimulus_mod2\": range(dim_ring + 1, 2 * dim_ring + 1),\n    }\n    shape = (1 + 2 * dim_ring,)\n    self.observation_space = spaces.Box(\n        -np.inf,\n        np.inf,\n        shape=shape,\n        dtype=np.float32,\n        name=name,\n    )\n\n    name = {\"fixation\": 0, \"choice\": range(1, dim_ring + 1)}\n    self.action_space = spaces.Discrete(1 + dim_ring, name=name)\n</code></pre>"},{"location":"api/envs/#neurogym.envs.contextdecisionmaking.ContextDecisionMaking","title":"ContextDecisionMaking","text":"<pre><code>ContextDecisionMaking(dt=100, rewards=None, timing=None, sigma=1.0)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>Context-dependent decision-making task.</p> <p>The agent simultaneously receives stimulus inputs from two modalities ( for example, a colored random dot motion pattern with color and motion modalities). The agent needs to make a perceptual decision based on only one of the two modalities, while ignoring the other. The relevant modality is explicitly indicated by a rule signal.</p> Source code in <code>neurogym/envs/contextdecisionmaking.py</code> <pre><code>def __init__(self, dt=100, rewards=None, timing=None, sigma=1.0) -&gt; None:\n    super().__init__(dt=dt)\n\n    # trial conditions\n    self.contexts = [0, 1]  # index for context inputs\n    self.choices = [1, 2]  # left, right choice\n    self.cohs = [5, 15, 50]\n    self.sigma = sigma / np.sqrt(self.dt)  # Input noise\n\n    # Rewards\n    self.rewards = {\"abort\": -0.1, \"correct\": +1.0}\n    if rewards:\n        self.rewards.update(rewards)\n\n    self.timing = {\n        \"fixation\": 300,\n        # 'target': 350, # noqa: ERA001\n        \"stimulus\": 750,\n        \"delay\": ngym.random.TruncExp(600, 300, 3000),\n        \"decision\": 100,\n    }\n    if timing:\n        self.timing.update(timing)\n\n    self.abort = False\n\n    # set action and observation space\n    names = [\n        \"fixation\",\n        \"stim1_mod1\",\n        \"stim2_mod1\",\n        \"stim1_mod2\",\n        \"stim2_mod2\",\n        \"context1\",\n        \"context2\",\n    ]\n    name = {name: i for i, name in enumerate(names)}\n    self.observation_space = spaces.Box(\n        -np.inf,\n        np.inf,\n        shape=(7,),\n        dtype=np.float32,\n        name=name,\n    )\n\n    name = {\"fixation\": 0, \"choice1\": 1, \"choice2\": 2}\n    self.action_space = spaces.Discrete(3, name=name)\n</code></pre>"},{"location":"api/envs/#neurogym.envs.dawtwostep","title":"dawtwostep","text":""},{"location":"api/envs/#neurogym.envs.dawtwostep.DawTwoStep","title":"DawTwoStep","text":"<pre><code>DawTwoStep(dt=100, rewards=None, timing=None)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>Daw Two-step task.</p> <p>On each trial, an initial choice between two options lead to either of two, second-stage states. In turn, these both demand another two-option choice, each of which is associated with a different chance of receiving reward.</p> Source code in <code>neurogym/envs/dawtwostep.py</code> <pre><code>def __init__(self, dt=100, rewards=None, timing=None) -&gt; None:\n    super().__init__(dt=dt)\n    if timing is not None:\n        print(\"Warning: Two-step task does not require timing variable.\")\n    # Actions are ('FIXATE', 'ACTION1', 'ACTION2')\n    self.actions = [0, 1, 2]\n\n    # trial conditions\n    self.p1 = 0.8  # prob of transitioning to state1 with action1 (&gt;=05)\n    self.p2 = 0.8  # prob of transitioning to state2 with action2 (&gt;=05)\n    self.p_switch = 0.025  # switch reward contingency\n    self.high_reward_p = 0.9\n    self.low_reward_p = 0.1\n    self.tmax = 3 * self.dt\n    self.mean_trial_duration = self.tmax\n    self.state1_high_reward = True\n    # Rewards\n    self.rewards = {\"abort\": -0.1, \"correct\": +1.0}\n    if rewards:\n        self.rewards.update(rewards)\n\n    self.action_space = spaces.Discrete(3)\n    self.observation_space = spaces.Box(\n        -np.inf,\n        np.inf,\n        shape=(3,),\n        dtype=np.float32,\n    )\n</code></pre>"},{"location":"api/envs/#neurogym.envs.delaycomparison","title":"delaycomparison","text":""},{"location":"api/envs/#neurogym.envs.delaycomparison.DelayComparison","title":"DelayComparison","text":"<pre><code>DelayComparison(dt=100, vpairs=None, rewards=None, timing=None, sigma=1.0)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>Delayed comparison.</p> <p>The agent needs to compare the magnitude of two stimuli are separated by a delay period. The agent reports its decision of the stronger stimulus during the decision period.</p> Source code in <code>neurogym/envs/delaycomparison.py</code> <pre><code>def __init__(self, dt=100, vpairs=None, rewards=None, timing=None, sigma=1.0) -&gt; None:\n    super().__init__(dt=dt)\n\n    # Pair of stimulus strengthes\n    if vpairs is None:\n        self.vpairs = [(18, 10), (22, 14), (26, 18), (30, 22), (34, 26)]\n    else:\n        self.vpairs = vpairs\n\n    self.sigma = sigma / np.sqrt(self.dt)  # Input noise\n\n    # Rewards\n    self.rewards = {\"abort\": -0.1, \"correct\": +1.0, \"fail\": 0.0}\n    if rewards:\n        self.rewards.update(rewards)\n\n    self.timing = {\n        \"fixation\": 500,\n        \"stimulus1\": 500,\n        \"delay\": 1000,\n        \"stimulus2\": 500,\n        \"decision\": 100,\n    }\n    if timing:\n        self.timing.update(timing)\n\n    self.abort = False\n\n    # Input scaling\n    self.vall = np.ravel(self.vpairs)\n    self.vmin = np.min(self.vall)\n    self.vmax = np.max(self.vall)\n\n    # action and observation space\n    name: dict[str, int | list] = {\"fixation\": 0, \"stimulus\": 1}\n    self.observation_space = spaces.Box(\n        -np.inf,\n        np.inf,\n        shape=(2,),\n        dtype=np.float32,\n        name=name,\n    )\n    name = {\"fixation\": 0, \"choice\": [1, 2]}\n    self.action_space = spaces.Discrete(3, name=name)\n\n    self.choices = [1, 2]\n</code></pre>"},{"location":"api/envs/#neurogym.envs.delaycomparison.DelayComparison.represent","title":"represent","text":"<pre><code>represent(v)\n</code></pre> <p>Input representation of stimulus value.</p> Source code in <code>neurogym/envs/delaycomparison.py</code> <pre><code>def represent(self, v):\n    \"\"\"Input representation of stimulus value.\"\"\"\n    # Scale to be between 0 and 1\n    v_ = (v - self.vmin) / (self.vmax - self.vmin)\n    # positive encoding, between 0.5 and 1\n    return (1 + v_) / 2\n</code></pre>"},{"location":"api/envs/#neurogym.envs.delaymatchcategory","title":"delaymatchcategory","text":""},{"location":"api/envs/#neurogym.envs.delaymatchcategory.DelayMatchCategory","title":"DelayMatchCategory","text":"<pre><code>DelayMatchCategory(dt=100, rewards=None, timing=None, sigma=1.0, dim_ring=2)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>Delayed match-to-category task.</p> <p>A sample stimulus is shown during the sample period. The stimulus is characterized by a one-dimensional variable, such as its orientation between 0 and 360 degree. This one-dimensional variable is separated into two categories (for example, 0-180 degree and 180-360 degree). After a delay period, a test stimulus is shown. The agent needs to determine whether the sample and the test stimuli belong to the same category, and report that decision during the decision period.</p> Source code in <code>neurogym/envs/delaymatchcategory.py</code> <pre><code>def __init__(self, dt=100, rewards=None, timing=None, sigma=1.0, dim_ring=2) -&gt; None:\n    super().__init__(dt=dt)\n    self.choices = [\"match\", \"non-match\"]  # match, non-match\n\n    self.sigma = sigma / np.sqrt(self.dt)  # Input noise\n\n    # Rewards\n    self.rewards = {\"abort\": -0.1, \"correct\": +1.0, \"fail\": 0.0}\n    if rewards:\n        self.rewards.update(rewards)\n\n    self.timing = {\"fixation\": 500, \"sample\": 650, \"first_delay\": 1000, \"test\": 650}\n\n    if timing:\n        self.timing.update(timing)\n\n    self.abort = False\n\n    self.theta = np.linspace(0, 2 * np.pi, dim_ring + 1)[:-1]\n\n    name = {\"fixation\": 0, \"stimulus\": range(1, dim_ring + 1)}\n    self.observation_space = spaces.Box(\n        -np.inf,\n        np.inf,\n        shape=(1 + dim_ring,),\n        dtype=np.float32,\n        name=name,\n    )\n\n    name = {\"fixation\": 0, \"match\": 1, \"non-match\": 2}\n    self.action_space = spaces.Discrete(3, name=name)\n</code></pre>"},{"location":"api/envs/#neurogym.envs.delaymatchsample","title":"delaymatchsample","text":""},{"location":"api/envs/#neurogym.envs.delaymatchsample.DelayMatchSample","title":"DelayMatchSample","text":"<pre><code>DelayMatchSample(dt=100, rewards=None, timing=None, sigma=1.0, dim_ring=2)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>Delayed match-to-sample task.</p> <p>A sample stimulus is shown during the sample period. The stimulus is characterized by a one-dimensional variable, such as its orientation between 0 and 360 degree. After a delay period, a test stimulus is shown. The agent needs to determine whether the sample and the test stimuli are equal, and report that decision during the decision period.</p> Source code in <code>neurogym/envs/delaymatchsample.py</code> <pre><code>def __init__(self, dt=100, rewards=None, timing=None, sigma=1.0, dim_ring=2) -&gt; None:\n    super().__init__(dt=dt)\n    self.choices = [1, 2]\n    self.sigma = sigma / np.sqrt(self.dt)  # Input noise\n\n    # Rewards\n    self.rewards = {\"abort\": -0.1, \"correct\": +1.0, \"fail\": 0.0}\n    if rewards:\n        self.rewards.update(rewards)\n\n    self.timing = {\n        \"fixation\": 300,\n        \"sample\": 500,\n        \"delay\": 1000,\n        \"test\": 500,\n        \"decision\": 900,\n    }\n    if timing:\n        self.timing.update(timing)\n\n    self.abort = False\n\n    self.theta = np.linspace(0, 2 * np.pi, dim_ring + 1)[:-1]\n\n    name = {\"fixation\": 0, \"stimulus\": range(1, dim_ring + 1)}\n    self.observation_space = spaces.Box(\n        -np.inf,\n        np.inf,\n        shape=(1 + dim_ring,),\n        dtype=np.float32,\n        name=name,\n    )\n\n    name = {\"fixation\": 0, \"match\": 1, \"non-match\": 2}\n    self.action_space = spaces.Discrete(3, name=name)\n</code></pre>"},{"location":"api/envs/#neurogym.envs.delaymatchsample.DelayMatchSampleDistractor1D","title":"DelayMatchSampleDistractor1D","text":"<pre><code>DelayMatchSampleDistractor1D(dt=100, rewards=None, timing=None, sigma=1.0)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>Delayed match-to-sample with multiple, potentially repeating distractors.</p> <p>A sample stimulus is shown during the sample period. The stimulus is characterized by a one-dimensional variable, such as its orientation between 0 and 360 degree. After a delay period, the first test stimulus is shown. The agent needs to determine whether the sample and this test stimuli are equal. If so, it needs to produce the match response. If the first test is not equal to the sample stimulus, another delay period and then a second test stimulus follow, and so on.</p> Source code in <code>neurogym/envs/delaymatchsample.py</code> <pre><code>def __init__(self, dt=100, rewards=None, timing=None, sigma=1.0) -&gt; None:\n    super().__init__(dt=dt)\n    self.choices = [1, 2, 3]\n    self.sigma = sigma / np.sqrt(self.dt)  # Input noise\n\n    # Rewards\n    self.rewards = {\"abort\": -0.1, \"correct\": +1.0, \"fail\": -1.0}\n    if rewards:\n        self.rewards.update(rewards)\n\n    self.timing = {\n        \"fixation\": 300,\n        \"sample\": 500,\n        \"delay1\": 1000,\n        \"test1\": 500,\n        \"delay2\": 1000,\n        \"test2\": 500,\n        \"delay3\": 1000,\n        \"test3\": 500,\n    }\n    if timing:\n        self.timing.update(timing)\n\n    self.abort = False\n\n    self.theta = np.arange(0, 2 * np.pi, 2 * np.pi / 32)\n\n    name = {\"fixation\": 0, \"stimulus\": range(1, 33)}\n    self.observation_space = spaces.Box(\n        -np.inf,\n        np.inf,\n        shape=(33,),\n        dtype=np.float32,\n        name=name,\n    )\n\n    name = {\"fixation\": 0, \"match\": 1}\n    self.action_space = spaces.Discrete(2, name=name)\n</code></pre>"},{"location":"api/envs/#neurogym.envs.delaypairedassociation","title":"delaypairedassociation","text":""},{"location":"api/envs/#neurogym.envs.delaypairedassociation.DelayPairedAssociation","title":"DelayPairedAssociation","text":"<pre><code>DelayPairedAssociation(dt=100, rewards=None, timing=None, sigma=1.0)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>Delayed paired-association task.</p> <p>The agent is shown a pair of two stimuli separated by a delay period. For half of the stimuli-pairs shown, the agent should choose the Go response. The agent is rewarded if it chose the Go response correctly.</p> Source code in <code>neurogym/envs/delaypairedassociation.py</code> <pre><code>def __init__(self, dt=100, rewards=None, timing=None, sigma=1.0) -&gt; None:\n    super().__init__(dt=dt)\n    self.choices = [0, 1]\n    # trial conditions\n    self.pairs = [(1, 3), (1, 4), (2, 3), (2, 4)]\n    self.association = 0  # GO if np.diff(self.pair)[0]%2==self.association\n    self.sigma = sigma / np.sqrt(self.dt)  # Input noise\n    # Durations (stimulus duration will be drawn from an exponential)\n\n    # Rewards\n    self.rewards = {\"abort\": -0.1, \"correct\": +1.0, \"fail\": -1.0, \"miss\": 0.0}\n    if rewards:\n        self.rewards.update(rewards)\n\n    self.timing = {\n        \"fixation\": 0,\n        \"stim1\": 1000,\n        \"delay_btw_stim\": 1000,\n        \"stim2\": 1000,\n        \"delay_aft_stim\": 1000,\n        \"decision\": 500,\n    }\n    if timing:\n        self.timing.update(timing)\n\n    self.abort = False\n    # action and observation spaces\n    name = {\"fixation\": 0, \"stimulus\": range(1, 5)}\n    self.observation_space = spaces.Box(\n        -np.inf,\n        np.inf,\n        shape=(5,),\n        dtype=np.float32,\n        name=name,\n    )\n\n    self.action_space = spaces.Discrete(2, name={\"fixation\": 0, \"go\": 1})\n</code></pre>"},{"location":"api/envs/#neurogym.envs.detection","title":"detection","text":"<p>Created on Mon Jan 27 11:00:26 2020.</p> <p>@author: martafradera</p>"},{"location":"api/envs/#neurogym.envs.detection.Detection","title":"Detection","text":"<pre><code>Detection(dt=100, rewards=None, timing=None, sigma=1.0, delay=None, stim_dur=100)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>The agent has to GO if a stimulus is presented.</p> <p>Parameters:</p> Name Type Description Default <code>delay</code> <p>If not None indicates the delay, from the moment of the start of the stimulus period when the actual stimulus is presented. Otherwise, the delay is drawn from a uniform distribution. (def: None (ms), int)</p> <code>None</code> <code>stim_dur</code> <p>Stimulus duration. (def: 100 (ms), int)</p> <code>100</code> Source code in <code>neurogym/envs/detection.py</code> <pre><code>def __init__(\n    self,\n    dt=100,\n    rewards=None,\n    timing=None,\n    sigma=1.0,\n    delay=None,\n    stim_dur=100,\n) -&gt; None:\n    super().__init__(dt=dt)\n    # Possible decisions at the end of the trial\n    self.choices = [0, 1]\n\n    self.sigma = sigma / np.sqrt(self.dt)  # Input noise\n    self.delay = delay\n    self.stim_dur = int(stim_dur / self.dt)  # in steps should be greater\n    # than 1 stp else it wont have enough time to respond within the window\n    if self.stim_dur == 1:\n        self.extra_step = 1\n        if delay is None:\n            warnings.warn(\n                \"Added an extra stp after the actual stimulus, else model will not be able to respond \"\n                \"within response window (stimulus epoch).\",\n                UserWarning,\n                stacklevel=2,\n            )\n    else:\n        self.extra_step = 0\n\n    if self.stim_dur &lt; 1:\n        warnings.warn(\"Stimulus duration shorter than dt\", stacklevel=2)\n\n    # Rewards\n    self.rewards = {\"abort\": -0.1, \"correct\": +1.0, \"fail\": -1.0, \"miss\": -1}\n    if rewards:\n        self.rewards.update(rewards)\n\n    self.timing = {\n        \"fixation\": 500,\n        \"stimulus\": ngym.random.TruncExp(1000, 500, 1500),\n    }\n    if timing:\n        self.timing.update(timing)\n\n    # whether to abort (T) or not (F) the trial when breaking fixation:\n    self.abort = False\n\n    name = {\"fixation\": 0, \"stimulus\": 1}\n    self.observation_space = spaces.Box(\n        -np.inf,\n        np.inf,\n        shape=(2,),\n        dtype=np.float32,\n        name=name,\n    )\n\n    self.action_space = spaces.Discrete(2, name={\"fixation\": 0, \"go\": 1})\n</code></pre>"},{"location":"api/envs/#neurogym.envs.dualdelaymatchsample","title":"dualdelaymatchsample","text":""},{"location":"api/envs/#neurogym.envs.dualdelaymatchsample.DualDelayMatchSample","title":"DualDelayMatchSample","text":"<pre><code>DualDelayMatchSample(dt=100, rewards=None, timing=None, sigma=1.0)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>Two-item Delay-match-to-sample.</p> <p>The trial starts with a fixation period. Then during the sample period, two sample stimuli are shown simultaneously. Followed by the first delay period, a cue is shown, indicating which sample stimulus will be tested. Then the first test stimulus is shown and the agent needs to report whether this test stimulus matches the cued sample stimulus. Then another delay and then test period follows, and the agent needs to report whether the other sample stimulus matches the second test stimulus.</p> Source code in <code>neurogym/envs/dualdelaymatchsample.py</code> <pre><code>def __init__(self, dt=100, rewards=None, timing=None, sigma=1.0) -&gt; None:\n    super().__init__(dt=dt)\n    self.choices = [1, 2]\n    self.cues = [0, 1]\n\n    self.sigma = sigma / np.sqrt(self.dt)  # Input noise\n\n    # Rewards\n    self.rewards = {\"abort\": -0.1, \"correct\": +1.0, \"fail\": 0.0}\n    if rewards:\n        self.rewards.update(rewards)\n\n    self.timing = {\n        \"fixation\": 500,\n        \"sample\": 500,\n        \"delay1\": 500,\n        \"cue1\": 500,\n        \"test1\": 500,\n        \"delay2\": 500,\n        \"cue2\": 500,\n        \"test2\": 500,\n    }\n    if timing:\n        self.timing.update(timing)\n\n    self.abort = False\n\n    name = {\n        \"fixation\": 0,\n        \"stimulus1\": range(1, 3),\n        \"stimulus2\": range(3, 5),\n        \"cue1\": 5,\n        \"cue2\": 6,\n    }\n    self.observation_space = spaces.Box(\n        -np.inf,\n        np.inf,\n        shape=(7,),\n        dtype=np.float32,\n        name=name,\n    )\n    name = {\"fixation\": 0, \"match\": 1, \"non-match\": 2}\n    self.action_space = spaces.Discrete(3, name=name)\n</code></pre>"},{"location":"api/envs/#neurogym.envs.economicdecisionmaking","title":"economicdecisionmaking","text":""},{"location":"api/envs/#neurogym.envs.economicdecisionmaking.EconomicDecisionMaking","title":"EconomicDecisionMaking","text":"<pre><code>EconomicDecisionMaking(dt=100, rewards=None, timing=None)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>Economic decision making task.</p> <p>A agent chooses between two options. Each option offers a certain amount of juice. Its amount is indicated by the stimulus. The two options offer different types of juice, and the agent prefers one over another.</p> Source code in <code>neurogym/envs/economicdecisionmaking.py</code> <pre><code>def __init__(self, dt=100, rewards=None, timing=None) -&gt; None:\n    super().__init__(dt=dt)\n\n    # trial conditions\n    self.B_to_A = 1 / 2.2\n    self.juices = [(\"a\", \"b\"), (\"b\", \"a\")]\n    self.offers = [\n        (0, 1),\n        (1, 3),\n        (1, 2),\n        (1, 1),\n        (2, 1),\n        (3, 1),\n        (4, 1),\n        (6, 1),\n        (2, 0),\n    ]\n\n    # Rewards\n    self.rewards = {\"abort\": -0.1, \"correct\": +0.22}\n    if rewards:\n        self.rewards.update(rewards)\n\n    self.timing = {\n        \"fixation\": 1500,\n        \"offer_on\": lambda: self.rng.uniform(1000, 2000),\n        \"decision\": 750,\n    }\n    if timing:\n        self.timing.update(timing)\n\n    self.R_B = self.B_to_A * self.rewards[\"correct\"]\n    self.R_A = self.rewards[\"correct\"]\n    self.abort = False\n    # Increase initial policy -&gt; baseline weights\n    self.baseline_Win = 10\n\n    name = {\n        \"fixation\": 0,\n        \"a1\": 1,\n        \"b1\": 2,  # a or b for choice 1\n        \"a2\": 3,\n        \"b2\": 4,  # a or b for choice 2\n        \"n1\": 5,\n        \"n2\": 6,  # amount for choice 1 or 2\n    }\n    self.observation_space = spaces.Box(\n        -np.inf,\n        np.inf,\n        shape=(7,),\n        dtype=np.float32,\n        name=name,\n    )\n\n    self.act_dict = {\"fixation\": 0, \"choice1\": 1, \"choice2\": 2}\n    self.action_space = spaces.Discrete(3, name=self.act_dict)\n</code></pre>"},{"location":"api/envs/#neurogym.envs.gonogo","title":"gonogo","text":""},{"location":"api/envs/#neurogym.envs.gonogo.GoNogo","title":"GoNogo","text":"<pre><code>GoNogo(dt=100, rewards=None, timing=None)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>Go/No-go task.</p> <p>A stimulus is shown during the stimulus period. The stimulus period is followed by a delay period, and then a decision period. If the stimulus is a Go stimulus, then the subject should choose the action Go during the decision period, otherwise, the subject should remain fixation.</p> Source code in <code>neurogym/envs/gonogo.py</code> <pre><code>def __init__(self, dt=100, rewards=None, timing=None) -&gt; None:\n    super().__init__(dt=dt)\n    # Actions are (FIXATE, GO)\n    self.actions = [0, 1]\n    # trial conditions\n    self.choices = [0, 1]\n\n    # Rewards\n    self.rewards = {\"abort\": -0.1, \"correct\": +1.0, \"fail\": -0.5, \"miss\": -0.5}\n    if rewards:\n        self.rewards.update(rewards)\n\n    self.timing = {\"fixation\": 0, \"stimulus\": 500, \"delay\": 500, \"decision\": 500}\n    if timing:\n        self.timing.update(timing)\n\n    self.abort = False\n    # set action and observation spaces\n    name = {\"fixation\": 0, \"nogo\": 1, \"go\": 2}\n    self.observation_space = spaces.Box(\n        -np.inf,\n        np.inf,\n        shape=(3,),\n        dtype=np.float32,\n        name=name,\n    )\n    self.action_space = spaces.Discrete(2, {\"fixation\": 0, \"go\": 1})\n</code></pre>"},{"location":"api/envs/#neurogym.envs.hierarchicalreasoning","title":"hierarchicalreasoning","text":"<p>Hierarchical reasoning tasks.</p>"},{"location":"api/envs/#neurogym.envs.hierarchicalreasoning.HierarchicalReasoning","title":"HierarchicalReasoning","text":"<pre><code>HierarchicalReasoning(dt=100, rewards=None, timing=None)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>Hierarchical reasoning of rules.</p> <p>On each trial, the subject receives two flashes separated by a delay period. The subject needs to judge whether the duration of this delay period is shorter than a threshold. Both flashes appear at the same location on each trial. For one trial type, the network should report its decision by going to the location of the flashes if the delay is shorter than the threshold. In another trial type, the network should go to the opposite direction of the flashes if the delay is short. The two types of trials are alternated across blocks, and the block transtion is unannouced.</p> Source code in <code>neurogym/envs/hierarchicalreasoning.py</code> <pre><code>def __init__(self, dt=100, rewards=None, timing=None) -&gt; None:\n    super().__init__(dt=dt)\n    self.choices = [0, 1]\n\n    self.rewards = {\"abort\": -0.1, \"correct\": +1.0, \"fail\": 0.0}\n    if rewards:\n        self.rewards.update(rewards)\n\n    self.timing = {\n        \"fixation\": ngym.random.TruncExp(600, 400, 800),\n        \"rule_target\": 1000,\n        \"fixation2\": ngym.random.TruncExp(600, 400, 900),\n        \"flash1\": 100,\n        \"delay\": (530, 610, 690, 770, 850, 930, 1010, 1090, 1170),\n        \"flash2\": 100,\n        \"decision\": 700,\n    }\n    if timing:\n        self.timing.update(timing)\n    self.mid_delay = np.median(self.timing[\"delay\"][1])\n\n    self.abort = False\n\n    name = {\"fixation\": 0, \"rule\": [1, 2], \"stimulus\": [3, 4]}\n    self.observation_space = spaces.Box(\n        -np.inf,\n        np.inf,\n        shape=(5,),\n        dtype=np.float32,\n        name=name,\n    )\n    name = {\"fixation\": 0, \"rule\": [1, 2], \"choice\": [3, 4]}\n    self.action_space = spaces.Discrete(5, name=name)\n\n    self.chose_correct_rule = False\n    self.rule = 0\n    self.trial_in_block = 0\n    self.block_size = 10\n    self.new_block()\n</code></pre>"},{"location":"api/envs/#neurogym.envs.intervaldiscrimination","title":"intervaldiscrimination","text":""},{"location":"api/envs/#neurogym.envs.intervaldiscrimination.IntervalDiscrimination","title":"IntervalDiscrimination","text":"<pre><code>IntervalDiscrimination(dt=80, rewards=None, timing=None)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>Comparing the time length of two stimuli.</p> <p>Two stimuli are shown sequentially, separated by a delay period. The duration of each stimulus is randomly sampled on each trial. The subject needs to judge which stimulus has a longer duration, and reports its decision during the decision period by choosing one of the two choice options.</p> Source code in <code>neurogym/envs/intervaldiscrimination.py</code> <pre><code>def __init__(self, dt=80, rewards=None, timing=None) -&gt; None:\n    super().__init__(dt=dt)\n    # Rewards\n    self.rewards = {\"abort\": -0.1, \"correct\": +1.0, \"fail\": 0.0}\n    if rewards:\n        self.rewards.update(rewards)\n\n    self.timing = {\n        \"fixation\": 300,\n        \"stim1\": lambda: self.rng.uniform(300, 600),\n        \"delay1\": lambda: self.rng.uniform(800, 1500),\n        \"stim2\": lambda: self.rng.uniform(300, 600),\n        \"delay2\": 500,\n        \"decision\": 300,\n    }\n    if timing:\n        self.timing.update(timing)\n\n    self.abort = False\n\n    name = {\"fixation\": 0, \"stim1\": 1, \"stim2\": 2}\n    self.observation_space = spaces.Box(\n        -np.inf,\n        np.inf,\n        shape=(3,),\n        dtype=np.float32,\n        name=name,\n    )\n    name = {\"fixation\": 0, \"choice1\": 1, \"choice2\": 2}\n    self.action_space = spaces.Discrete(3, name=name)\n</code></pre>"},{"location":"api/envs/#neurogym.envs.multisensory","title":"multisensory","text":"<p>Multi-Sensory Integration.</p>"},{"location":"api/envs/#neurogym.envs.multisensory.MultiSensoryIntegration","title":"MultiSensoryIntegration","text":"<pre><code>MultiSensoryIntegration(dt=100, rewards=None, timing=None, sigma=1.0, dim_ring=2)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>Multi-sensory integration.</p> <p>Two stimuli are shown in two input modalities. Each stimulus points to one of the possible responses with a certain strength (coherence). The correct choice is the response with the highest summed strength from both stimuli. The agent is therefore encouraged to integrate information from both modalities equally.</p> Source code in <code>neurogym/envs/multisensory.py</code> <pre><code>def __init__(self, dt=100, rewards=None, timing=None, sigma=1.0, dim_ring=2) -&gt; None:\n    super().__init__(dt=dt)\n\n    # trial conditions\n    self.cohs = [5, 15, 50]\n\n    self.sigma = sigma / np.sqrt(self.dt)  # Input noise\n\n    # Rewards\n    self.rewards = {\"abort\": -0.1, \"correct\": +1.0}\n    if rewards:\n        self.rewards.update(rewards)\n\n    self.timing = {\"fixation\": 300, \"stimulus\": 750, \"decision\": 100}\n    if timing:\n        self.timing.update(timing)\n    self.abort = False\n\n    # set action and observation space\n    self.theta = np.linspace(0, 2 * np.pi, dim_ring + 1)[:-1]\n    self.choices = np.arange(dim_ring)\n\n    name = {\n        \"fixation\": 0,\n        \"stimulus_mod1\": range(1, dim_ring + 1),\n        \"stimulus_mod2\": range(dim_ring + 1, 2 * dim_ring + 1),\n    }\n    self.observation_space = spaces.Box(\n        -np.inf,\n        np.inf,\n        shape=(1 + 2 * dim_ring,),\n        dtype=np.float32,\n        name=name,\n    )\n\n    name = {\"fixation\": 0, \"choice\": range(1, dim_ring + 1)}\n    self.action_space = spaces.Discrete(1 + dim_ring, name=name)\n</code></pre>"},{"location":"api/envs/#neurogym.envs.null","title":"null","text":""},{"location":"api/envs/#neurogym.envs.null.Null","title":"Null","text":"<pre><code>Null(dt=100)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>Null task.</p> Source code in <code>neurogym/envs/null.py</code> <pre><code>def __init__(self, dt=100) -&gt; None:\n    super().__init__(dt=dt)\n    self.action_space = spaces.Discrete(1)\n    self.observation_space = spaces.Box(\n        -np.inf,\n        np.inf,\n        shape=(1,),\n        dtype=np.float32,\n    )\n</code></pre>"},{"location":"api/envs/#neurogym.envs.perceptualdecisionmaking","title":"perceptualdecisionmaking","text":""},{"location":"api/envs/#neurogym.envs.perceptualdecisionmaking.PerceptualDecisionMaking","title":"PerceptualDecisionMaking","text":"<pre><code>PerceptualDecisionMaking(dt=100, rewards=None, timing=None, cohs=None, sigma=1.0, dim_ring=2)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>Two-alternative forced choice task: subject has to integrate two stimuli to decide which is higher on average.</p> <p>A noisy stimulus is shown during the stimulus period. The strength ( coherence) of the stimulus is randomly sampled every trial. Because the stimulus is noisy, the agent is encouraged to integrate the stimulus over time.</p> <p>Parameters:</p> Name Type Description Default <code>cohs</code> <p>list of float, coherence levels controlling the difficulty of the task</p> <code>None</code> <code>sigma</code> <p>float, input noise level</p> <code>1.0</code> <code>dim_ring</code> <p>int, dimension of ring input and output</p> <code>2</code> Source code in <code>neurogym/envs/perceptualdecisionmaking.py</code> <pre><code>def __init__(\n    self,\n    dt=100,\n    rewards=None,\n    timing=None,\n    cohs=None,\n    sigma=1.0,\n    dim_ring=2,\n) -&gt; None:\n    super().__init__(dt=dt)\n    if cohs is None:\n        self.cohs = np.array([0, 6.4, 12.8, 25.6, 51.2])\n    else:\n        self.cohs = cohs\n    self.sigma = sigma / np.sqrt(self.dt)  # Input noise\n\n    # Rewards\n    self.rewards = {\"abort\": -0.1, \"correct\": +1.0, \"fail\": 0.0}\n    if rewards:\n        self.rewards.update(rewards)\n\n    self.timing = {\"fixation\": 100, \"stimulus\": 2000, \"delay\": 0, \"decision\": 100}\n    if timing:\n        self.timing.update(timing)\n\n    self.abort = False\n\n    self.theta = np.linspace(0, 2 * np.pi, dim_ring + 1)[:-1]\n    self.choices = np.arange(dim_ring)\n\n    name = {\"fixation\": 0, \"stimulus\": range(1, dim_ring + 1)}\n    self.observation_space = spaces.Box(\n        -np.inf,\n        np.inf,\n        shape=(1 + dim_ring,),\n        dtype=np.float32,\n        name=name,\n    )\n    name = {\"fixation\": 0, \"choice\": range(1, dim_ring + 1)}\n    self.action_space = spaces.Discrete(1 + dim_ring, name=name)\n</code></pre>"},{"location":"api/envs/#neurogym.envs.perceptualdecisionmaking.PerceptualDecisionMakingDelayResponse","title":"PerceptualDecisionMakingDelayResponse","text":"<pre><code>PerceptualDecisionMakingDelayResponse(dt=100, rewards=None, timing=None, stim_scale=1.0, sigma=1.0)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>Perceptual decision-making with delayed responses.</p> <p>Agents have to integrate two stimuli and report which one is larger on average after a delay.</p> <p>Parameters:</p> Name Type Description Default <code>stim_scale</code> <p>Controls the difficulty of the experiment. (def: 1., float)</p> <code>1.0</code> Source code in <code>neurogym/envs/perceptualdecisionmaking.py</code> <pre><code>def __init__(self, dt=100, rewards=None, timing=None, stim_scale=1.0, sigma=1.0) -&gt; None:\n    super().__init__(dt=dt)\n    self.choices = [1, 2]\n    # cohs specifies the amount of evidence (modulated by stim_scale)\n    self.cohs = np.array([0, 6.4, 12.8, 25.6, 51.2]) * stim_scale\n    self.sigma = sigma / np.sqrt(self.dt)  # Input noise\n\n    # Rewards\n    self.rewards = {\"abort\": -0.1, \"correct\": +1.0, \"fail\": 0.0}\n    if rewards:\n        self.rewards.update(rewards)\n\n    self.timing = {\n        \"fixation\": 0,\n        \"stimulus\": 1150,\n        #  TODO: sampling of delays follows exponential\n        \"delay\": (300, 500, 700, 900, 1200, 2000, 3200, 4000),\n        # 'go_cue': 100,  # noqa: ERA001 TODO: Not implemented\n        \"decision\": 1500,\n    }\n    if timing:\n        self.timing.update(timing)\n\n    self.abort = False\n\n    # action and observation spaces\n    self.action_space = spaces.Discrete(3)\n    self.observation_space = spaces.Box(\n        -np.inf,\n        np.inf,\n        shape=(3,),\n        dtype=np.float32,\n    )\n</code></pre>"},{"location":"api/envs/#neurogym.envs.perceptualdecisionmaking.PulseDecisionMaking","title":"PulseDecisionMaking","text":"<pre><code>PulseDecisionMaking(dt=10, rewards=None, timing=None, p_pulse=(0.3, 0.7), n_bin=6)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>Pulse-based decision making task.</p> <p>Discrete stimuli are presented briefly as pulses.</p> <p>Parameters:</p> Name Type Description Default <code>p_pulse</code> <p>array-like, probability of pulses for each choice</p> <code>(0.3, 0.7)</code> <code>n_bin</code> <p>int, number of stimulus bins</p> <code>6</code> Source code in <code>neurogym/envs/perceptualdecisionmaking.py</code> <pre><code>def __init__(self, dt=10, rewards=None, timing=None, p_pulse=(0.3, 0.7), n_bin=6) -&gt; None:\n    super().__init__(dt=dt)\n    self.p_pulse = p_pulse\n    self.n_bin = n_bin\n\n    # Rewards\n    self.rewards = {\"abort\": -0.1, \"correct\": +1.0, \"fail\": 0.0}\n    if rewards:\n        self.rewards.update(rewards)\n\n    self.timing = {\"fixation\": 500, \"decision\": 500}\n    for i in range(n_bin):\n        self.timing[f\"cue{i}\"] = 10\n        self.timing[f\"bin{i}\"] = 240\n    if timing:\n        self.timing.update(timing)\n\n    self.abort = False\n\n    name = {\"fixation\": 0, \"stimulus\": [1, 2]}\n    self.observation_space = spaces.Box(\n        -np.inf,\n        np.inf,\n        shape=(3,),\n        dtype=np.float32,\n        name=name,\n    )\n    name = {\"fixation\": 0, \"choice\": [1, 2]}\n    self.action_space = spaces.Discrete(3, name=name)\n</code></pre>"},{"location":"api/envs/#neurogym.envs.postdecisionwager","title":"postdecisionwager","text":""},{"location":"api/envs/#neurogym.envs.postdecisionwager.PostDecisionWager","title":"PostDecisionWager","text":"<pre><code>PostDecisionWager(dt=100, rewards=None, timing=None, dim_ring=2, sigma=1.0)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>Post-decision wagering task assessing confidence.</p> <p>The agent first performs a perceptual discrimination task (see for more details the PerceptualDecisionMaking task). On a random half of the trials, the agent is given the option to abort the sensory discrimination and to choose instead a sure-bet option that guarantees a small reward. Therefore, the agent is encouraged to choose the sure-bet option when it is uncertain about its perceptual decision.</p> Source code in <code>neurogym/envs/postdecisionwager.py</code> <pre><code>def __init__(self, dt=100, rewards=None, timing=None, dim_ring=2, sigma=1.0) -&gt; None:\n    super().__init__(dt=dt)\n\n    self.wagers = [True, False]\n    self.theta = np.linspace(0, 2 * np.pi, dim_ring + 1)[:-1]\n    self.choices = np.arange(dim_ring)\n    self.cohs = [0, 3.2, 6.4, 12.8, 25.6, 51.2]\n    self.sigma = sigma / np.sqrt(self.dt)  # Input noise\n\n    # Rewards\n    self.rewards = {\"abort\": -0.1, \"correct\": +1.0, \"fail\": 0.0}\n    if rewards:\n        self.rewards.update(rewards)\n    self.rewards[\"sure\"] = 0.7 * self.rewards[\"correct\"]\n\n    self.timing = {\n        \"fixation\": 100,\n        # 'target':  0,  # noqa: ERA001\n        \"stimulus\": ngym.random.TruncExp(180, 100, 900),\n        \"delay\": ngym.random.TruncExp(1350, 1200, 1800),\n        \"pre_sure\": lambda: self.rng.uniform(500, 750),\n        \"decision\": 100,\n    }\n    if timing:\n        self.timing.update(timing)\n\n    self.abort = False\n\n    # set action and observation space\n    name = {\"fixation\": 0, \"stimulus\": [1, 2], \"sure\": 3}\n    self.observation_space = spaces.Box(\n        -np.inf,\n        np.inf,\n        shape=(4,),\n        dtype=np.float32,\n        name=name,\n    )\n    name = {\"fixation\": 0, \"choice\": [1, 2], \"sure\": 3}\n    self.action_space = spaces.Discrete(4, name=name)\n</code></pre>"},{"location":"api/envs/#neurogym.envs.probabilisticreasoning","title":"probabilisticreasoning","text":"<p>Random dot motion task.</p>"},{"location":"api/envs/#neurogym.envs.probabilisticreasoning.ProbabilisticReasoning","title":"ProbabilisticReasoning","text":"<pre><code>ProbabilisticReasoning(dt=100, rewards=None, timing=None, shape_weight=None, n_loc=4)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>Probabilistic reasoning.</p> <p>The agent is shown a sequence of stimuli. Each stimulus is associated with a certain log-likelihood of the correct response being one choice versus the other. The final log-likelihood of the target response being, for example, option 1, is the sum of all log-likelihood associated with the presented stimuli. A delay period separates each stimulus, so the agent is encouraged to lean the log-likelihood association and integrate these values over time within a trial.</p> <p>Parameters:</p> Name Type Description Default <code>shape_weight</code> <p>array-like, evidence weight of each shape</p> <code>None</code> <code>n_loc</code> <p>int, number of location of show shapes</p> <code>4</code> Source code in <code>neurogym/envs/probabilisticreasoning.py</code> <pre><code>def __init__(self, dt=100, rewards=None, timing=None, shape_weight=None, n_loc=4) -&gt; None:\n    super().__init__(dt=dt)\n    # The evidence weight of each stimulus\n    if shape_weight is not None:\n        self.shape_weight = shape_weight\n    else:\n        self.shape_weight = [-10, -0.9, -0.7, -0.5, -0.3, 0.3, 0.5, 0.7, 0.9, 10]\n\n    self.n_shape = len(self.shape_weight)\n    dim_shape = self.n_shape\n    # Shape representation needs to be fixed cross-platform\n    self.shapes = np.eye(self.n_shape, dim_shape)\n    self.n_loc = n_loc\n\n    # Rewards\n    self.rewards = {\"abort\": -0.1, \"correct\": +1.0, \"fail\": 0.0}\n    if rewards:\n        self.rewards.update(rewards)\n\n    self.timing = {\n        \"fixation\": 500,\n        \"delay\": lambda: self.rng.uniform(450, 550),\n        \"decision\": 500,\n    }\n    for i_loc in range(n_loc):\n        self.timing[f\"stimulus{i_loc}\"] = 500\n    if timing:\n        self.timing.update(timing)\n\n    self.abort = False\n\n    obs_name: dict[str, int | range] = {\"fixation\": 0}\n    start = 1\n    for i_loc in range(n_loc):\n        obs_name[f\"loc{i_loc}\"] = range(start, start + dim_shape)\n        start += dim_shape\n    self.observation_space = spaces.Box(\n        -np.inf,\n        np.inf,\n        shape=(1 + dim_shape * n_loc,),\n        dtype=np.float32,\n        name=obs_name,\n    )\n\n    action_name = {\"fixation\": 0, \"choice\": [1, 2]}\n    self.action_space = spaces.Discrete(3, name=action_name)\n</code></pre>"},{"location":"api/envs/#neurogym.envs.reaching","title":"reaching","text":"<p>Reaching to target.</p>"},{"location":"api/envs/#neurogym.envs.reaching.Reaching1D","title":"Reaching1D","text":"<pre><code>Reaching1D(dt=100, rewards=None, timing=None, dim_ring=16)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>Reaching to the stimulus.</p> <p>The agent is shown a stimulus during the fixation period. The stimulus encodes a one-dimensional variable such as a movement direction. At the end of the fixation period, the agent needs to respond by reaching towards the stimulus direction.</p> Source code in <code>neurogym/envs/reaching.py</code> <pre><code>def __init__(self, dt=100, rewards=None, timing=None, dim_ring=16) -&gt; None:\n    super().__init__(dt=dt)\n    # Rewards\n    self.rewards = {\"correct\": +1.0, \"fail\": -0.1}\n    if rewards:\n        self.rewards.update(rewards)\n\n    self.timing = {\"fixation\": 500, \"reach\": 500}\n    if timing:\n        self.timing.update(timing)\n\n    # action and observation spaces\n    obs_name = {\"self\": range(dim_ring, 2 * dim_ring), \"target\": range(dim_ring)}\n    self.observation_space = spaces.Box(\n        -np.inf,\n        np.inf,\n        shape=(2 * dim_ring,),\n        dtype=np.float32,\n        name=obs_name,\n    )\n    action_name = {\"fixation\": 0, \"left\": 1, \"right\": 2}\n    self.action_space = spaces.Discrete(3, name=action_name)\n\n    self.theta = np.arange(0, 2 * np.pi, 2 * np.pi / dim_ring)\n    self.state = np.pi\n    self.dim_ring = dim_ring\n</code></pre>"},{"location":"api/envs/#neurogym.envs.reaching.Reaching1D.post_step","title":"post_step","text":"<pre><code>post_step(ob, reward, terminated, truncated, info)\n</code></pre> <p>Modify observation.</p> Source code in <code>neurogym/envs/reaching.py</code> <pre><code>def post_step(self, ob, reward, terminated, truncated, info):\n    \"\"\"Modify observation.\"\"\"\n    ob[self.dim_ring :] = np.cos(self.theta - self.state)\n    return ob, reward, terminated, truncated, info\n</code></pre>"},{"location":"api/envs/#neurogym.envs.reaching.Reaching1DWithSelfDistraction","title":"Reaching1DWithSelfDistraction","text":"<pre><code>Reaching1DWithSelfDistraction(dt=100, rewards=None, timing=None)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>Reaching with self distraction.</p> <p>In this task, the reaching state itself generates strong inputs that overshadows the actual target input. This task is inspired by behavior in electric fish where the electric sensing organ is distracted by discharges from its own electric organ for active sensing. Similar phenomena in bats.</p> Source code in <code>neurogym/envs/reaching.py</code> <pre><code>def __init__(self, dt=100, rewards=None, timing=None) -&gt; None:\n    super().__init__(dt=dt)\n    # Rewards\n    self.rewards = {\"correct\": +1.0, \"fail\": -0.1}\n    if rewards:\n        self.rewards.update(rewards)\n\n    self.timing = {\"fixation\": 500, \"reach\": 500}\n    if timing:\n        self.timing.update(timing)\n\n    # action and observation spaces\n    self.action_space = spaces.Discrete(3)\n    self.observation_space = spaces.Box(\n        -np.inf,\n        np.inf,\n        shape=(32,),\n        dtype=np.float32,\n    )\n    self.theta = np.arange(0, 2 * np.pi, 2 * np.pi / 32)\n    self.state = np.pi\n</code></pre>"},{"location":"api/envs/#neurogym.envs.reaching.Reaching1DWithSelfDistraction.post_step","title":"post_step","text":"<pre><code>post_step(ob, reward, terminated, truncated, info)\n</code></pre> <p>Modify observation.</p> Source code in <code>neurogym/envs/reaching.py</code> <pre><code>def post_step(self, ob, reward, terminated, truncated, info):\n    \"\"\"Modify observation.\"\"\"\n    ob += np.cos(self.theta - self.state)\n    return ob, reward, terminated, truncated, info\n</code></pre>"},{"location":"api/envs/#neurogym.envs.reachingdelayresponse","title":"reachingdelayresponse","text":""},{"location":"api/envs/#neurogym.envs.reachingdelayresponse.ReachingDelayResponse","title":"ReachingDelayResponse","text":"<pre><code>ReachingDelayResponse(dt=100, rewards=None, timing=None, lowbound=0.0, highbound=1.0)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>Reaching task with a delay period.</p> <p>A reaching direction is presented by the stimulus during the stimulus period. Followed by a delay period, the agent needs to respond to the direction of the stimulus during the decision period.</p> Source code in <code>neurogym/envs/reachingdelayresponse.py</code> <pre><code>def __init__(self, dt=100, rewards=None, timing=None, lowbound=0.0, highbound=1.0) -&gt; None:\n    super().__init__(dt=dt)\n    self.lowbound = lowbound\n    self.highbound = highbound\n\n    # Rewards\n    self.rewards = {\"abort\": -0.1, \"correct\": +1.0, \"fail\": -0.0, \"miss\": -0.5}\n    if rewards:\n        self.rewards.update(rewards)\n\n    self.timing = {\"stimulus\": 500, \"delay\": (0, 1000, 2000), \"decision\": 500}\n    if timing:\n        self.timing.update(timing)\n\n    self.r_tmax = self.rewards[\"miss\"]\n    self.abort = False\n\n    name = {\"go\": 0, \"stimulus\": 1}\n    self.observation_space = spaces.Box(\n        low=np.array([0.0, -2]),\n        high=np.array([1, 2.0]),\n        dtype=np.float32,\n        name=name,\n    )\n\n    self.action_space = spaces.Box(\n        low=np.array((-1.0, -1.0)),\n        high=np.array((1.0, 2.0)),\n        dtype=np.float32,\n    )\n</code></pre>"},{"location":"api/envs/#neurogym.envs.readysetgo","title":"readysetgo","text":"<p>Ready-set-go task.</p>"},{"location":"api/envs/#neurogym.envs.readysetgo.ReadySetGo","title":"ReadySetGo","text":"<pre><code>ReadySetGo(dt=80, rewards=None, timing=None, gain=1, prod_margin=0.2)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>Agents have to measure and produce different time intervals.</p> <p>A stimulus is briefly shown during a ready period, then again during a set period. The ready and set periods are separated by a measure period, the duration of which is randomly sampled on each trial. The agent is required to produce a response after the set cue such that the interval between the response and the set cue is as close as possible to the duration of the measure period.</p> <p>Parameters:</p> Name Type Description Default <code>gain</code> <p>Controls the measure that the agent has to produce. (def: 1, int)</p> <code>1</code> <code>prod_margin</code> <p>controls the interval around the ground truth production time within which the agent receives proportional reward</p> <code>0.2</code> Source code in <code>neurogym/envs/readysetgo.py</code> <pre><code>def __init__(self, dt=80, rewards=None, timing=None, gain=1, prod_margin=0.2) -&gt; None:\n    super().__init__(dt=dt)\n    self.prod_margin = prod_margin\n\n    self.gain = gain\n\n    # Rewards\n    self.rewards = {\"abort\": -0.1, \"correct\": +1.0, \"fail\": 0.0}\n    if rewards:\n        self.rewards.update(rewards)\n\n    self.timing = {\n        \"fixation\": 100,\n        \"ready\": 83,\n        \"measure\": lambda: self.rng.uniform(800, 1500),\n        \"set\": 83,\n    }\n    if timing:\n        self.timing.update(timing)\n\n    self.abort = False\n    # set action and observation space\n    name = {\"fixation\": 0, \"ready\": 1, \"set\": 2}\n    self.observation_space = spaces.Box(\n        -np.inf,\n        np.inf,\n        shape=(3,),\n        dtype=np.float32,\n        name=name,\n    )\n\n    name = {\"fixation\": 0, \"go\": 1}\n    self.action_space = spaces.Discrete(2, name=name)  # (fixate, go)\n</code></pre>"},{"location":"api/envs/#neurogym.envs.readysetgo.MotorTiming","title":"MotorTiming","text":"<pre><code>MotorTiming(dt=80, rewards=None, timing=None, prod_margin=0.2)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>Agents have to produce different time intervals using different effectors (actions).</p> <p>Parameters:</p> Name Type Description Default <code>prod_margin</code> <p>controls the interval around the ground truth production time within which the agent receives proportional reward.</p> <code>0.2</code> Source code in <code>neurogym/envs/readysetgo.py</code> <pre><code>def __init__(self, dt=80, rewards=None, timing=None, prod_margin=0.2) -&gt; None:\n    super().__init__(dt=dt)\n    self.prod_margin = prod_margin\n    self.production_ind = [0, 1]\n    self.intervals = [800, 1500]\n\n    # Rewards\n    self.rewards = {\"abort\": -0.1, \"correct\": +1.0, \"fail\": 0.0}\n    if rewards:\n        self.rewards.update(rewards)\n\n    self.timing = {\n        \"fixation\": 500,  # XXX: not specified\n        \"cue\": lambda: self.rng.uniform(1000, 3000),\n        \"set\": 50,\n    }\n    if timing:\n        self.timing.update(timing)\n\n    self.abort = False\n    # set action and observation space\n    self.action_space = spaces.Discrete(2)  # (fixate, go)\n    # Fixation, Interval indicator x2, Set\n    self.observation_space = spaces.Box(\n        -np.inf,\n        np.inf,\n        shape=(4,),\n        dtype=np.float32,\n    )\n</code></pre>"},{"location":"api/envs/#neurogym.envs.readysetgo.OneTwoThreeGo","title":"OneTwoThreeGo","text":"<pre><code>OneTwoThreeGo(dt=80, rewards=None, timing=None, prod_margin=0.2)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>Agents reproduce time intervals based on two samples.</p> <p>Parameters:</p> Name Type Description Default <code>prod_margin</code> <p>controls the interval around the ground truth production         time within which the agent receives proportional reward</p> <code>0.2</code> Source code in <code>neurogym/envs/readysetgo.py</code> <pre><code>def __init__(self, dt=80, rewards=None, timing=None, prod_margin=0.2) -&gt; None:\n    super().__init__(dt=dt)\n\n    self.prod_margin = prod_margin\n\n    # Rewards\n    self.rewards = {\"abort\": -0.1, \"correct\": +1.0, \"fail\": 0.0}\n    if rewards:\n        self.rewards.update(rewards)\n\n    self.timing = {\n        \"fixation\": ngym.random.TruncExp(400, 100, 800),\n        \"target\": ngym.random.TruncExp(1000, 500, 1500),\n        \"s1\": 100,\n        \"interval1\": (600, 700, 800, 900, 1000),\n        \"s2\": 100,\n        \"interval2\": 0,\n        \"s3\": 100,\n        \"interval3\": 0,\n        \"response\": 1000,\n    }\n    if timing:\n        self.timing.update(timing)\n\n    self.abort = False\n    # set action and observation space\n    name = {\"fixation\": 0, \"stimulus\": 1, \"target\": 2}\n    self.observation_space = spaces.Box(\n        -np.inf,\n        np.inf,\n        shape=(3,),\n        dtype=np.float32,\n        name=name,\n    )\n    name = {\"fixation\": 0, \"go\": 1}\n    self.action_space = spaces.Discrete(2, name=name)\n</code></pre>"},{"location":"api/envs/#neurogym.envs.registration","title":"registration","text":""},{"location":"api/envs/#neurogym.envs.registration.all_envs","title":"all_envs","text":"<pre><code>all_envs(tag=None, psychopy=False, contrib=False, collections=False)\n</code></pre> <p>Return a list of all envs in neurogym.</p> Source code in <code>neurogym/envs/registration.py</code> <pre><code>def all_envs(tag=None, psychopy=False, contrib=False, collections=False):\n    \"\"\"Return a list of all envs in neurogym.\"\"\"\n    envs = ALL_NATIVE_ENVS.copy()\n    if psychopy:\n        envs.update(ALL_PSYCHOPY_ENVS)\n    if contrib:\n        envs.update(ALL_CONTRIB_ENVS)\n    if collections:\n        envs.update(ALL_COLLECTIONS_ENVS)\n    env_list = sorted(envs.keys())\n    if tag is None:\n        return env_list\n    if not isinstance(tag, str):\n        msg = f\"{type(tag)=} must be a string.\"\n        raise TypeError(msg)\n\n    new_env_list = []\n    for env in env_list:\n        from_, class_ = envs[env].split(\":\")\n        imported = getattr(__import__(from_, fromlist=[class_]), class_)\n        env_tag = imported.metadata.get(\"tags\", [])\n        if tag in env_tag:\n            new_env_list.append(env)\n    return new_env_list\n</code></pre>"},{"location":"api/envs/#neurogym.envs.spatialsuppressmotion","title":"spatialsuppressmotion","text":""},{"location":"api/envs/#neurogym.envs.spatialsuppressmotion.SpatialSuppressMotion","title":"SpatialSuppressMotion","text":"<pre><code>SpatialSuppressMotion(dt=8.3, timing=None, rewards=None)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>Spatial suppression motion task.</p> <p>This task is useful to study center-surround interaction in monkey MT and human psychophysical performance in motion perception.</p> <p>Tha task is derived from (Tadin et al. Nature, 2003). In this task, there is no fixation or decision stage. We only present a stimulus and a subject needs to perform a 4-AFC motion direction judgement. The ground-truth is the probabilities for choosing the four directions at a given time point. The probabilities depend on stimulus contrast and size, and the probabilities are derived from emprically measured human psychophysical performance.</p> <p>In this version, the input size is 4 (directions) x 8 (size) = 32 neurons. This setting aims to simulate four pools (8 neurons in each pool) of neurons that are selective for four directions.</p> <p>Parameters:</p> Name Type Description Default <code>&lt;dt&gt;</code> <p>millisecs per image frame, default: 8.3 (given 120HZ monitor)</p> required <code>&lt;win_size&gt;</code> <p>size per image frame</p> required <code>&lt;timing&gt;</code> <p>millisecs, stimulus duration, default: 8.3 * 36 frames ~ 300 ms. This is the longest duration we need (i.e., probability reach ceilling)</p> required <p>Note that please input default seq_len = 36 frames when creating dataset object.</p>"},{"location":"api/envs/#neurogym.envs.spatialsuppressmotion.SpatialSuppressMotion--fixme-find-more-stable-way-of-enforcing-above","title":"FIXME: find more stable way of enforcing above.","text":"Source code in <code>neurogym/envs/spatialsuppressmotion.py</code> <pre><code>def __init__(self, dt=8.3, timing=None, rewards=None) -&gt; None:\n    if timing is None:\n        timing = {\"stimulus\": 300}\n    super().__init__(dt=dt)\n\n    # Rewards\n    self.rewards = {\"abort\": -0.1, \"correct\": +1.0, \"fail\": 0.0}\n    if rewards:\n        self.rewards.update(rewards)\n\n    # Timing\n    self.timing = {\n        \"stimulus\": 300,  # we only need stimulus period for psychophysical task\n    }\n    if timing:\n        self.timing.update(timing)\n\n    self.abort = False\n\n    # define action space four directions\n    self.action_space = spaces.Box(\n        0,\n        1,\n        shape=(4,),\n        dtype=np.float32,\n    )  # the probabilities for four direction\n\n    # define observation space\n    self.observation_space = spaces.Box(\n        0,\n        np.inf,\n        shape=(32,),\n        dtype=np.float32,\n    )  # observation space, 4 directions * 8 sizes\n    # larger stimulus could elicit more neurons to fire\n\n    self.directions = [1, 2, 3, 4]  # motion direction left/right/up/down\n    self.theta = [-np.pi / 2, np.pi / 2, 0, np.pi]  # direction angle of the four directions\n    self.directions_anti = [2, 1, 4, 3]\n    self.directions_ortho = [[3, 4], [3, 4], [1, 2], [1, 2]]\n</code></pre>"},{"location":"api/envs/#neurogym.envs.spatialsuppressmotion.SpatialSuppressMotion.getgroundtruth","title":"getgroundtruth","text":"<pre><code>getgroundtruth(trial)\n</code></pre> <p>The utility function to obtain ground truth probabilities for four direction.</p> <p>Input trial is a dict, contains fields , ,  <p>We output a (4,) tuple indicate the probabilities to perceive left/right/up/down direction. This label comes from emprically measured human performance</p> Source code in <code>neurogym/envs/spatialsuppressmotion.py</code> <pre><code>def getgroundtruth(self, trial):\n    \"\"\"The utility function to obtain ground truth probabilities for four direction.\n\n    Input trial is a dict, contains fields &lt;duration&gt;, &lt;contrast&gt;, &lt;diameter&gt;\n\n    We output a (4,) tuple indicate the probabilities to perceive left/right/up/down direction. This label comes\n    from emprically measured human performance\n    \"\"\"\n    frame_ind = [8, 9, 10, 13, 15, 18, 21, 28, 36, 37, 38, 39]\n    xx = [1, 2, 3, 4, 5, 6, 7]\n    yy = [0.249] * 7\n\n    frame_ind = xx + frame_ind  # to fill in the first a few frames\n    frame_ind = [i - 1 for i in frame_ind]  # frame index start from\n\n    seq_len = self.view_ob(period=\"stimulus\").shape[0]\n    xnew = np.arange(seq_len)\n\n    if trial[\"contrast\"] &gt; 0.5:\n        # large size (11 deg radius), High contrast\n        prob_corr = [*yy, 0.249, 0.249, 0.249, 0.27, 0.32, 0.4583, 0.65, 0.85, 0.99, 0.99, 0.99, 0.99]\n        prob_anti = [*yy, 0.249, 0.29, 0.31, 0.4, 0.475, 0.4167, 0.3083, 0.075, 0.04, 0.04, 0.03, 0.03]\n\n    elif trial[\"contrast\"] &lt; 0.5:\n        # large size (11 deg radius), low contrast\n        prob_corr = [*yy, 0.25, 0.26, 0.2583, 0.325, 0.45, 0.575, 0.875, 0.933, 0.99, 0.99, 0.99, 0.99]\n        prob_anti = [*yy, 0.25, 0.26, 0.2583, 0.267, 0.1417, 0.1167, 0.058, 0.016, 0.003, 0.003, 0.003, 0.003]\n\n    corr_prob = interp1d(\n        frame_ind,\n        prob_corr,\n        kind=\"slinear\",\n        fill_value=\"extrapolate\",\n    )(xnew)\n    anti_prob = interp1d(\n        frame_ind,\n        prob_anti,\n        kind=\"slinear\",\n        fill_value=\"extrapolate\",\n    )(xnew)\n    ortho_prob = (1 - (corr_prob + anti_prob)) / 2\n\n    direction = trial[\"direction\"] - 1\n    direction_anti = self.directions_anti[direction] - 1\n    direction_ortho = [i - 1 for i in self.directions_ortho[direction]]\n\n    gt = np.zeros((4, seq_len))\n    gt[direction, :] = corr_prob\n    gt[direction_anti, :] = anti_prob\n    gt[direction_ortho, :] = ortho_prob\n\n    return gt.T  # gt is a seq_len x 4 numpy array\n</code></pre>"},{"location":"api/envs/#neurogym.envs.tonedetection","title":"tonedetection","text":"<p>auditory tone detection task.</p>"},{"location":"api/envs/#neurogym.envs.tonedetection.ToneDetection","title":"ToneDetection","text":"<pre><code>ToneDetection(dt=50, sigma=0.2, timing=None)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>A subject is asked to report whether a pure tone is embeddied within a background noise.</p> <p>If yes, should indicate the position of the tone. The tone lasts 50ms and could appear at the 500ms, 1000ms, and 1500ms. The tone is embbeded within noises.</p> <p>By Ru-Yuan Zhang (ruyuanzhang@gmail.com)</p> <p>Note in this version we did not consider the fixation period as we mainly aim to model human data.</p> <p>For an animal version of this task, please consider to include fixation and saccade cues. See https://www.nature.com/articles/nn1386</p> <p>Note that the output labels is of shape (seq_len, batch_size). For a human perceptual task, you can simply run labels = labels[-1, :] get the final output.</p> <p>Parameters:</p> Name Type Description Default <code>&lt;dt&gt;</code> <p>milliseconds, delta time,</p> required <code>&lt;sigma&gt;</code> <p>float, input noise level, control the task difficulty</p> required <code>&lt;timing&gt;</code> <p>stimulus timing</p> required Source code in <code>neurogym/envs/tonedetection.py</code> <pre><code>def __init__(self, dt=50, sigma=0.2, timing=None) -&gt; None:\n    super().__init__(dt=dt)\n    \"\"\"\n    Here the key variables are\n    &lt;self.toneDur&gt;: ms, duration of the tone\n    &lt;self.toneTiming&gt;: ms, onset of the tone\n    \"\"\"\n    self.sigma = sigma / np.sqrt(self.dt)  # Input noise\n\n    # Rewards\n    self.rewards = {\n        \"abort\": -0.1,\n        \"correct\": +1.0,\n        \"noresp\": -0.1,\n    }  # need to change here\n\n    self.timing = {\n        \"stimulus\": 2000,\n        \"toneTiming\": [500, 1000, 1500],\n        \"toneDur\": 50,\n    }\n    if timing:\n        self.timing.update(timing)\n\n    self.toneTiming = self.timing[\"toneTiming\"]\n    self.toneDur = self.timing[\"toneDur\"]  # ms, the duration of a tone\n\n    if dt &gt; self.toneDur:\n        msg = f\"{dt=} must be smaller or equal tp tone duration {self.toneDur} (default=50).\"\n        raise ValueError(msg)\n\n    self.toneDurIdx = int(self.toneDur / dt)  # how many data point it lasts\n\n    self.toneTimingIdx = [int(i / dt) for i in self.toneTiming]\n    self.stimArray = np.zeros(int(self.timing[\"stimulus\"] / dt))\n\n    self.abort = False\n\n    self.signals = np.linspace(0, 1, 5)[:-1]  # signal strength\n    self.conditions = [0, 1, 2, 3]  # no tone, tone at position 1/2/3\n\n    self.observation_space = spaces.Box(\n        -np.inf,\n        np.inf,\n        shape=(1,),\n        dtype=np.float32,\n    )\n    self.ob_dict = {\"fixation\": 0, \"stimulus\": 1}\n    self.action_space = spaces.Discrete(4)\n    self.act_dict = {\"fixation\": 0, \"choice\": range(1, 5 + 1)}\n</code></pre>"},{"location":"api/tags/","title":"Tags","text":""},{"location":"api/tags/#confidence","title":"Confidence","text":""},{"location":"api/tags/#neurogym.envs.postdecisionwager.PostDecisionWager","title":"PostDecisionWager","text":"<pre><code>PostDecisionWager(dt=100, rewards=None, timing=None, dim_ring=2, sigma=1.0)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>Post-decision wagering task assessing confidence.</p> <p>The agent first performs a perceptual discrimination task (see for more details the PerceptualDecisionMaking task). On a random half of the trials, the agent is given the option to abort the sensory discrimination and to choose instead a sure-bet option that guarantees a small reward. Therefore, the agent is encouraged to choose the sure-bet option when it is uncertain about its perceptual decision.</p> Source code in <code>neurogym/envs/postdecisionwager.py</code> <pre><code>def __init__(self, dt=100, rewards=None, timing=None, dim_ring=2, sigma=1.0) -&gt; None:\n    super().__init__(dt=dt)\n\n    self.wagers = [True, False]\n    self.theta = np.linspace(0, 2 * np.pi, dim_ring + 1)[:-1]\n    self.choices = np.arange(dim_ring)\n    self.cohs = [0, 3.2, 6.4, 12.8, 25.6, 51.2]\n    self.sigma = sigma / np.sqrt(self.dt)  # Input noise\n\n    # Rewards\n    self.rewards = {\"abort\": -0.1, \"correct\": +1.0, \"fail\": 0.0}\n    if rewards:\n        self.rewards.update(rewards)\n    self.rewards[\"sure\"] = 0.7 * self.rewards[\"correct\"]\n\n    self.timing = {\n        \"fixation\": 100,\n        # 'target':  0,  # noqa: ERA001\n        \"stimulus\": ngym.random.TruncExp(180, 100, 900),\n        \"delay\": ngym.random.TruncExp(1350, 1200, 1800),\n        \"pre_sure\": lambda: self.rng.uniform(500, 750),\n        \"decision\": 100,\n    }\n    if timing:\n        self.timing.update(timing)\n\n    self.abort = False\n\n    # set action and observation space\n    name = {\"fixation\": 0, \"stimulus\": [1, 2], \"sure\": 3}\n    self.observation_space = spaces.Box(\n        -np.inf,\n        np.inf,\n        shape=(4,),\n        dtype=np.float32,\n        name=name,\n    )\n    name = {\"fixation\": 0, \"choice\": [1, 2], \"sure\": 3}\n    self.action_space = spaces.Discrete(4, name=name)\n</code></pre>"},{"location":"api/tags/#context-dependent","title":"Context Dependent","text":""},{"location":"api/tags/#neurogym.envs.contextdecisionmaking.ContextDecisionMaking","title":"ContextDecisionMaking","text":"<pre><code>ContextDecisionMaking(dt=100, rewards=None, timing=None, sigma=1.0)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>Context-dependent decision-making task.</p> <p>The agent simultaneously receives stimulus inputs from two modalities ( for example, a colored random dot motion pattern with color and motion modalities). The agent needs to make a perceptual decision based on only one of the two modalities, while ignoring the other. The relevant modality is explicitly indicated by a rule signal.</p> Source code in <code>neurogym/envs/contextdecisionmaking.py</code> <pre><code>def __init__(self, dt=100, rewards=None, timing=None, sigma=1.0) -&gt; None:\n    super().__init__(dt=dt)\n\n    # trial conditions\n    self.contexts = [0, 1]  # index for context inputs\n    self.choices = [1, 2]  # left, right choice\n    self.cohs = [5, 15, 50]\n    self.sigma = sigma / np.sqrt(self.dt)  # Input noise\n\n    # Rewards\n    self.rewards = {\"abort\": -0.1, \"correct\": +1.0}\n    if rewards:\n        self.rewards.update(rewards)\n\n    self.timing = {\n        \"fixation\": 300,\n        # 'target': 350, # noqa: ERA001\n        \"stimulus\": 750,\n        \"delay\": ngym.random.TruncExp(600, 300, 3000),\n        \"decision\": 100,\n    }\n    if timing:\n        self.timing.update(timing)\n\n    self.abort = False\n\n    # set action and observation space\n    names = [\n        \"fixation\",\n        \"stim1_mod1\",\n        \"stim2_mod1\",\n        \"stim1_mod2\",\n        \"stim2_mod2\",\n        \"context1\",\n        \"context2\",\n    ]\n    name = {name: i for i, name in enumerate(names)}\n    self.observation_space = spaces.Box(\n        -np.inf,\n        np.inf,\n        shape=(7,),\n        dtype=np.float32,\n        name=name,\n    )\n\n    name = {\"fixation\": 0, \"choice1\": 1, \"choice2\": 2}\n    self.action_space = spaces.Discrete(3, name=name)\n</code></pre>"},{"location":"api/tags/#neurogym.envs.contextdecisionmaking.SingleContextDecisionMaking","title":"SingleContextDecisionMaking","text":"<pre><code>SingleContextDecisionMaking(dt=100, context=0, rewards=None, timing=None, sigma=1.0, dim_ring=2)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>Context-dependent decision-making task.</p> <p>The agent simultaneously receives stimulus inputs from two modalities ( for example, a colored random dot motion pattern with color and motion modalities). The agent needs to make a perceptual decision based on only one of the two modalities, while ignoring the other. The agent reports its decision during the decision period, with an optional delay period in between the stimulus period and the decision period. The relevant modality is not explicitly signaled.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <p>int, 0 or 1 for the two context (rules). If 0, need to focus on modality 0 (the first one)</p> <code>0</code> Source code in <code>neurogym/envs/contextdecisionmaking.py</code> <pre><code>def __init__(\n    self,\n    dt=100,\n    context=0,\n    rewards=None,\n    timing=None,\n    sigma=1.0,\n    dim_ring=2,\n) -&gt; None:\n    super().__init__(dt=dt)\n\n    # trial conditions\n    self.cohs = [5, 15, 50]\n    self.sigma = sigma / np.sqrt(self.dt)  # Input noise\n    self.context = context\n\n    # Rewards\n    self.rewards = {\"abort\": -0.1, \"correct\": +1.0}\n    if rewards:\n        self.rewards.update(rewards)\n\n    self.timing = {\n        \"fixation\": 300,\n        # 'target': 350, # noqa: ERA001\n        \"stimulus\": 750,\n        \"delay\": ngym.random.TruncExp(600, 300, 3000),\n        \"decision\": 100,\n    }\n    if timing:\n        self.timing.update(timing)\n\n    self.abort = False\n\n    # set action and observation space\n    self.theta = np.linspace(0, 2 * np.pi, dim_ring + 1)[:-1]\n    self.choices = np.arange(dim_ring)\n\n    name = {\n        \"fixation\": 0,\n        \"stimulus_mod1\": range(1, dim_ring + 1),\n        \"stimulus_mod2\": range(dim_ring + 1, 2 * dim_ring + 1),\n    }\n    shape = (1 + 2 * dim_ring,)\n    self.observation_space = spaces.Box(\n        -np.inf,\n        np.inf,\n        shape=shape,\n        dtype=np.float32,\n        name=name,\n    )\n\n    name = {\"fixation\": 0, \"choice\": range(1, dim_ring + 1)}\n    self.action_space = spaces.Discrete(1 + dim_ring, name=name)\n</code></pre>"},{"location":"api/tags/#continuous-action-space","title":"Continuous Action Space","text":""},{"location":"api/tags/#neurogym.envs.reachingdelayresponse.ReachingDelayResponse","title":"ReachingDelayResponse","text":"<pre><code>ReachingDelayResponse(dt=100, rewards=None, timing=None, lowbound=0.0, highbound=1.0)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>Reaching task with a delay period.</p> <p>A reaching direction is presented by the stimulus during the stimulus period. Followed by a delay period, the agent needs to respond to the direction of the stimulus during the decision period.</p> Source code in <code>neurogym/envs/reachingdelayresponse.py</code> <pre><code>def __init__(self, dt=100, rewards=None, timing=None, lowbound=0.0, highbound=1.0) -&gt; None:\n    super().__init__(dt=dt)\n    self.lowbound = lowbound\n    self.highbound = highbound\n\n    # Rewards\n    self.rewards = {\"abort\": -0.1, \"correct\": +1.0, \"fail\": -0.0, \"miss\": -0.5}\n    if rewards:\n        self.rewards.update(rewards)\n\n    self.timing = {\"stimulus\": 500, \"delay\": (0, 1000, 2000), \"decision\": 500}\n    if timing:\n        self.timing.update(timing)\n\n    self.r_tmax = self.rewards[\"miss\"]\n    self.abort = False\n\n    name = {\"go\": 0, \"stimulus\": 1}\n    self.observation_space = spaces.Box(\n        low=np.array([0.0, -2]),\n        high=np.array([1, 2.0]),\n        dtype=np.float32,\n        name=name,\n    )\n\n    self.action_space = spaces.Box(\n        low=np.array((-1.0, -1.0)),\n        high=np.array((1.0, 2.0)),\n        dtype=np.float32,\n    )\n</code></pre>"},{"location":"api/tags/#delayed-response","title":"Delayed Response","text":""},{"location":"api/tags/#neurogym.envs.gonogo.GoNogo","title":"GoNogo","text":"<pre><code>GoNogo(dt=100, rewards=None, timing=None)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>Go/No-go task.</p> <p>A stimulus is shown during the stimulus period. The stimulus period is followed by a delay period, and then a decision period. If the stimulus is a Go stimulus, then the subject should choose the action Go during the decision period, otherwise, the subject should remain fixation.</p> Source code in <code>neurogym/envs/gonogo.py</code> <pre><code>def __init__(self, dt=100, rewards=None, timing=None) -&gt; None:\n    super().__init__(dt=dt)\n    # Actions are (FIXATE, GO)\n    self.actions = [0, 1]\n    # trial conditions\n    self.choices = [0, 1]\n\n    # Rewards\n    self.rewards = {\"abort\": -0.1, \"correct\": +1.0, \"fail\": -0.5, \"miss\": -0.5}\n    if rewards:\n        self.rewards.update(rewards)\n\n    self.timing = {\"fixation\": 0, \"stimulus\": 500, \"delay\": 500, \"decision\": 500}\n    if timing:\n        self.timing.update(timing)\n\n    self.abort = False\n    # set action and observation spaces\n    name = {\"fixation\": 0, \"nogo\": 1, \"go\": 2}\n    self.observation_space = spaces.Box(\n        -np.inf,\n        np.inf,\n        shape=(3,),\n        dtype=np.float32,\n        name=name,\n    )\n    self.action_space = spaces.Discrete(2, {\"fixation\": 0, \"go\": 1})\n</code></pre>"},{"location":"api/tags/#neurogym.envs.intervaldiscrimination.IntervalDiscrimination","title":"IntervalDiscrimination","text":"<pre><code>IntervalDiscrimination(dt=80, rewards=None, timing=None)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>Comparing the time length of two stimuli.</p> <p>Two stimuli are shown sequentially, separated by a delay period. The duration of each stimulus is randomly sampled on each trial. The subject needs to judge which stimulus has a longer duration, and reports its decision during the decision period by choosing one of the two choice options.</p> Source code in <code>neurogym/envs/intervaldiscrimination.py</code> <pre><code>def __init__(self, dt=80, rewards=None, timing=None) -&gt; None:\n    super().__init__(dt=dt)\n    # Rewards\n    self.rewards = {\"abort\": -0.1, \"correct\": +1.0, \"fail\": 0.0}\n    if rewards:\n        self.rewards.update(rewards)\n\n    self.timing = {\n        \"fixation\": 300,\n        \"stim1\": lambda: self.rng.uniform(300, 600),\n        \"delay1\": lambda: self.rng.uniform(800, 1500),\n        \"stim2\": lambda: self.rng.uniform(300, 600),\n        \"delay2\": 500,\n        \"decision\": 300,\n    }\n    if timing:\n        self.timing.update(timing)\n\n    self.abort = False\n\n    name = {\"fixation\": 0, \"stim1\": 1, \"stim2\": 2}\n    self.observation_space = spaces.Box(\n        -np.inf,\n        np.inf,\n        shape=(3,),\n        dtype=np.float32,\n        name=name,\n    )\n    name = {\"fixation\": 0, \"choice1\": 1, \"choice2\": 2}\n    self.action_space = spaces.Discrete(3, name=name)\n</code></pre>"},{"location":"api/tags/#neurogym.envs.perceptualdecisionmaking.PerceptualDecisionMakingDelayResponse","title":"PerceptualDecisionMakingDelayResponse","text":"<pre><code>PerceptualDecisionMakingDelayResponse(dt=100, rewards=None, timing=None, stim_scale=1.0, sigma=1.0)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>Perceptual decision-making with delayed responses.</p> <p>Agents have to integrate two stimuli and report which one is larger on average after a delay.</p> <p>Parameters:</p> Name Type Description Default <code>stim_scale</code> <p>Controls the difficulty of the experiment. (def: 1., float)</p> <code>1.0</code> Source code in <code>neurogym/envs/perceptualdecisionmaking.py</code> <pre><code>def __init__(self, dt=100, rewards=None, timing=None, stim_scale=1.0, sigma=1.0) -&gt; None:\n    super().__init__(dt=dt)\n    self.choices = [1, 2]\n    # cohs specifies the amount of evidence (modulated by stim_scale)\n    self.cohs = np.array([0, 6.4, 12.8, 25.6, 51.2]) * stim_scale\n    self.sigma = sigma / np.sqrt(self.dt)  # Input noise\n\n    # Rewards\n    self.rewards = {\"abort\": -0.1, \"correct\": +1.0, \"fail\": 0.0}\n    if rewards:\n        self.rewards.update(rewards)\n\n    self.timing = {\n        \"fixation\": 0,\n        \"stimulus\": 1150,\n        #  TODO: sampling of delays follows exponential\n        \"delay\": (300, 500, 700, 900, 1200, 2000, 3200, 4000),\n        # 'go_cue': 100,  # noqa: ERA001 TODO: Not implemented\n        \"decision\": 1500,\n    }\n    if timing:\n        self.timing.update(timing)\n\n    self.abort = False\n\n    # action and observation spaces\n    self.action_space = spaces.Discrete(3)\n    self.observation_space = spaces.Box(\n        -np.inf,\n        np.inf,\n        shape=(3,),\n        dtype=np.float32,\n    )\n</code></pre>"},{"location":"api/tags/#neurogym.envs.postdecisionwager.PostDecisionWager","title":"PostDecisionWager","text":"<pre><code>PostDecisionWager(dt=100, rewards=None, timing=None, dim_ring=2, sigma=1.0)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>Post-decision wagering task assessing confidence.</p> <p>The agent first performs a perceptual discrimination task (see for more details the PerceptualDecisionMaking task). On a random half of the trials, the agent is given the option to abort the sensory discrimination and to choose instead a sure-bet option that guarantees a small reward. Therefore, the agent is encouraged to choose the sure-bet option when it is uncertain about its perceptual decision.</p> Source code in <code>neurogym/envs/postdecisionwager.py</code> <pre><code>def __init__(self, dt=100, rewards=None, timing=None, dim_ring=2, sigma=1.0) -&gt; None:\n    super().__init__(dt=dt)\n\n    self.wagers = [True, False]\n    self.theta = np.linspace(0, 2 * np.pi, dim_ring + 1)[:-1]\n    self.choices = np.arange(dim_ring)\n    self.cohs = [0, 3.2, 6.4, 12.8, 25.6, 51.2]\n    self.sigma = sigma / np.sqrt(self.dt)  # Input noise\n\n    # Rewards\n    self.rewards = {\"abort\": -0.1, \"correct\": +1.0, \"fail\": 0.0}\n    if rewards:\n        self.rewards.update(rewards)\n    self.rewards[\"sure\"] = 0.7 * self.rewards[\"correct\"]\n\n    self.timing = {\n        \"fixation\": 100,\n        # 'target':  0,  # noqa: ERA001\n        \"stimulus\": ngym.random.TruncExp(180, 100, 900),\n        \"delay\": ngym.random.TruncExp(1350, 1200, 1800),\n        \"pre_sure\": lambda: self.rng.uniform(500, 750),\n        \"decision\": 100,\n    }\n    if timing:\n        self.timing.update(timing)\n\n    self.abort = False\n\n    # set action and observation space\n    name = {\"fixation\": 0, \"stimulus\": [1, 2], \"sure\": 3}\n    self.observation_space = spaces.Box(\n        -np.inf,\n        np.inf,\n        shape=(4,),\n        dtype=np.float32,\n        name=name,\n    )\n    name = {\"fixation\": 0, \"choice\": [1, 2], \"sure\": 3}\n    self.action_space = spaces.Discrete(4, name=name)\n</code></pre>"},{"location":"api/tags/#neurogym.envs.reachingdelayresponse.ReachingDelayResponse","title":"ReachingDelayResponse","text":"<pre><code>ReachingDelayResponse(dt=100, rewards=None, timing=None, lowbound=0.0, highbound=1.0)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>Reaching task with a delay period.</p> <p>A reaching direction is presented by the stimulus during the stimulus period. Followed by a delay period, the agent needs to respond to the direction of the stimulus during the decision period.</p> Source code in <code>neurogym/envs/reachingdelayresponse.py</code> <pre><code>def __init__(self, dt=100, rewards=None, timing=None, lowbound=0.0, highbound=1.0) -&gt; None:\n    super().__init__(dt=dt)\n    self.lowbound = lowbound\n    self.highbound = highbound\n\n    # Rewards\n    self.rewards = {\"abort\": -0.1, \"correct\": +1.0, \"fail\": -0.0, \"miss\": -0.5}\n    if rewards:\n        self.rewards.update(rewards)\n\n    self.timing = {\"stimulus\": 500, \"delay\": (0, 1000, 2000), \"decision\": 500}\n    if timing:\n        self.timing.update(timing)\n\n    self.r_tmax = self.rewards[\"miss\"]\n    self.abort = False\n\n    name = {\"go\": 0, \"stimulus\": 1}\n    self.observation_space = spaces.Box(\n        low=np.array([0.0, -2]),\n        high=np.array([1, 2.0]),\n        dtype=np.float32,\n        name=name,\n    )\n\n    self.action_space = spaces.Box(\n        low=np.array((-1.0, -1.0)),\n        high=np.array((1.0, 2.0)),\n        dtype=np.float32,\n    )\n</code></pre>"},{"location":"api/utils/","title":"Utils","text":""},{"location":"api/utils/#neurogym.utils.data","title":"data","text":"<p>Utilities for data.</p>"},{"location":"api/utils/#neurogym.utils.data.Dataset","title":"Dataset","text":"<pre><code>Dataset(env, env_kwargs=None, batch_size=1, seq_len=None, max_batch=inf, batch_first=False, cache_len=None)\n</code></pre> <p>Make an environment into an iterable dataset for supervised learning.</p> <p>Create an iterator that at each call returns     inputs: numpy array (sequence_length, batch_size, input_units)     target: numpy array (sequence_length, batch_size, output_units)</p> <p>Parameters:</p> Name Type Description Default <code>env</code> <p>str for env id or gym.Env objects</p> required <code>env_kwargs</code> <p>dict, additional kwargs for environment, if env is str</p> <code>None</code> <code>batch_size</code> <p>int, batch size</p> <code>1</code> <code>seq_len</code> <p>int, sequence length</p> <code>None</code> <code>max_batch</code> <p>int, maximum number of batch for iterator, default infinite</p> <code>inf</code> <code>batch_first</code> <p>bool, if True, return (batch, seq_len, n_units), default False</p> <code>False</code> <code>cache_len</code> <p>int, default length of caching</p> <code>None</code> Source code in <code>neurogym/utils/data.py</code> <pre><code>def __init__(\n    self,\n    env,\n    env_kwargs=None,\n    batch_size=1,\n    seq_len=None,\n    max_batch=np.inf,\n    batch_first=False,\n    cache_len=None,\n) -&gt; None:\n    if not isinstance(env, str | gym.Env):\n        msg = f\"{type(env)=} must be `gym.Env` or `str`.\"\n        raise TypeError(msg)\n    if isinstance(env, gym.Env):\n        self.envs = [copy.deepcopy(env) for _ in range(batch_size)]\n    else:\n        if env_kwargs is None:\n            env_kwargs = {}\n        self.envs = [gym.make(env, **env_kwargs) for _ in range(batch_size)]\n    for env_ in self.envs:\n        env_.reset()\n    self.seed()\n\n    env = self.envs[0]\n    self.env = env\n    self.batch_size = batch_size\n    self.batch_first = batch_first\n\n    if seq_len is None:\n        # TODO: infer sequence length from task\n        seq_len = 1000\n\n    obs_shape = env.observation_space.shape\n    action_shape = env.action_space.shape\n    if len(action_shape) == 0:\n        self._expand_action = True\n    else:\n        self._expand_action = False\n    if cache_len is None:\n        # Infer cache len\n        cache_len = 1e5  # Probably too low\n        cache_len /= np.prod(obs_shape) + np.prod(action_shape)\n        cache_len /= batch_size\n    cache_len = int((1 + (cache_len // seq_len)) * seq_len)\n\n    self.seq_len = seq_len\n    self._cache_len = cache_len\n\n    if batch_first:\n        shape1, shape2 = [batch_size, seq_len], [batch_size, cache_len]\n    else:\n        shape1, shape2 = [seq_len, batch_size], [cache_len, batch_size]\n\n    self.inputs_shape = shape1 + list(obs_shape)\n    self.target_shape = shape1 + list(action_shape)\n    self._cache_inputs_shape = shape2 + list(obs_shape)\n    self._cache_target_shape = shape2 + list(action_shape)\n\n    self._inputs = np.zeros(\n        self._cache_inputs_shape,\n        dtype=env.observation_space.dtype,\n    )\n    self._target = np.zeros(self._cache_target_shape, dtype=env.action_space.dtype)\n\n    self._cache()\n\n    self._i_batch = 0\n    self.max_batch = max_batch\n</code></pre>"},{"location":"api/utils/#neurogym.utils.info","title":"info","text":"<p>Formatting information about envs and wrappers.</p>"},{"location":"api/utils/#neurogym.utils.info.info","title":"info","text":"<pre><code>info(env=None, show_code=False)\n</code></pre> <p>Script to get envs info.</p> Source code in <code>neurogym/utils/info.py</code> <pre><code>def info(env=None, show_code=False):\n    \"\"\"Script to get envs info.\"\"\"\n    string = \"\"\n    env_name = env\n    env = ngym.make(env)\n    # remove extra wrappers (make can add a OrderEnforcer wrapper)\n    env = env.unwrapped\n    string = env_string(env)\n    # show source code\n    if show_code:\n        string += \"\"\"\\n#### Source code #### \\n\\n\"\"\"\n        env_ref = ALL_ENVS[env_name]\n        from_, class_ = env_ref.split(\":\")\n        imported = getattr(__import__(from_, fromlist=[class_]), class_)\n        lines = inspect.getsource(imported)\n        string += lines + \"\\n\\n\"\n    return string\n</code></pre>"},{"location":"api/utils/#neurogym.utils.info.info_wrapper","title":"info_wrapper","text":"<pre><code>info_wrapper(wrapper=None, show_code=False)\n</code></pre> <p>Script to get wrappers info.</p> Source code in <code>neurogym/utils/info.py</code> <pre><code>def info_wrapper(wrapper=None, show_code=False):\n    \"\"\"Script to get wrappers info.\"\"\"\n    string = \"\"\n\n    wrapp_ref = ALL_WRAPPERS[wrapper]\n    from_, class_ = wrapp_ref.split(\":\")\n    imported = getattr(__import__(from_, fromlist=[class_]), class_)\n    metadata = imported.metadata\n\n    if not isinstance(metadata, dict):\n        metadata = {}\n\n    string += f\"### {wrapper}\\n\\n\"\n    paper_name = metadata.get(\"paper_name\", None)\n    paper_link = metadata.get(\"paper_link\", None)\n    wrapper_description = metadata.get(\"description\", None) or \"Missing description\"\n    string += f\"Logic: {wrapper_description}\\n\\n\"\n    if paper_name is not None:\n        string += \"Reference paper \\n\\n\"\n        if paper_link is None:\n            string += f\"{paper_name}\\n\\n\"\n        else:\n            string += f\"[{paper_name}]({paper_link})\\n\\n\"\n    # add extra info\n    other_info = list(set(metadata.keys()) - set(METADATA_DEF_KEYS))\n    if len(other_info) &gt; 0:\n        string += \"Input parameters: \\n\\n\"\n        for key in other_info:\n            string += f\"{key} : {metadata[key]}\\n\\n\"\n\n    # show source code\n    if show_code:\n        string += \"\"\"\\n#### Source code #### \\n\\n\"\"\"\n        lines = inspect.getsource(imported)\n        string += lines + \"\\n\\n\"\n\n    return string\n</code></pre>"},{"location":"api/utils/#neurogym.utils.info.all_tags","title":"all_tags","text":"<pre><code>all_tags(verbose=0)\n</code></pre> <p>Script to get all tags.</p> Source code in <code>neurogym/utils/info.py</code> <pre><code>def all_tags(verbose=0):\n    \"\"\"Script to get all tags.\"\"\"\n    envs = all_envs()\n    tags = []\n    for env_name in sorted(envs):\n        try:\n            env = ngym.make(env_name)\n            metadata = env.metadata\n            tags += metadata.get(\"tags\", [])\n        except BaseException as e:  # noqa: BLE001, PERF203 # FIXME: unclear which error is expected here.\n            print(\"Failure in \", env_name)\n            print(e)\n    tags = set(tags)\n    if verbose:\n        print(\"\\nTAGS:\\n\")\n        for tag in tags:\n            print(tag)\n    return tags\n</code></pre>"},{"location":"api/utils/#neurogym.utils.plotting","title":"plotting","text":"<p>Plotting functions.</p>"},{"location":"api/utils/#neurogym.utils.plotting.plot_env","title":"plot_env","text":"<pre><code>plot_env(env, num_steps=200, num_trials=None, def_act=None, model=None, name=None, legend=True, ob_traces=None, fig_kwargs=None, fname=None)\n</code></pre> <p>Plot environment with agent.</p> <p>Parameters:</p> Name Type Description Default <code>env</code> <p>already built neurogym task or name of it</p> required <code>num_steps</code> <p>number of steps to run the task</p> <code>200</code> <code>num_trials</code> <p>if not None, the number of trials to run</p> <code>None</code> <code>def_act</code> <p>if not None (and model=None), the task will be run with the      specified action</p> <code>None</code> <code>model</code> <p>if not None, the task will be run with the actions predicted by    model, which so far is assumed to be created and trained with the    stable-baselines3 toolbox:        (https://stable-baselines3.readthedocs.io/en/master/)</p> <code>None</code> <code>name</code> <p>title to show on the rewards panel</p> <code>None</code> <code>legend</code> <p>whether to show the legend for actions panel or not.</p> <code>True</code> <code>ob_traces</code> <p>if != [] observations will be plot as traces, with the labels         specified by ob_traces</p> <code>None</code> <code>fig_kwargs</code> <p>figure properties admited by matplotlib.pyplot.subplots() fun.</p> <code>None</code> <code>fname</code> <p>if not None, save fig or movie to fname</p> <code>None</code> Source code in <code>neurogym/utils/plotting.py</code> <pre><code>def plot_env(\n    env,\n    num_steps=200,\n    num_trials=None,\n    def_act=None,\n    model=None,\n    name=None,\n    legend=True,\n    ob_traces=None,\n    fig_kwargs=None,\n    fname=None,\n):\n    \"\"\"Plot environment with agent.\n\n    Args:\n        env: already built neurogym task or name of it\n        num_steps: number of steps to run the task\n        num_trials: if not None, the number of trials to run\n        def_act: if not None (and model=None), the task will be run with the\n                 specified action\n        model: if not None, the task will be run with the actions predicted by\n               model, which so far is assumed to be created and trained with the\n               stable-baselines3 toolbox:\n                   (https://stable-baselines3.readthedocs.io/en/master/)\n        name: title to show on the rewards panel\n        legend: whether to show the legend for actions panel or not.\n        ob_traces: if != [] observations will be plot as traces, with the labels\n                    specified by ob_traces\n        fig_kwargs: figure properties admited by matplotlib.pyplot.subplots() fun.\n        fname: if not None, save fig or movie to fname\n    \"\"\"\n    # We don't use monitor here because:\n    # 1) env could be already prewrapped with monitor\n    # 2) monitor will save data and so the function will need a folder\n\n    if fig_kwargs is None:\n        fig_kwargs = {}\n    if ob_traces is None:\n        ob_traces = []\n    if isinstance(env, str):\n        env = gym.make(env)\n    if name is None:\n        name = type(env).__name__\n    data = run_env(\n        env=env,\n        num_steps=num_steps,\n        num_trials=num_trials,\n        def_act=def_act,\n        model=model,\n    )\n\n    return fig_(\n        data[\"ob\"],\n        data[\"actions\"],\n        gt=data[\"gt\"],\n        rewards=data[\"rewards\"],\n        legend=legend,\n        performance=data[\"perf\"],\n        states=data[\"states\"],\n        name=name,\n        ob_traces=ob_traces,\n        fig_kwargs=fig_kwargs,\n        env=env,\n        fname=fname,\n    )\n</code></pre>"},{"location":"api/utils/#neurogym.utils.plotting.fig_","title":"fig_","text":"<pre><code>fig_(ob, actions, gt=None, rewards=None, performance=None, states=None, legend=True, ob_traces=None, name='', fname=None, fig_kwargs=None, env=None)\n</code></pre> <p>Visualize a run in a simple environment.</p> <p>Parameters:</p> Name Type Description Default <code>ob</code> <p>np array of observation (n_step, n_unit)</p> required <code>actions</code> <p>np array of action (n_step, n_unit)</p> required <code>gt</code> <p>np array of groud truth</p> <code>None</code> <code>rewards</code> <p>np array of rewards</p> <code>None</code> <code>performance</code> <p>np array of performance</p> <code>None</code> <code>states</code> <p>np array of network states</p> <code>None</code> <code>name</code> <p>title to show on the rewards panel and name to save figure</p> <code>''</code> <code>fname</code> <p>if != '', where to save the figure</p> <code>None</code> <code>legend</code> <p>whether to show the legend for actions panel or not.</p> <code>True</code> <code>ob_traces</code> <p>None or list. If list, observations will be plot as traces, with the labels specified by ob_traces</p> <code>None</code> <code>fig_kwargs</code> <p>figure properties admited by matplotlib.pyplot.subplots() fun.</p> <code>None</code> <code>env</code> <p>environment class for extra information</p> <code>None</code> Source code in <code>neurogym/utils/plotting.py</code> <pre><code>def fig_(\n    ob,\n    actions,\n    gt=None,\n    rewards=None,\n    performance=None,\n    states=None,\n    legend=True,\n    ob_traces=None,\n    name=\"\",\n    fname=None,\n    fig_kwargs=None,\n    env=None,\n):\n    \"\"\"Visualize a run in a simple environment.\n\n    Args:\n        ob: np array of observation (n_step, n_unit)\n        actions: np array of action (n_step, n_unit)\n        gt: np array of groud truth\n        rewards: np array of rewards\n        performance: np array of performance\n        states: np array of network states\n        name: title to show on the rewards panel and name to save figure\n        fname: if != '', where to save the figure\n        legend: whether to show the legend for actions panel or not.\n        ob_traces: None or list.\n            If list, observations will be plot as traces, with the labels\n            specified by ob_traces\n        fig_kwargs: figure properties admited by matplotlib.pyplot.subplots() fun.\n        env: environment class for extra information\n    \"\"\"\n    if fig_kwargs is None:\n        fig_kwargs = {}\n    ob = np.array(ob)\n    actions = np.array(actions)\n\n    if len(ob.shape) == 2:\n        return plot_env_1dbox(\n            ob,\n            actions,\n            gt=gt,\n            rewards=rewards,\n            performance=performance,\n            states=states,\n            legend=legend,\n            ob_traces=ob_traces,\n            name=name,\n            fname=fname,\n            fig_kwargs=fig_kwargs,\n            env=env,\n        )\n    if len(ob.shape) == 4:\n        return plot_env_3dbox(ob, fname=fname, env=env)\n\n    msg = f\"{ob.shape=} not supported.\"\n    raise ValueError(msg)\n</code></pre>"},{"location":"api/utils/#neurogym.utils.plotting.plot_env_1dbox","title":"plot_env_1dbox","text":"<pre><code>plot_env_1dbox(ob, actions, gt=None, rewards=None, performance=None, states=None, legend=True, ob_traces=None, name='', fname=None, fig_kwargs=None, env=None)\n</code></pre> <p>Plot environment with 1-D Box observation space.</p> Source code in <code>neurogym/utils/plotting.py</code> <pre><code>def plot_env_1dbox(\n    ob,\n    actions,\n    gt=None,\n    rewards=None,\n    performance=None,\n    states=None,\n    legend=True,\n    ob_traces=None,\n    name=\"\",\n    fname=None,\n    fig_kwargs=None,\n    env=None,\n):\n    \"\"\"Plot environment with 1-D Box observation space.\"\"\"\n    if fig_kwargs is None:\n        fig_kwargs = {}\n    if len(ob.shape) != 2:\n        msg = \"ob has to be 2-dimensional.\"\n        raise ValueError(msg)\n    steps = np.arange(ob.shape[0])  # XXX: +1? 1st ob doesn't have action/gt\n\n    n_row = 2  # observation and action\n    n_row += rewards is not None\n    n_row += performance is not None\n    n_row += states is not None\n\n    gt_colors = \"gkmcry\"\n    if not fig_kwargs:\n        fig_kwargs = {\"sharex\": True, \"figsize\": (5, n_row * 1.2)}\n\n    f, axes = plt.subplots(n_row, 1, **fig_kwargs)\n    i_ax = 0\n    # ob\n    ax = axes[i_ax]\n    i_ax += 1\n    if ob_traces:\n        if len(ob_traces) != ob.shape[1]:\n            msg = f\"Please provide label for each of the {ob.shape[1]} traces in the observations.\"\n            raise ValueError(msg)\n        yticks = []\n        for ind_tr, tr in enumerate(ob_traces):\n            ax.plot(ob[:, ind_tr], label=tr)\n            yticks.append(np.mean(ob[:, ind_tr]))\n        if legend:\n            ax.legend()\n        ax.set_xlim([-0.5, len(steps) - 0.5])\n        ax.set_yticks(yticks)\n        ax.set_yticklabels(ob_traces)\n    else:\n        ax.imshow(ob.T, aspect=\"auto\", origin=\"lower\")\n        if env and hasattr(env.observation_space, \"name\"):\n            # Plot environment annotation\n            yticks = []\n            yticklabels = []\n            for key, val in env.observation_space.name.items():\n                yticks.append((np.min(val) + np.max(val)) / 2)\n                yticklabels.append(key)\n            ax.set_yticks(yticks)\n            ax.set_yticklabels(yticklabels)\n        else:\n            ax.set_yticks([])\n        ax.spines[\"top\"].set_visible(False)\n        ax.spines[\"bottom\"].set_visible(False)\n        ax.spines[\"right\"].set_visible(False)\n\n    if name:\n        ax.set_title(f\"{name} env\")\n    ax.set_ylabel(\"Obs.\")\n    ax.set_xticks([])\n    # actions\n    ax = axes[i_ax]\n    i_ax += 1\n    if len(actions.shape) &gt; 1:\n        # Changes not implemented yet\n        ax.plot(steps, actions, marker=\"+\", label=\"Actions\")\n    else:\n        ax.plot(steps, actions, marker=\"+\", label=\"Actions\")\n    if gt is not None:\n        gt = np.array(gt)\n        if len(gt.shape) &gt; 1:\n            for ind_gt in range(gt.shape[1]):\n                ax.plot(\n                    steps,\n                    gt[:, ind_gt],\n                    f\"--{gt_colors[ind_gt]}\",\n                    label=f\"Ground truth {ind_gt}\",\n                )\n        else:\n            ax.plot(steps, gt, f\"--{gt_colors[0]}\", label=\"Ground truth\")\n    ax.set_xlim([-0.5, len(steps) - 0.5])\n    ax.set_ylabel(\"Act.\")\n    ax.spines[\"top\"].set_visible(False)\n    ax.spines[\"right\"].set_visible(False)\n    if legend:\n        ax.legend()\n    if env and hasattr(env.action_space, \"name\"):\n        # Plot environment annotation\n        yticks = []\n        yticklabels = []\n        for key, val in env.action_space.name.items():\n            yticks.append((np.min(val) + np.max(val)) / 2)\n            yticklabels.append(key)\n        ax.set_yticks(yticks)\n        ax.set_yticklabels(yticklabels)\n    if n_row &gt; 2:\n        ax.set_xticks([])\n    # rewards\n    if rewards is not None:\n        ax = axes[i_ax]\n        i_ax += 1\n        ax.plot(steps, rewards, \"r\", label=\"Rewards\")\n        ax.set_ylabel(\"Rew.\")\n        ax.spines[\"top\"].set_visible(False)\n        ax.spines[\"right\"].set_visible(False)\n        if legend:\n            ax.legend()\n        ax.set_xlim([-0.5, len(steps) - 0.5])\n\n        if env and hasattr(env, \"rewards\") and env.rewards is not None:\n            # Plot environment annotation\n            yticks = []\n            yticklabels = []\n\n            if isinstance(env.rewards, dict):\n                for key, val in env.rewards.items():\n                    yticks.append(val)\n                    yticklabels.append(f\"{key[:4]} {val:0.2f}\")\n            else:\n                for val in env.rewards:\n                    yticks.append(val)\n                    yticklabels.append(f\"{val:0.2f}\")\n\n            ax.set_yticks(yticks)\n            ax.set_yticklabels(yticklabels)\n    if n_row &gt; 3:\n        ax.set_xticks([])\n    # performance\n    if performance is not None:\n        ax = axes[i_ax]\n        i_ax += 1\n        ax.plot(steps, performance, \"k\", label=\"Performance\")\n        ax.set_ylabel(\"Performance\")\n        performance = np.array(performance)\n        mean_perf = np.mean(performance[performance != -1])\n        ax.set_title(f\"Mean performance: {np.round(mean_perf, 2)}\")\n        ax.spines[\"top\"].set_visible(False)\n        ax.spines[\"right\"].set_visible(False)\n        if legend:\n            ax.legend()\n        ax.set_xlim([-0.5, len(steps) - 0.5])\n\n    # states\n    if states is not None:\n        ax.set_xticks([])\n        ax = axes[i_ax]\n        i_ax += 1\n        plt.imshow(states[:, int(states.shape[1] / 2) :].T, aspect=\"auto\")\n        ax.set_title(\"Activity\")\n        ax.set_ylabel(\"Neurons\")\n        ax.spines[\"top\"].set_visible(False)\n        ax.spines[\"right\"].set_visible(False)\n\n    ax.set_xlabel(\"Steps\")\n    plt.tight_layout()\n    if fname:\n        fname = str(fname)\n        if not (fname.endswith((\".png\", \".svg\"))):\n            fname += \".png\"\n        f.savefig(fname, dpi=300)\n        plt.close(f)\n    return f\n</code></pre>"},{"location":"api/utils/#neurogym.utils.plotting.plot_env_3dbox","title":"plot_env_3dbox","text":"<pre><code>plot_env_3dbox(ob, fname='', env=None) -&gt; None\n</code></pre> <p>Plot environment with 3-D Box observation space.</p> Source code in <code>neurogym/utils/plotting.py</code> <pre><code>def plot_env_3dbox(ob, fname=\"\", env=None) -&gt; None:\n    \"\"\"Plot environment with 3-D Box observation space.\"\"\"\n    ob = ob.astype(np.uint8)  # TODO: Temporary\n    fig = plt.figure()\n    ax = fig.add_axes((0.1, 0.1, 0.8, 0.8))\n    ax.axis(\"off\")\n    im = ax.imshow(ob[0], animated=True)\n\n    def animate(i, *args, **kwargs):\n        im.set_array(ob[i])\n        return (im,)\n\n    interval = env.dt if env is not None else 50\n    ani = animation.FuncAnimation(fig, animate, frames=ob.shape[0], interval=interval)\n    if fname:\n        writer = animation.writers[\"ffmpeg\"](fps=int(1000 / interval))\n        fname = str(fname)\n        if not fname.endswith(\".mp4\"):\n            fname += \".mp4\"\n        ani.save(fname, writer=writer, dpi=300)\n</code></pre>"},{"location":"api/utils/#neurogym.utils.random","title":"random","text":""},{"location":"api/utils/#neurogym.utils.random.TruncExp","title":"TruncExp","text":"<pre><code>TruncExp(vmean, vmin=0, vmax=inf)\n</code></pre> Source code in <code>neurogym/utils/random.py</code> <pre><code>def __init__(self, vmean, vmin=0, vmax=np.inf) -&gt; None:\n    self.vmean = vmean\n    self.vmin = vmin\n    self.vmax = vmax\n    self.rng = np.random.RandomState()\n</code></pre>"},{"location":"api/utils/#neurogym.utils.random.TruncExp.seed","title":"seed","text":"<pre><code>seed(seed=None) -&gt; None\n</code></pre> <p>Seed the PRNG of this space.</p> Source code in <code>neurogym/utils/random.py</code> <pre><code>def seed(self, seed=None) -&gt; None:\n    \"\"\"Seed the PRNG of this space.\"\"\"\n    self.rng = np.random.RandomState(seed)\n</code></pre>"},{"location":"api/utils/#neurogym.utils.random.trunc_exp","title":"trunc_exp","text":"<pre><code>trunc_exp(rng, vmean, vmin=0, vmax=inf)\n</code></pre> <p>Function for generating period durations.</p> Source code in <code>neurogym/utils/random.py</code> <pre><code>def trunc_exp(rng, vmean, vmin=0, vmax=np.inf):\n    \"\"\"Function for generating period durations.\"\"\"\n    if vmin &gt;= vmax:  # the &gt; is to avoid issues when making vmin as big as dt\n        return vmax\n    while True:\n        x = rng.exponential(vmean)\n        if vmin &lt;= x &lt; vmax:\n            return x\n</code></pre>"},{"location":"api/utils/#neurogym.utils.random.random_number_fn","title":"random_number_fn","text":"<pre><code>random_number_fn(dist, args, rng)\n</code></pre> <p>Return a random number generating function from a distribution.</p> Source code in <code>neurogym/utils/random.py</code> <pre><code>def random_number_fn(dist, args, rng):\n    \"\"\"Return a random number generating function from a distribution.\"\"\"\n    if dist == \"uniform\":\n        return lambda: rng.uniform(*args)\n    if dist == \"choice\":\n        return lambda: rng.choice(args)\n    if dist == \"truncated_exponential\":\n        return lambda: trunc_exp(rng, *args)\n    if dist == \"constant\":\n        return lambda: args\n    msg = f\"Unknown distribution: {dist}.\"\n    raise ValueError(msg)\n</code></pre>"},{"location":"api/utils/#neurogym.utils.random.random_number_name","title":"random_number_name","text":"<pre><code>random_number_name(dist, args)\n</code></pre> <p>Return a string explaining the dist and args.</p> Source code in <code>neurogym/utils/random.py</code> <pre><code>def random_number_name(dist, args):\n    \"\"\"Return a string explaining the dist and args.\"\"\"\n    if dist == \"uniform\":\n        return f\"{dist} between {args[0]} and {args[1]}\"\n    if dist == \"choice\":\n        return f\"{dist} within {args}\"\n    if dist == \"truncated_exponential\":\n        string = f\"truncated exponential with mean {args[0]}\"\n        if len(args) &gt; 1:\n            string += f\", min {args[1]}\"\n        if len(args) &gt; 2:\n            string += f\", max {args[2]}\"\n        return string\n    if dist == \"constant\":\n        return f\"dist{args}\"\n    msg = f\"Unknown distribution: {dist}.\"\n    raise ValueError(msg)\n</code></pre>"},{"location":"api/utils/#neurogym.utils.scheduler","title":"scheduler","text":"<p>Trial scheduler class.</p>"},{"location":"api/utils/#neurogym.utils.scheduler.BaseSchedule","title":"BaseSchedule","text":"<pre><code>BaseSchedule(n)\n</code></pre> <p>Base schedule.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <p>int, number of conditions to schedule</p> required Source code in <code>neurogym/utils/scheduler.py</code> <pre><code>def __init__(self, n) -&gt; None:\n    self.n = n\n    self.total_count = 0  # total count\n    self.count = 0  # count within a condition\n    self.i = 0  # initialize at 0\n    self.rng = np.random.RandomState()\n</code></pre>"},{"location":"api/utils/#neurogym.utils.scheduler.SequentialSchedule","title":"SequentialSchedule","text":"<pre><code>SequentialSchedule(n)\n</code></pre> <p>               Bases: <code>BaseSchedule</code></p> <p>Sequential schedules.</p> Source code in <code>neurogym/utils/scheduler.py</code> <pre><code>def __init__(self, n) -&gt; None:\n    super().__init__(n)\n</code></pre>"},{"location":"api/utils/#neurogym.utils.scheduler.RandomSchedule","title":"RandomSchedule","text":"<pre><code>RandomSchedule(n)\n</code></pre> <p>               Bases: <code>BaseSchedule</code></p> <p>Random schedules.</p> Source code in <code>neurogym/utils/scheduler.py</code> <pre><code>def __init__(self, n) -&gt; None:\n    super().__init__(n)\n</code></pre>"},{"location":"api/utils/#neurogym.utils.scheduler.SequentialBlockSchedule","title":"SequentialBlockSchedule","text":"<pre><code>SequentialBlockSchedule(n, block_lens)\n</code></pre> <p>               Bases: <code>BaseSchedule</code></p> <p>Sequential block schedules.</p> Source code in <code>neurogym/utils/scheduler.py</code> <pre><code>def __init__(self, n, block_lens) -&gt; None:\n    super().__init__(n)\n    self.block_lens = block_lens\n    if len(block_lens) != n:\n        msg = f\"{len(block_lens)=} must be equal to {n=}.\"\n        raise ValueError(msg)\n</code></pre>"},{"location":"api/utils/#neurogym.utils.scheduler.RandomBlockSchedule","title":"RandomBlockSchedule","text":"<pre><code>RandomBlockSchedule(n, block_lens)\n</code></pre> <p>               Bases: <code>BaseSchedule</code></p> <p>Random block schedules.</p> Source code in <code>neurogym/utils/scheduler.py</code> <pre><code>def __init__(self, n, block_lens) -&gt; None:\n    super().__init__(n)\n    self.block_lens = block_lens\n    if len(block_lens) != n:\n        msg = f\"{len(block_lens)=} must be equal to {n=}.\"\n        raise ValueError(msg)\n</code></pre>"},{"location":"api/utils/#neurogym.utils.spaces","title":"spaces","text":""},{"location":"api/utils/#neurogym.utils.spaces.Box","title":"Box","text":"<pre><code>Box(low, high, name=None, **kwargs)\n</code></pre> <p>               Bases: <code>Box</code></p> <p>Thin wrapper of gymnasium.spaces.Box.</p> <p>Allow the user to give names to each dimension of the Box.</p> <p>Parameters:</p> Name Type Description Default <code>low,</code> <code>(high, kwargs)</code> <p>see gymnasium.spaces.Box</p> required <code>name</code> <p>dict describing the name of different dimensions</p> <code>None</code> Example usage <p>observation_space = Box(low=0, high=1,                         name={'fixation': 0, 'stimulus': [1, 2]})</p> Source code in <code>neurogym/utils/spaces.py</code> <pre><code>def __init__(self, low, high, name=None, **kwargs) -&gt; None:\n    super().__init__(low, high, **kwargs)\n    if isinstance(name, dict):\n        self.name = name\n    elif name is not None:\n        msg = f\"{type(name)=} must be `dict` or `NoneType`.\"\n        raise TypeError(msg)\n</code></pre>"},{"location":"api/utils/#neurogym.utils.spaces.Discrete","title":"Discrete","text":"<pre><code>Discrete(n, name=None, **kwargs)\n</code></pre> <p>               Bases: <code>Discrete</code></p> <p>Thin wrapper of gymnasium.spaces.Discrete.</p> <p>Allow the user to give names to each dimension of the Discrete space.</p> <p>Parameters:</p> Name Type Description Default <code>low,</code> <code>(high, kwargs)</code> <p>see gymnasium.spaces.Box</p> required <code>name</code> <p>dict describing the name of different dimensions</p> <code>None</code> Example usage <p>observation_space = Discrete(n=3, name={'fixation': 0, 'stimulus': [1, 2]})</p> Source code in <code>neurogym/utils/spaces.py</code> <pre><code>def __init__(self, n, name=None, **kwargs) -&gt; None:\n    super().__init__(n)\n    if isinstance(name, dict):\n        self.name = name\n    elif name is not None:\n        msg = f\"{type(name)=} must be `dict` or `NoneType`.\"\n        raise TypeError(msg)\n</code></pre>"},{"location":"api/utils/#neurogym.utils.tasktools","title":"tasktools","text":""},{"location":"api/utils/#neurogym.utils.tasktools.to_map","title":"to_map","text":"<pre><code>to_map(*args)\n</code></pre> <p>Produces ordered dict from given inputs.</p> Source code in <code>neurogym/utils/tasktools.py</code> <pre><code>def to_map(*args):\n    \"\"\"Produces ordered dict from given inputs.\"\"\"\n    var_list = args[0] if isinstance(args[0], list) else args\n    od = OrderedDict()\n    for i, v in enumerate(var_list):\n        od[v] = i\n\n    return od\n</code></pre>"},{"location":"api/utils/#neurogym.utils.tasktools.get_idx","title":"get_idx","text":"<pre><code>get_idx(t, start_end)\n</code></pre> <p>Auxiliary function for defining task periods.</p> Source code in <code>neurogym/utils/tasktools.py</code> <pre><code>def get_idx(t, start_end):\n    \"\"\"Auxiliary function for defining task periods.\"\"\"\n    start, end = start_end\n    return list(np.where((start &lt;= t) &amp; (t &lt; end))[0])\n</code></pre>"},{"location":"api/utils/#neurogym.utils.tasktools.get_periods_idx","title":"get_periods_idx","text":"<pre><code>get_periods_idx(dt, periods)\n</code></pre> <p>Function for defining task periods.</p> Source code in <code>neurogym/utils/tasktools.py</code> <pre><code>def get_periods_idx(dt, periods):\n    \"\"\"Function for defining task periods.\"\"\"\n    t = np.linspace(0, periods[\"tmax\"], int(periods[\"tmax\"] / dt) + 1)\n\n    return t, {k: get_idx(t, v) for k, v in periods.items() if k != \"tmax\"}\n</code></pre>"},{"location":"api/utils/#neurogym.utils.tasktools.minmax_number","title":"minmax_number","text":"<pre><code>minmax_number(dist, args)\n</code></pre> <p>Given input to the random_number_fn function, return min and max.</p> Source code in <code>neurogym/utils/tasktools.py</code> <pre><code>def minmax_number(dist, args):\n    \"\"\"Given input to the random_number_fn function, return min and max.\"\"\"\n    if dist == \"uniform\":\n        return args[0], args[1]\n    if dist == \"choice\":\n        return np.min(args), np.max(args)\n    if dist == \"truncated_exponential\":\n        return args[1], args[2]\n    if dist == \"constant\":\n        return args, args\n    msg = f\"Unknown distribution: {dist}.\"\n    raise ValueError(msg)\n</code></pre>"},{"location":"api/utils/#neurogym.utils.tasktools.circular_dist","title":"circular_dist","text":"<pre><code>circular_dist(original_dist)\n</code></pre> <p>Get the distance in periodic boundary conditions.</p> Source code in <code>neurogym/utils/tasktools.py</code> <pre><code>def circular_dist(original_dist):\n    \"\"\"Get the distance in periodic boundary conditions.\"\"\"\n    return np.minimum(abs(original_dist), 2 * np.pi - abs(original_dist))\n</code></pre>"},{"location":"api/utils/#neurogym.utils.tasktools.correct_2AFC","title":"correct_2AFC","text":"<pre><code>correct_2AFC(perf)\n</code></pre> <p>Computes performance.</p> Source code in <code>neurogym/utils/tasktools.py</code> <pre><code>def correct_2AFC(perf):  # noqa: N802\n    \"\"\"Computes performance.\"\"\"\n    p_decision = perf.n_decision / perf.n_trials\n    p_correct = divide(perf.n_correct, perf.n_decision)\n\n    return p_decision, p_correct\n</code></pre>"},{"location":"api/wrappers/","title":"Wrappers","text":""},{"location":"api/wrappers/#neurogym.wrappers.block","title":"block","text":""},{"location":"api/wrappers/#neurogym.wrappers.block.ScheduleAttr","title":"ScheduleAttr","text":"<pre><code>ScheduleAttr(env, schedule, attr_list)\n</code></pre> <p>               Bases: <code>TrialWrapper</code></p> <p>Schedule attributes.</p> <p>Parameters:</p> Name Type Description Default <code>env</code> <p>TrialEnv object</p> required <code>schedule</code> required Source code in <code>neurogym/wrappers/block.py</code> <pre><code>def __init__(self, env, schedule, attr_list) -&gt; None:\n    super().__init__(env)\n    self.schedule = schedule\n    self.attr_list = attr_list\n</code></pre>"},{"location":"api/wrappers/#neurogym.wrappers.block.MultiEnvs","title":"MultiEnvs","text":"<pre><code>MultiEnvs(envs, env_input=False)\n</code></pre> <p>               Bases: <code>TrialWrapper</code></p> <p>Wrap multiple environments.</p> <p>Parameters:</p> Name Type Description Default <code>envs</code> <p>list of env object</p> required <code>env_input</code> <p>bool, if True, add scalar inputs indicating current envinronment. default False.</p> <code>False</code> Source code in <code>neurogym/wrappers/block.py</code> <pre><code>def __init__(self, envs, env_input=False) -&gt; None:\n    super().__init__(envs[0])\n    for env in envs:\n        env.unwrapped.set_top(self)\n    self.envs = envs\n    self.i_env = 0\n\n    self.env_input = env_input\n    if env_input:\n        env_shape = envs[0].observation_space.shape\n        if len(env_shape) &gt; 1:\n            msg = f\"Env must have 1-D Box shape but got {env_shape}.\"\n            raise ValueError(msg)\n        _have_equal_shape(envs)\n        self.observation_space: spaces.Box = spaces.Box(\n            -np.inf,\n            np.inf,\n            shape=(env_shape[0] + len(self.envs),),\n            dtype=self.envs[0].observation_space.dtype,\n        )\n</code></pre>"},{"location":"api/wrappers/#neurogym.wrappers.block.MultiEnvs.set_i","title":"set_i","text":"<pre><code>set_i(i) -&gt; None\n</code></pre> <p>Set the i-th environment.</p> Source code in <code>neurogym/wrappers/block.py</code> <pre><code>def set_i(self, i) -&gt; None:\n    \"\"\"Set the i-th environment.\"\"\"\n    self.i_env = i\n    self.env = self.envs[self.i_env]\n</code></pre>"},{"location":"api/wrappers/#neurogym.wrappers.block.ScheduleEnvs","title":"ScheduleEnvs","text":"<pre><code>ScheduleEnvs(envs, schedule, env_input=False)\n</code></pre> <p>               Bases: <code>TrialWrapper</code></p> <p>Schedule environments.</p> <p>Parameters:</p> Name Type Description Default <code>envs</code> <p>list of env object</p> required <code>schedule</code> <p>utils.scheduler.BaseSchedule object</p> required <code>env_input</code> <p>bool, if True, add scalar inputs indicating current environment. default False.</p> <code>False</code> Source code in <code>neurogym/wrappers/block.py</code> <pre><code>def __init__(self, envs, schedule, env_input=False) -&gt; None:\n    super().__init__(envs[0])\n    for env in envs:\n        env.unwrapped.set_top(self)\n    self.envs = envs\n    self.schedule = schedule\n    self.i_env = self.next_i_env = 0\n\n    self.env_input = env_input\n    if env_input:\n        env_shape = envs[0].observation_space.shape\n        if len(env_shape) &gt; 1:\n            msg = f\"Env must have 1-D Box shape but got {env_shape}.\"\n            raise ValueError(msg)\n        _have_equal_shape(envs)\n        self.observation_space: spaces.Box = spaces.Box(\n            -np.inf,\n            np.inf,\n            shape=(env_shape[0] + len(self.envs),),\n            dtype=np.float32,\n        )\n</code></pre>"},{"location":"api/wrappers/#neurogym.wrappers.block.ScheduleEnvs.reset","title":"reset","text":"<pre><code>reset(**kwargs)\n</code></pre> <p>Resets environments.</p> <p>Reset each environment in self.envs and use the scheduler to select the environment returning the initial observation. This environment is also used to set the current environment self.env.</p> Source code in <code>neurogym/wrappers/block.py</code> <pre><code>def reset(self, **kwargs):\n    # TODO: kwargs to specify the condition for new_trial\n    \"\"\"Resets environments.\n\n    Reset each environment in self.envs and use the scheduler to select the environment returning\n    the initial observation. This environment is also used to set the current environment self.env.\n    \"\"\"\n    self.schedule.reset()\n    return_i_env = self.schedule()\n\n    # first reset all the env excepted return_i_env\n    for i, env in enumerate(self.envs):\n        if i == return_i_env:\n            continue\n\n        # change the current env so that calling _top.new_trial() in env.reset() will generate a trial for the env\n        # being currently reset (and not an env that is not yet reset)\n        self.set_i(i)\n        # same env used here and in the first call to new_trial()\n        self.next_i_env = self.i_env\n\n        env.reset(**kwargs)\n\n    # then reset return_i_env and return the result\n    self.set_i(return_i_env)\n    self.next_i_env = self.i_env\n    return self.env.reset()\n</code></pre>"},{"location":"api/wrappers/#neurogym.wrappers.block.ScheduleEnvs.set_i","title":"set_i","text":"<pre><code>set_i(i) -&gt; None\n</code></pre> <p>Set the current environment to the i-th environment in the list envs.</p> Source code in <code>neurogym/wrappers/block.py</code> <pre><code>def set_i(self, i) -&gt; None:\n    \"\"\"Set the current environment to the i-th environment in the list envs.\"\"\"\n    self.i_env = i\n    self.env = self.envs[self.i_env]\n    self.schedule.i = i\n</code></pre>"},{"location":"api/wrappers/#neurogym.wrappers.block.TrialHistoryV2","title":"TrialHistoryV2","text":"<pre><code>TrialHistoryV2(env, probs=None)\n</code></pre> <p>               Bases: <code>TrialWrapper</code></p> <p>Change ground truth probability based on previous outcome.</p> <p>Parameters:</p> Name Type Description Default <code>probs</code> <p>matrix of probabilities of the current choice conditioned on the previous. Shape, num-choices x num-choices</p> <code>None</code> Source code in <code>neurogym/wrappers/block.py</code> <pre><code>def __init__(self, env, probs=None) -&gt; None:\n    super().__init__(env)\n    try:\n        self.n_ch = len(self.choices)  # max num of choices\n    except AttributeError as e:\n        msg = \"TrialHistory requires task to have attribute choices.\"\n        raise AttributeError(msg) from e\n    if probs is None:\n        probs = np.ones((self.n_ch, self.n_ch)) / self.n_ch  # uniform\n    self.probs = probs\n    if self.probs.shape != (self.n_ch, self.n_ch):\n        msg = f\"{self.probs.shape=} should be {self.n_ch, self.n_ch=}.\"\n        raise ValueError(msg)\n    self.prev_trial = self.rng.choice(self.n_ch)  # random initialization\n</code></pre>"},{"location":"api/wrappers/#neurogym.wrappers.monitor","title":"monitor","text":""},{"location":"api/wrappers/#neurogym.wrappers.monitor.Monitor","title":"Monitor","text":"<pre><code>Monitor(env, folder=None, sv_per=100000, sv_stp='trial', verbose=False, sv_fig=False, num_stps_sv_fig=100, name='', fig_type='png', step_fn=None)\n</code></pre> <p>               Bases: <code>Wrapper</code></p> <p>Monitor task.</p> <p>Saves relevant behavioral information: rewards, actions, observations, new trial, ground truth.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <p>Folder where the data will be saved. (def: None, str) sv_per and sv_stp: Data will be saved every sv_per sv_stp's. (def: 100000, int)</p> <code>None</code> <code>verbose</code> <p>Whether to print information about average reward and number of trials. (def: False, bool)</p> <code>False</code> <code>sv_fig</code> <p>Whether to save a figure of the experiment structure. If True, a figure will be updated every sv_per. (def: False, bool)</p> <code>False</code> <code>num_stps_sv_fig</code> <p>Number of trial steps to include in the figure. (def: 100, int)</p> <code>100</code> Source code in <code>neurogym/wrappers/monitor.py</code> <pre><code>def __init__(\n    self,\n    env,\n    folder=None,\n    sv_per=100000,\n    sv_stp=\"trial\",\n    verbose=False,\n    sv_fig=False,\n    num_stps_sv_fig=100,\n    name=\"\",\n    fig_type=\"png\",\n    step_fn=None,\n) -&gt; None:\n    super().__init__(env)\n    self.env = env\n    self.num_tr = 0\n    self.step_fn = step_fn\n    # data to save\n    self.data: dict[str, list] = {\"action\": [], \"reward\": []}\n    self.sv_per = sv_per\n    self.sv_stp = sv_stp\n    self.fig_type = fig_type\n    if self.sv_stp == \"timestep\":\n        self.t = 0\n    self.verbose = verbose\n    if folder is None:\n        # FIXME is it ok to use tempfile.TemporaryDirectory instead or does this need to be stored locally always?\n        self.folder = \"tmp\"\n    Path(self.folder).mkdir(parents=True, exist_ok=True)\n    # seeding\n    self.sv_name = self.folder + self.env.__class__.__name__ + \"_bhvr_data_\" + name + \"_\"  # FIXME: use pathlib\n    # figure\n    self.sv_fig = sv_fig\n    if self.sv_fig:\n        self.num_stps_sv_fig = num_stps_sv_fig\n        self.stp_counter = 0\n        self.ob_mat: list = []\n        self.act_mat: list = []\n        self.rew_mat: list = []\n        self.gt_mat: list = []\n        self.perf_mat: list = []\n</code></pre>"},{"location":"api/wrappers/#neurogym.wrappers.noise","title":"noise","text":"<p>Noise wrapper.</p> <p>Created on Thu Feb 28 15:07:21 2019</p> <p>@author: molano</p>"},{"location":"api/wrappers/#neurogym.wrappers.noise.Noise","title":"Noise","text":"<pre><code>Noise(env, std_noise=0.1)\n</code></pre> <p>               Bases: <code>Wrapper</code></p> <p>Add Gaussian noise to the observations.</p> <p>Parameters:</p> Name Type Description Default <code>std_noise</code> <p>Standard deviation of noise. (def: 0.1)</p> <code>0.1</code> <code>perf_th</code> <p>If != None, the wrapper will adjust the noise so the mean performance is not larger than perf_th. (def: None, float)</p> required <code>w</code> <p>Window used to compute the mean performance. (def: 100, int)</p> required <code>step_noise</code> <p>Step used to increment/decrease std. (def: 0.001, float)</p> required Source code in <code>neurogym/wrappers/noise.py</code> <pre><code>def __init__(self, env, std_noise=0.1) -&gt; None:\n    super().__init__(env)\n    self.env = env\n    self.std_noise = std_noise\n</code></pre>"},{"location":"api/wrappers/#neurogym.wrappers.pass_action","title":"pass_action","text":""},{"location":"api/wrappers/#neurogym.wrappers.pass_action.PassAction","title":"PassAction","text":"<pre><code>PassAction(env)\n</code></pre> <p>               Bases: <code>Wrapper</code></p> <p>Modifies observation by adding the previous action.</p> Source code in <code>neurogym/wrappers/pass_action.py</code> <pre><code>def __init__(self, env) -&gt; None:\n    super().__init__(env)\n    self.env = env\n    # TODO: This is not adding one-hot\n    env_oss = env.observation_space.shape[0]\n    self.observation_space = spaces.Box(\n        -np.inf,\n        np.inf,\n        shape=(env_oss + 1,),\n        dtype=np.float32,\n    )\n</code></pre>"},{"location":"api/wrappers/#neurogym.wrappers.pass_reward","title":"pass_reward","text":""},{"location":"api/wrappers/#neurogym.wrappers.pass_reward.PassReward","title":"PassReward","text":"<pre><code>PassReward(env)\n</code></pre> <p>               Bases: <code>Wrapper</code></p> <p>Modifies observation by adding the previous reward.</p> Source code in <code>neurogym/wrappers/pass_reward.py</code> <pre><code>def __init__(self, env) -&gt; None:\n    \"\"\"Modifies observation by adding the previous reward.\"\"\"\n    super().__init__(env)\n    env_oss = env.observation_space.shape[0]\n    self.observation_space = spaces.Box(\n        -np.inf,\n        np.inf,\n        shape=(env_oss + 1,),\n        dtype=np.float32,\n    )\n</code></pre>"},{"location":"api/wrappers/#neurogym.wrappers.reaction_time","title":"reaction_time","text":"<p>Noise wrapper.</p> <p>Created on Thu Feb 28 15:07:21 2019</p> <p>@author: molano</p>"},{"location":"api/wrappers/#neurogym.wrappers.reaction_time.ReactionTime","title":"ReactionTime","text":"<pre><code>ReactionTime(env, urgency=0.0)\n</code></pre> <p>               Bases: <code>Wrapper</code></p> <p>Allow reaction time response.</p> <p>Modifies a given environment by allowing the network to act at any time after the fixation period.</p> Source code in <code>neurogym/wrappers/reaction_time.py</code> <pre><code>def __init__(self, env, urgency=0.0) -&gt; None:\n    super().__init__(env)\n    self.env = env\n    self.urgency = urgency\n    self.tr_dur = 0\n</code></pre>"},{"location":"api/wrappers/#neurogym.wrappers.side_bias","title":"side_bias","text":""},{"location":"api/wrappers/#neurogym.wrappers.side_bias.SideBias","title":"SideBias","text":"<pre><code>SideBias(env, probs=None, block_dur=200)\n</code></pre> <p>               Bases: <code>TrialWrapper</code></p> <p>Changes the probability of ground truth.</p> <p>Parameters:</p> Name Type Description Default <code>prob</code> <p>Specifies probabilities for each choice. Within each block,the probability should sum up to 1. (def: None, numpy array (n_block, n_choices))</p> required <code>block_dur</code> <p>Number of trials per block. (def: 200, int)</p> <code>200</code> Source code in <code>neurogym/wrappers/side_bias.py</code> <pre><code>def __init__(self, env, probs=None, block_dur=200) -&gt; None:\n    super().__init__(env)\n    try:\n        self.choices = self.task.choices\n    except AttributeError as e:\n        msg = \"SideBias requires task to have attribute choices.\"\n        raise AttributeError(msg) from e\n    if not isinstance(self.task, ngym.TrialEnv):\n        msg = \"Task has to be TrialEnv.\"\n        raise TypeError(msg)\n    if probs is None:\n        msg = \"Please provide choices probabilities.\"\n        raise ValueError(msg)\n    if isinstance(probs, float | int):\n        mat = np.eye(len(self.choices)) * probs\n        mat[mat == 0] = 1 - probs\n        self.choice_prob = mat\n    else:\n        self.choice_prob = np.array(probs)\n    if self.choice_prob.shape[1] != len(self.choices):\n        msg = (\n            f\"The number of choices {self.choice_prob.shape[1]} inferred from prob mismatches \"\n            f\"{len(self.choices)} inferred from choices.\"\n        )\n        raise ValueError(msg)\n\n    self.n_block = self.choice_prob.shape[0]\n    self.curr_block = self.task.rng.choice(range(self.n_block))\n    self.block_dur = block_dur\n</code></pre>"},{"location":"examples/demo/","title":"Demo","text":"<p>NeuroGym is a comprehensive toolkit that allows training any network model on many established neuroscience tasks using Reinforcement Learning techniques. It includes working memory tasks, value-based decision tasks and context-dependent perceptual categorization tasks.</p> <p>In this notebook we first show how to install the relevant toolbox.</p> <p>We then show how to access the available tasks and their relevant information.</p> <p>Finally we train an LSTM network on the Random Dots Motion task using the A2C algorithm Mnih et al. 2016 implemented in the stable-baselines3 toolbox, and plot the results.</p> <p>You can easily change the code to train a network on any other available task or using a different algorithm (e.g. ACER, PPO2).</p> In\u00a0[6]: Copied! <pre># Install gymnasium\n! pip install gymnasium\n# Install neurogym\n! git clone https://github.com/gyyang/neurogym.git\n%cd neurogym/\n! pip install -e .\n# Install stable-baselines3\n! pip install stable-baselines3\n</pre> # Install gymnasium ! pip install gymnasium # Install neurogym ! git clone https://github.com/gyyang/neurogym.git %cd neurogym/ ! pip install -e . # Install stable-baselines3 ! pip install stable-baselines3 <pre>UsageError: Line magic function `%tensorflow_version` not found.\n</pre> In\u00a0[49]: Copied! <pre>import warnings\nimport gymnasium as gym\nimport neurogym as ngym\nfrom neurogym.utils import info, plotting\nwarnings.filterwarnings('ignore')\ninfo.all_tasks()\n</pre> import warnings import gymnasium as gym import neurogym as ngym from neurogym.utils import info, plotting warnings.filterwarnings('ignore') info.all_tasks() <pre>AntiReach-v0\nBandit-v0\nContextDecisionMaking-v0\nDawTwoStep-v0\nDelayComparison-v0\nDelayMatchCategory-v0\nDelayMatchSample-v0\nDelayMatchSampleDistractor1D-v0\nDelayPairedAssociation-v0\nDualDelayMatchSample-v0\nEconomicDecisionMaking-v0\nGoNogo-v0\nHierarchicalReasoning-v0\nIntervalDiscrimination-v0\nMotorTiming-v0\nMultiSensoryIntegration-v0\nNull-v0\nOneTwoThreeGo-v0\nPerceptualDecisionMaking-v0\nPerceptualDecisionMakingDelayResponse-v0\nPostDecisionWager-v0\nProbabilisticReasoning-v0\nPulseDecisionMaking-v0\nReaching1D-v0\nReaching1DWithSelfDistraction-v0\nReachingDelayResponse-v0\nReadySetGo-v0\nSingleContextDecisionMaking-v0\npsychopy.RandomDotMotion-v0\npsychopy.SpatialSuppressMotion-v0\npsychopy.VisualSearch-v0\n</pre> In\u00a0[50]: Copied! <pre>task = 'GoNogo-v0'\nenv = gym.make(task)\nprint(env)\n# plotting.plot_env(env, num_steps=300, def_act=0, ob_traces=['Fixation cue', 'Stim1', 'Stim2'], fig_kwargs={'figsize': (12, 12)})\nfig = plotting.plot_env(\n    env,\n    num_steps=100,\n    # def_act=0,\n    ob_traces=['Fixation cue', 'NoGo', 'Go'],\n    # fig_kwargs={'figsize': (12, 12)}\n    )\n</pre> task = 'GoNogo-v0' env = gym.make(task) print(env) # plotting.plot_env(env, num_steps=300, def_act=0, ob_traces=['Fixation cue', 'Stim1', 'Stim2'], fig_kwargs={'figsize': (12, 12)}) fig = plotting.plot_env(     env,     num_steps=100,     # def_act=0,     ob_traces=['Fixation cue', 'NoGo', 'Go'],     # fig_kwargs={'figsize': (12, 12)}     ) <pre>&lt;OrderEnforcing&lt;PassiveEnvChecker&lt;GoNogo&gt;&gt;&gt;\n</pre> In\u00a0[51]: Copied! <pre>info.all_wrappers()\n</pre> info.all_wrappers() <pre>Monitor-v0\nNoise-v0\nPassAction-v0\nPassReward-v0\nRandomGroundTruth-v0\nReactionTime-v0\nScheduleAttr-v0\nScheduleEnvs-v0\nSideBias-v0\nTrialHistoryV2-v0\n</pre> In\u00a0[52]: Copied! <pre>info.info_wrapper('TrialHistoryV2-v0', show_code=True)\n</pre> info.info_wrapper('TrialHistoryV2-v0', show_code=True) Out[52]: <pre>'### TrialHistoryV2-v0\\n\\nLogic: Missing description\\n\\n\\n#### Source code #### \\n\\nclass TrialHistoryV2(TrialWrapper):\\n    \"\"\"Change ground truth probability based on previous outcome.\\n\\n    Args:\\n        probs: matrix of probabilities of the current choice conditioned\\n            on the previous. Shape, num-choices x num-choices\\n    \"\"\"\\n\\n    def __init__(self, env, probs=None):\\n        super().__init__(env)\\n        try:\\n            self.n_ch = len(self.choices)  # max num of choices\\n        except AttributeError:\\n            raise AttributeError(\\n                \"TrialHistory requires task to \" \"have attribute choices\"\\n            )\\n        if probs is None:\\n            probs = np.ones((self.n_ch, self.n_ch)) / self.n_ch  # uniform\\n        self.probs = probs\\n        assert self.probs.shape == (self.n_ch, self.n_ch), (\\n            \"probs shape wrong, should be\" + str((self.n_ch, self.n_ch))\\n        )\\n        self.prev_trial = self.rng.choice(self.n_ch)  # random initialization\\n\\n    def new_trial(self, **kwargs):\\n        if \"probs\" in kwargs:\\n            probs = kwargs[\"probs\"]\\n        else:\\n            probs = self.probs\\n        p = probs[self.prev_trial, :]\\n        # Choose ground truth and update previous trial info\\n        self.prev_trial = self.rng.choice(self.n_ch, p=p)\\n        ground_truth = self.choices[self.prev_trial]\\n        kwargs.update({\"ground_truth\": ground_truth, \"probs\": probs})\\n        return self.env.new_trial(**kwargs)\\n\\n\\n'</pre> In\u00a0[53]: Copied! <pre>import warnings\nimport numpy as np\nfrom neurogym.wrappers import monitor, TrialHistoryV2\nfrom stable_baselines3.common.vec_env import DummyVecEnv\nfrom stable_baselines3 import A2C  # ACER, PPO2\nwarnings.filterwarnings('default')\n# task paremters\ntiming = {'fixation': ('constant', 300),\n          'stimulus': ('constant', 700),\n          'decision': ('constant', 300)}\nkwargs = {'dt': 100, 'timing': timing}\n# wrapper parameters\nn_ch = 2\np = 0.8\nnum_blocks = 2\nprobs = np.array([[p, 1-p], [1-p, p]])  # repeating block\n\n# build task\nenv = gym.make(task, **kwargs)\n# Apply the wrapper\nenv = TrialHistoryV2(env, probs=probs)\nenv = monitor.Monitor(env, folder='content/tests/', sv_per=10000, verbose=1, sv_fig=True, num_stps_sv_fig=100)\n# the env is now wrapped automatically when passing it to the constructor\nenv = DummyVecEnv([lambda: env])\nmodel = A2C(\"MlpPolicy\", env, verbose=1, policy_kwargs={'net_arch': [64, 64]})\nmodel.learn(total_timesteps=500000, log_interval=100000)\nenv.close()\n</pre> import warnings import numpy as np from neurogym.wrappers import monitor, TrialHistoryV2 from stable_baselines3.common.vec_env import DummyVecEnv from stable_baselines3 import A2C  # ACER, PPO2 warnings.filterwarnings('default') # task paremters timing = {'fixation': ('constant', 300),           'stimulus': ('constant', 700),           'decision': ('constant', 300)} kwargs = {'dt': 100, 'timing': timing} # wrapper parameters n_ch = 2 p = 0.8 num_blocks = 2 probs = np.array([[p, 1-p], [1-p, p]])  # repeating block  # build task env = gym.make(task, **kwargs) # Apply the wrapper env = TrialHistoryV2(env, probs=probs) env = monitor.Monitor(env, folder='content/tests/', sv_per=10000, verbose=1, sv_fig=True, num_stps_sv_fig=100) # the env is now wrapped automatically when passing it to the constructor env = DummyVecEnv([lambda: env]) model = A2C(\"MlpPolicy\", env, verbose=1, policy_kwargs={'net_arch': [64, 64]}) model.learn(total_timesteps=500000, log_interval=100000) env.close() <pre>/Users/giuliacrocioni/miniforge3/envs/neurogym/lib/python3.10/site-packages/gymnasium/envs/registration.py:481: UserWarning: WARN: The environment creator metadata doesn't include `render_modes`, contains: ['paper_link', 'paper_name', 'tags']\n  logger.warn(\n/Users/giuliacrocioni/miniforge3/envs/neurogym/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: WARN: env.choices to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.choices` for environment variables or `env.get_wrapper_attr('choices')` that will search the reminding wrappers.\n  logger.warn(\n/Users/giuliacrocioni/miniforge3/envs/neurogym/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: WARN: env.rng to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.rng` for environment variables or `env.get_wrapper_attr('rng')` that will search the reminding wrappers.\n  logger.warn(\n/Users/giuliacrocioni/miniforge3/envs/neurogym/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: WARN: env.new_trial to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.new_trial` for environment variables or `env.get_wrapper_attr('new_trial')` that will search the reminding wrappers.\n  logger.warn(\n</pre> <pre>Using cpu device\n--------------------\nNumber of steps:  10000.0\nAverage reward:  0.27335\n--------------------\n--------------------\nNumber of steps:  20000.0\nAverage reward:  0.2262\n--------------------\n--------------------\nNumber of steps:  30000.0\nAverage reward:  0.20145\n--------------------\n-------------------------------------\n| time/                 |           |\n|    fps                | 2300      |\n|    iterations         | 100000    |\n|    time_elapsed       | 217       |\n|    total_timesteps    | 500000    |\n| train/                |           |\n|    entropy_loss       | -0.000595 |\n|    explained_variance | -6.48     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 99999     |\n|    policy_loss        | -1.45e-06 |\n|    value_loss         | 0.000583  |\n-------------------------------------\n</pre> In\u00a0[56]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n# Create task\nenv = gym.make(task, **kwargs)\n# Apply the wrapper\nenv = TrialHistoryV2(env, probs=probs)\nenv = DummyVecEnv([lambda: env])\nfig = plotting.plot_env(\n    env,\n    num_steps=100,\n    # def_act=0,\n    ob_traces=['Fixation cue', 'NoGo', 'Go'],\n    # fig_kwargs={'figsize': (12, 12)},\n    model=model)\n</pre> import numpy as np import matplotlib.pyplot as plt # Create task env = gym.make(task, **kwargs) # Apply the wrapper env = TrialHistoryV2(env, probs=probs) env = DummyVecEnv([lambda: env]) fig = plotting.plot_env(     env,     num_steps=100,     # def_act=0,     ob_traces=['Fixation cue', 'NoGo', 'Go'],     # fig_kwargs={'figsize': (12, 12)},     model=model) <pre>/Users/giuliacrocioni/miniforge3/envs/neurogym/lib/python3.10/site-packages/gymnasium/envs/registration.py:481: UserWarning: WARN: The environment creator metadata doesn't include `render_modes`, contains: ['paper_link', 'paper_name', 'tags']\n  logger.warn(\n/Users/giuliacrocioni/miniforge3/envs/neurogym/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: WARN: env.choices to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.choices` for environment variables or `env.get_wrapper_attr('choices')` that will search the reminding wrappers.\n  logger.warn(\n/Users/giuliacrocioni/miniforge3/envs/neurogym/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: WARN: env.rng to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.rng` for environment variables or `env.get_wrapper_attr('rng')` that will search the reminding wrappers.\n  logger.warn(\n/Users/giuliacrocioni/miniforge3/envs/neurogym/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: WARN: env.new_trial to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.new_trial` for environment variables or `env.get_wrapper_attr('new_trial')` that will search the reminding wrappers.\n  logger.warn(\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/demo/#exploring-neurogym-tasks","title":"Exploring NeuroGym tasks\u00b6","text":""},{"location":"examples/demo/#installation-on-google-colab","title":"Installation on google colab\u00b6","text":""},{"location":"examples/demo/#explore-tasks","title":"Explore tasks\u00b6","text":""},{"location":"examples/demo/#visualize-a-single-task","title":"Visualize a single task\u00b6","text":""},{"location":"examples/demo/#explore-wrappers","title":"Explore wrappers\u00b6","text":""},{"location":"examples/demo/#train-a-network","title":"Train a network\u00b6","text":""},{"location":"examples/demo/#visualize-results","title":"Visualize results\u00b6","text":""},{"location":"examples/example_annubes/","title":"Example annubes","text":"<p>This notebook is a simple example of how to use the <code>AnnubesEnv</code> class to create a custom environment and use it to train a reinforcement learning agent with <code>stable_baselines3</code>.</p> In\u00a0[\u00a0]: Copied! <pre>import neurogym as ngym\nfrom neurogym.envs.annubes import AnnubesEnv\nfrom stable_baselines3.common.env_checker import check_env\n\nenv = AnnubesEnv()\n\n# check the custom environment and output additional warnings (if any)\ncheck_env(env)\n\n# check the environment with a random agent\nobs, info = env.reset()\nn_steps = 10\nfor _ in range(n_steps):\n    # random action\n    action = env.action_space.sample()\n    obs, reward, terminated, truncated, info = env.step(action)\n    if terminated:\n        obs, info = env.reset()\n\nprint(env.timing)\nprint(\"----------------\")\nprint(env.observation_space)\nprint(env.observation_space.name)\nprint(\"----------------\")\nprint(env.action_space)\nprint(env.action_space.name)\n</pre> import neurogym as ngym from neurogym.envs.annubes import AnnubesEnv from stable_baselines3.common.env_checker import check_env  env = AnnubesEnv()  # check the custom environment and output additional warnings (if any) check_env(env)  # check the environment with a random agent obs, info = env.reset() n_steps = 10 for _ in range(n_steps):     # random action     action = env.action_space.sample()     obs, reward, terminated, truncated, info = env.step(action)     if terminated:         obs, info = env.reset()  print(env.timing) print(\"----------------\") print(env.observation_space) print(env.observation_space.name) print(\"----------------\") print(env.action_space) print(env.action_space.name) In\u00a0[\u00a0]: Copied! <pre>fig = ngym.utils.plot_env(\n    env,\n    ob_traces=[\"fixation\", \"start\", \"v\", \"a\"],\n    num_trials=10\n)\n</pre> fig = ngym.utils.plot_env(     env,     ob_traces=[\"fixation\", \"start\", \"v\", \"a\"],     num_trials=10 ) In\u00a0[\u00a0]: Copied! <pre>import warnings\n\nfrom stable_baselines3 import A2C\nfrom stable_baselines3.common.vec_env import DummyVecEnv\n\nwarnings.filterwarnings(\"default\")\n\n# train agent\nenv = AnnubesEnv()\nenv_vec = DummyVecEnv([lambda: env])\nmodel = A2C(\"MlpPolicy\", env_vec, verbose=0)\nmodel.learn(total_timesteps=20000, log_interval=1000)\nenv_vec.close()\n\n# plot example trials with trained agent\ndata = ngym.utils.plot_env(env, num_trials=10, ob_traces=[\"fixation\", \"start\", \"v\", \"a\"], model=model)\n</pre> import warnings  from stable_baselines3 import A2C from stable_baselines3.common.vec_env import DummyVecEnv  warnings.filterwarnings(\"default\")  # train agent env = AnnubesEnv() env_vec = DummyVecEnv([lambda: env]) model = A2C(\"MlpPolicy\", env_vec, verbose=0) model.learn(total_timesteps=20000, log_interval=1000) env_vec.close()  # plot example trials with trained agent data = ngym.utils.plot_env(env, num_trials=10, ob_traces=[\"fixation\", \"start\", \"v\", \"a\"], model=model) In\u00a0[\u00a0]: Copied! <pre>env1 = AnnubesEnv({\"v\": 1, \"a\": 0})\nenv1_vec = DummyVecEnv([lambda: env1])\n# create a model and train it with the first environment\nmodel = A2C(\"MlpPolicy\", env1_vec, verbose=0)\nmodel.learn(total_timesteps=10000)\nenv1_vec.close()\n\n# plot example trials with trained agent\ndata = ngym.utils.plot_env(env1, num_trials=10, ob_traces=[\"fixation\", \"start\", \"v\", \"a\"], model=model)\n</pre> env1 = AnnubesEnv({\"v\": 1, \"a\": 0}) env1_vec = DummyVecEnv([lambda: env1]) # create a model and train it with the first environment model = A2C(\"MlpPolicy\", env1_vec, verbose=0) model.learn(total_timesteps=10000) env1_vec.close()  # plot example trials with trained agent data = ngym.utils.plot_env(env1, num_trials=10, ob_traces=[\"fixation\", \"start\", \"v\", \"a\"], model=model) In\u00a0[\u00a0]: Copied! <pre># switch to the second environment and continue training\nenv2 = AnnubesEnv({\"v\": 0, \"a\": 1})\nenv2_vec = DummyVecEnv([lambda: env2])\n# set the model's environment to the new environment\nmodel.set_env(env2_vec)\nmodel.learn(total_timesteps=10000)\nenv2_vec.close()\n\n# plot example trials with trained agent\ndata = ngym.utils.plot_env(env2, num_trials=10, ob_traces=[\"fixation\", \"start\", \"v\", \"a\"], model=model)\n</pre> # switch to the second environment and continue training env2 = AnnubesEnv({\"v\": 0, \"a\": 1}) env2_vec = DummyVecEnv([lambda: env2]) # set the model's environment to the new environment model.set_env(env2_vec) model.learn(total_timesteps=10000) env2_vec.close()  # plot example trials with trained agent data = ngym.utils.plot_env(env2, num_trials=10, ob_traces=[\"fixation\", \"start\", \"v\", \"a\"], model=model) In\u00a0[\u00a0]: Copied! <pre># Switch to the third environment and finish training\nenv3 = AnnubesEnv({\"v\": 0.5, \"a\": 0.5})\nenv3_vec = DummyVecEnv([lambda: env3])\n# set the model's environment to the new environment\nmodel.set_env(env3_vec)\nmodel.learn(total_timesteps=20000)\nenv3_vec.close()\n\n# plot example trials with trained agent\ndata = ngym.utils.plot_env(env3, num_trials=10, ob_traces=[\"fixation\", \"start\", \"v\", \"a\"], model=model)\n</pre> # Switch to the third environment and finish training env3 = AnnubesEnv({\"v\": 0.5, \"a\": 0.5}) env3_vec = DummyVecEnv([lambda: env3]) # set the model's environment to the new environment model.set_env(env3_vec) model.learn(total_timesteps=20000) env3_vec.close()  # plot example trials with trained agent data = ngym.utils.plot_env(env3, num_trials=10, ob_traces=[\"fixation\", \"start\", \"v\", \"a\"], model=model) In\u00a0[\u00a0]: Copied! <pre># Save the final model after all training\nmodel.save(\"final_model\")\n</pre> # Save the final model after all training model.save(\"final_model\")"},{"location":"examples/example_annubes/#annubesenv-environment","title":"<code>AnnubesEnv</code> environment\u00b6","text":"<p>Let's create an environment, check it works and visualize it.</p>"},{"location":"examples/example_annubes/#training-annubesenv","title":"Training <code>AnnubesEnv</code>\u00b6","text":""},{"location":"examples/example_annubes/#1-regular-training","title":"1. Regular training\u00b6","text":"<p>We can train <code>AnnubesEnv</code> using one of the models defined in <code>stable_baselines3</code>, for example <code>A2C</code>.</p>"},{"location":"examples/example_annubes/#2-sequential-training","title":"2. Sequential training\u00b6","text":"<p>We can also train <code>AnnubesEnv</code> using a sequential training approach. This is useful when we want to train the agent in multiple stages, each with a different environment configuration. This can be useful for:</p> <ul> <li><p>Curriculum learning: Gradually increase the difficulty of the environments. Start with simpler tasks and progressively move to more complex ones, allowing the agent to build on its previous experiences.</p> </li> <li><p>Domain randomization: Vary the environment dynamics (e.g., physics, obstacles) during training to improve the agent's robustness to changes in the environment.</p> </li> <li><p>Transfer learning: If you have access to different agents or architectures, you can use transfer learning techniques to fine-tune the model on a new environment.</p> </li> </ul> <p>In this case it is important to include all the possible observations in each environment, even if not all of them are used. This is because the model is initialized with the first environment's observation space and it is not possible to change it later.</p>"},{"location":"examples/example_neurogym_keras/","title":"NeuroGym with Keras","text":"<p>NeuroGym is a comprehensive toolkit that allows training any network model on many established neuroscience tasks using Reinforcement Learning techniques. It includes working memory tasks, value-based decision tasks and context-dependent perceptual categorization tasks.</p> <p>In this notebook we first show how to install the relevant toolbox.</p> <p>We then show how to access the available tasks and their relevant information.</p> <p>Finally we train an LSTM network on the Random Dots Motion task using standard supervised learning techniques (with Keras), and plot the results.</p> In\u00a0[\u00a0]: Copied! <pre>%tensorflow_version 1.x\n# Install gymnasium\n! pip install gymnasium\n# Install neurogym\n! git clone https://github.com/gyyang/neurogym.git\n%cd neurogym/\n! pip install -e .\n</pre> %tensorflow_version 1.x # Install gymnasium ! pip install gymnasium # Install neurogym ! git clone https://github.com/gyyang/neurogym.git %cd neurogym/ ! pip install -e . In\u00a0[\u00a0]: Copied! <pre>import warnings\n\nimport numpy as np\nimport neurogym as ngym\n\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, LSTM, TimeDistributed, Input\n\nwarnings.filterwarnings('ignore')\nwarnings.filterwarnings('default')\n\n# Environment\ntask = 'PerceptualDecisionMaking-v0'\nkwargs = {'dt': 100}\nseq_len = 100\n\n# Make supervised dataset\ndataset = ngym.Dataset(task, env_kwargs=kwargs, batch_size=16,\n                       seq_len=seq_len)\nenv = dataset.env\nobs_size = env.observation_space.shape[0]\nact_size = env.action_space.n\n\n# Model\nnum_h = 64\n# from https://www.tensorflow.org/guide/keras/rnn\nxin = Input(batch_shape=(None, None, obs_size), dtype='float32')\nseq = LSTM(num_h, return_sequences=True, time_major=True)(xin)\nmlp = TimeDistributed(Dense(act_size, activation='softmax'))(seq)\nmodel = Model(inputs=xin, outputs=mlp)\nmodel.summary()\nmodel.compile(optimizer='Adam', loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\n# Train network\nsteps_per_epoch = 2000\ndata_generator = (dataset() for i in range(steps_per_epoch))\nhistory = model.fit(data_generator, steps_per_epoch=steps_per_epoch)\n</pre> import warnings  import numpy as np import neurogym as ngym  from tensorflow.keras.models import Model from tensorflow.keras.layers import Dense, LSTM, TimeDistributed, Input  warnings.filterwarnings('ignore') warnings.filterwarnings('default')  # Environment task = 'PerceptualDecisionMaking-v0' kwargs = {'dt': 100} seq_len = 100  # Make supervised dataset dataset = ngym.Dataset(task, env_kwargs=kwargs, batch_size=16,                        seq_len=seq_len) env = dataset.env obs_size = env.observation_space.shape[0] act_size = env.action_space.n  # Model num_h = 64 # from https://www.tensorflow.org/guide/keras/rnn xin = Input(batch_shape=(None, None, obs_size), dtype='float32') seq = LSTM(num_h, return_sequences=True, time_major=True)(xin) mlp = TimeDistributed(Dense(act_size, activation='softmax'))(seq) model = Model(inputs=xin, outputs=mlp) model.summary() model.compile(optimizer='Adam', loss='sparse_categorical_crossentropy',               metrics=['accuracy'])  # Train network steps_per_epoch = 2000 data_generator = (dataset() for i in range(steps_per_epoch)) history = model.fit(data_generator, steps_per_epoch=steps_per_epoch) <pre>/Users/gryang/tf1/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n/Users/gryang/tf1/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n/Users/gryang/tf1/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n/Users/gryang/tf1/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n/Users/gryang/tf1/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n/Users/gryang/tf1/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n</pre> <pre>WARNING:tensorflow:From /Users/gryang/tf1/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\nInstructions for updating:\nCall initializer instance with the dtype argument instead of passing it to the constructor\n</pre> <pre>/Users/gryang/tf1/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n/Users/gryang/tf1/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n/Users/gryang/tf1/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n/Users/gryang/tf1/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n/Users/gryang/tf1/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n/Users/gryang/tf1/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n/Users/gryang/Dropbox/Code/MyPython/neurogym/neurogym/core.py:253: UserWarning: Warning: Time for period fixation 100.000000  lasts only one timestep. Agents will not have time to respond (e.g. make a choice) on time.\n  ' time to respond (e.g. make a choice) on time.')\n/Users/gryang/Dropbox/Code/MyPython/neurogym/neurogym/core.py:253: UserWarning: Warning: Time for period decision 100.000000  lasts only one timestep. Agents will not have time to respond (e.g. make a choice) on time.\n  ' time to respond (e.g. make a choice) on time.')\n</pre> <pre>Model: \"model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(None, None, 3)]         0         \n_________________________________________________________________\nlstm (LSTM)                  (None, None, 64)          17408     \n_________________________________________________________________\ntime_distributed (TimeDistri (None, None, 3)           195       \n=================================================================\nTotal params: 17,603\nTrainable params: 17,603\nNon-trainable params: 0\n_________________________________________________________________\nWARNING:tensorflow:From /Users/gryang/tf1/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.&lt;locals&gt;.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.where in 2.0, which has the same broadcast rule as np.where\n   7/2000 [..............................] - ETA: 5:35 - loss: 1.0581 - acc: 0.4196</pre> <pre>/Users/gryang/Dropbox/Code/MyPython/neurogym/neurogym/core.py:253: UserWarning: Warning: Time for period fixation 100.000000  lasts only one timestep. Agents will not have time to respond (e.g. make a choice) on time.\n  ' time to respond (e.g. make a choice) on time.')\n/Users/gryang/Dropbox/Code/MyPython/neurogym/neurogym/core.py:253: UserWarning: Warning: Time for period decision 100.000000  lasts only one timestep. Agents will not have time to respond (e.g. make a choice) on time.\n  ' time to respond (e.g. make a choice) on time.')\n</pre> <pre>2000/2000 [==============================] - 106s 53ms/step - loss: 0.0569 - acc: 0.9835\n</pre> In\u00a0[2]: Copied! <pre>perf = 0\nnum_trial = 200\nfor i in range(num_trial):\n    env.new_trial()\n    obs, gt = env.obs, env.gt\n    obs = obs[:, np.newaxis, :]\n\n    action_pred = model.predict(obs)\n    action_pred = np.argmax(action_pred, axis=-1)\n    perf += gt[-1] == action_pred[-1, 0]\n\nperf /= num_trial\nprint(perf)\n</pre> perf = 0 num_trial = 200 for i in range(num_trial):     env.new_trial()     obs, gt = env.obs, env.gt     obs = obs[:, np.newaxis, :]      action_pred = model.predict(obs)     action_pred = np.argmax(action_pred, axis=-1)     perf += gt[-1] == action_pred[-1, 0]  perf /= num_trial print(perf) <pre>0.895\n</pre> In\u00a0[3]: Copied! <pre>_ = ngym.utils.plotting.fig_(obs[0], action_pred[0], gt)\n</pre> _ = ngym.utils.plotting.fig_(obs[0], action_pred[0], gt) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/example_neurogym_keras/#keras-example-of-supervised-learning-a-neurogym-task","title":"Keras example of supervised learning a NeuroGym task\u00b6","text":""},{"location":"examples/example_neurogym_keras/#installation-when-used-on-google-colab","title":"Installation when used on Google Colab\u00b6","text":""},{"location":"examples/example_neurogym_keras/#task-network-and-training","title":"Task, network, and training\u00b6","text":""},{"location":"examples/example_neurogym_keras/#analysis","title":"Analysis\u00b6","text":""},{"location":"examples/example_neurogym_pytorch/","title":"NeuroGym with PyTorch","text":"In\u00a0[\u00a0]: Copied! <pre># Install gymnasium\n! pip install gymnasium\n# Install neurogym\n! git clone https://github.com/gyyang/neurogym.git\n%cd neurogym/\n! pip install -e .\n</pre> # Install gymnasium ! pip install gymnasium # Install neurogym ! git clone https://github.com/gyyang/neurogym.git %cd neurogym/ ! pip install -e . In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport torch\nimport torch.nn as nn\n\nimport neurogym as ngym\n\n# Environment\ntask = 'PerceptualDecisionMaking-v0'\nkwargs = {'dt': 100}\nseq_len = 100\n\n# Make supervised dataset\ndataset = ngym.Dataset(task, env_kwargs=kwargs, batch_size=16,\n                       seq_len=seq_len)\nenv = dataset.env\nob_size = env.observation_space.shape[0]\nact_size = env.action_space.n\n</pre> import numpy as np import torch import torch.nn as nn  import neurogym as ngym  # Environment task = 'PerceptualDecisionMaking-v0' kwargs = {'dt': 100} seq_len = 100  # Make supervised dataset dataset = ngym.Dataset(task, env_kwargs=kwargs, batch_size=16,                        seq_len=seq_len) env = dataset.env ob_size = env.observation_space.shape[0] act_size = env.action_space.n In\u00a0[2]: Copied! <pre>class Net(nn.Module):\n    def __init__(self, num_h):\n        super(Net, self).__init__()\n        self.lstm = nn.LSTM(ob_size, num_h)\n        self.linear = nn.Linear(num_h, act_size)\n\n    def forward(self, x):\n        out, hidden = self.lstm(x)\n        x = self.linear(out)\n        return x\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nnet = Net(num_h=64).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(net.parameters(), lr=1e-2)\n\nrunning_loss = 0.0\nfor i in range(2000):\n    inputs, labels = dataset()\n    inputs = torch.from_numpy(inputs).type(torch.float).to(device)\n    labels = torch.from_numpy(labels.flatten()).type(torch.long).to(device)\n\n    # zero the parameter gradients\n    optimizer.zero_grad()\n\n    # forward + backward + optimize\n    outputs = net(inputs)\n\n    loss = criterion(outputs.view(-1, act_size), labels)\n    loss.backward()\n    optimizer.step()\n\n    # print statistics\n    running_loss += loss.item()\n    if i % 200 == 199:\n        print('{:d} loss: {:0.5f}'.format(i + 1, running_loss / 200))\n        running_loss = 0.0\n\nprint('Finished Training')\n</pre> class Net(nn.Module):     def __init__(self, num_h):         super(Net, self).__init__()         self.lstm = nn.LSTM(ob_size, num_h)         self.linear = nn.Linear(num_h, act_size)      def forward(self, x):         out, hidden = self.lstm(x)         x = self.linear(out)         return x  device = 'cuda' if torch.cuda.is_available() else 'cpu' net = Net(num_h=64).to(device) criterion = nn.CrossEntropyLoss() optimizer = torch.optim.Adam(net.parameters(), lr=1e-2)  running_loss = 0.0 for i in range(2000):     inputs, labels = dataset()     inputs = torch.from_numpy(inputs).type(torch.float).to(device)     labels = torch.from_numpy(labels.flatten()).type(torch.long).to(device)      # zero the parameter gradients     optimizer.zero_grad()      # forward + backward + optimize     outputs = net(inputs)      loss = criterion(outputs.view(-1, act_size), labels)     loss.backward()     optimizer.step()      # print statistics     running_loss += loss.item()     if i % 200 == 199:         print('{:d} loss: {:0.5f}'.format(i + 1, running_loss / 200))         running_loss = 0.0  print('Finished Training') <pre>200 loss: 0.11658\n400 loss: 0.03191\n600 loss: 0.01894\n800 loss: 0.01393\n1000 loss: 0.01351\n1200 loss: 0.01275\n1400 loss: 0.01280\n1600 loss: 0.01279\n1800 loss: 0.01223\n2000 loss: 0.01239\nFinished Training\n</pre> In\u00a0[4]: Copied! <pre>perf = 0\nnum_trial = 200\nfor i in range(num_trial):\n    env.new_trial()\n    ob, gt = env.ob, env.gt\n    ob = ob[:, np.newaxis, :]  # Add batch axis\n    inputs = torch.from_numpy(ob).type(torch.float).to(device)\n\n    action_pred = net(inputs)\n    action_pred = action_pred.detach().numpy()\n    action_pred = np.argmax(action_pred, axis=-1)\n    perf += gt[-1] == action_pred[-1, 0]\n\nperf /= num_trial\nprint('Average performance in {:d} trials'.format(num_trial))\nprint(perf)\n</pre> perf = 0 num_trial = 200 for i in range(num_trial):     env.new_trial()     ob, gt = env.ob, env.gt     ob = ob[:, np.newaxis, :]  # Add batch axis     inputs = torch.from_numpy(ob).type(torch.float).to(device)      action_pred = net(inputs)     action_pred = action_pred.detach().numpy()     action_pred = np.argmax(action_pred, axis=-1)     perf += gt[-1] == action_pred[-1, 0]  perf /= num_trial print('Average performance in {:d} trials'.format(num_trial)) print(perf) <pre>Average performance in 200 trials\n0.83\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/example_neurogym_pytorch/#pytorch-supervised-learning-of-perceptual-decision-making-task","title":"Pytorch supervised learning of perceptual decision making task\u00b6","text":"<p>Pytorch-based example code for training a RNN on a perceptual decision-making task.</p>"},{"location":"examples/example_neurogym_pytorch/#installation-when-used-on-google-colab","title":"Installation when used on Google Colab\u00b6","text":""},{"location":"examples/example_neurogym_pytorch/#dataset","title":"Dataset\u00b6","text":""},{"location":"examples/example_neurogym_pytorch/#network-and-training","title":"Network and Training\u00b6","text":""},{"location":"examples/example_neurogym_pytorch/#analysis","title":"Analysis\u00b6","text":""},{"location":"examples/example_neurogym_rl/","title":"NeuroGym with RL","text":"<p>NeuroGym is a toolkit that allows training any network model on many established neuroscience tasks techniques such as standard Supervised  Learning or Reinforcement Learning (RL). In this notebook we will use RL to train an LSTM network on the classical Random Dots Motion (RDM) task (Britten et al. 1992).</p> <p>We first show how to install the relevant toolboxes. We then show how build the task of interest (in the example the RDM task), wrapp it with the pass-reward wrapper in one line and visualize the structure of the final task. Finally we train an LSTM network on the task using the A2C algorithm Mnih et al. 2016 implemented in the stable-baselines3 toolbox, and plot the results.</p> <p>It is straightforward to change the code to train a network on any other available task or using a different RL algorithm (e.g. ACER, PPO2).</p> In\u00a0[1]: Copied! <pre>%tensorflow_version 1.x\n# Install gymnasium\n! pip install gymnasium\n# Install neurogym\n! git clone https://github.com/gyyang/neurogym.git\n%cd neurogym/\n! pip install -e .\n# Install stable-baselines3\n! pip install stable-baselines3\n</pre> %tensorflow_version 1.x # Install gymnasium ! pip install gymnasium # Install neurogym ! git clone https://github.com/gyyang/neurogym.git %cd neurogym/ ! pip install -e . # Install stable-baselines3 ! pip install stable-baselines3 <pre>TensorFlow 1.x selected.\nRequirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.17.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1)\nRequirement already satisfied: cloudpickle&lt;1.4.0,&gt;=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.0)\nRequirement already satisfied: numpy&gt;=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.18.5)\nRequirement already satisfied: pyglet&lt;=1.5.0,&gt;=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.5.0)\nRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet&lt;=1.5.0,&gt;=1.4.0-&gt;gym) (0.16.0)\nfatal: destination path 'neurogym' already exists and is not an empty directory.\n/content/neurogym\nObtaining file:///content/neurogym\nRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from neurogym==0.0.1) (1.18.5)\nRequirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from neurogym==0.0.1) (0.17.2)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from neurogym==0.0.1) (3.2.2)\nRequirement already satisfied: pyglet&lt;=1.5.0,&gt;=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym-&gt;neurogym==0.0.1) (1.5.0)\nRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym-&gt;neurogym==0.0.1) (1.4.1)\nRequirement already satisfied: cloudpickle&lt;1.4.0,&gt;=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym-&gt;neurogym==0.0.1) (1.3.0)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib-&gt;neurogym==0.0.1) (1.2.0)\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib-&gt;neurogym==0.0.1) (2.4.7)\nRequirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib-&gt;neurogym==0.0.1) (0.10.0)\nRequirement already satisfied: python-dateutil&gt;=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib-&gt;neurogym==0.0.1) (2.8.1)\nRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet&lt;=1.5.0,&gt;=1.4.0-&gt;gym-&gt;neurogym==0.0.1) (0.16.0)\nRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler&gt;=0.10-&gt;matplotlib-&gt;neurogym==0.0.1) (1.12.0)\nInstalling collected packages: neurogym\n  Found existing installation: neurogym 0.0.1\n    Can't uninstall 'neurogym'. No files were found to uninstall.\n  Running setup.py develop for neurogym\nSuccessfully installed neurogym\nRequirement already up-to-date: stable-baselines in /usr/local/lib/python3.6/dist-packages (2.10.0)\nRequirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from stable-baselines) (1.4.1)\nRequirement already satisfied, skipping upgrade: opencv-python in /usr/local/lib/python3.6/dist-packages (from stable-baselines) (4.1.2.30)\nRequirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from stable-baselines) (1.18.5)\nRequirement already satisfied, skipping upgrade: matplotlib in /usr/local/lib/python3.6/dist-packages (from stable-baselines) (3.2.2)\nRequirement already satisfied, skipping upgrade: gym[atari,classic_control]&gt;=0.11 in /usr/local/lib/python3.6/dist-packages (from stable-baselines) (0.17.2)\nRequirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.6/dist-packages (from stable-baselines) (0.15.1)\nRequirement already satisfied, skipping upgrade: cloudpickle&gt;=0.5.5 in /usr/local/lib/python3.6/dist-packages (from stable-baselines) (1.3.0)\nRequirement already satisfied, skipping upgrade: pandas in /usr/local/lib/python3.6/dist-packages (from stable-baselines) (1.0.5)\nRequirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib-&gt;stable-baselines) (2.4.7)\nRequirement already satisfied, skipping upgrade: python-dateutil&gt;=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib-&gt;stable-baselines) (2.8.1)\nRequirement already satisfied, skipping upgrade: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib-&gt;stable-baselines) (1.2.0)\nRequirement already satisfied, skipping upgrade: cycler&gt;=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib-&gt;stable-baselines) (0.10.0)\nRequirement already satisfied, skipping upgrade: pyglet&lt;=1.5.0,&gt;=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]&gt;=0.11-&gt;stable-baselines) (1.5.0)\nRequirement already satisfied, skipping upgrade: Pillow; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]&gt;=0.11-&gt;stable-baselines) (7.0.0)\nRequirement already satisfied, skipping upgrade: atari-py~=0.2.0; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]&gt;=0.11-&gt;stable-baselines) (0.2.6)\nRequirement already satisfied, skipping upgrade: pytz&gt;=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas-&gt;stable-baselines) (2018.9)\nRequirement already satisfied, skipping upgrade: six&gt;=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil&gt;=2.1-&gt;matplotlib-&gt;stable-baselines) (1.12.0)\nRequirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from pyglet&lt;=1.5.0,&gt;=1.4.0-&gt;gym[atari,classic_control]&gt;=0.11-&gt;stable-baselines) (0.16.0)\n</pre> <p>here we build the Random Dots Motion task, specifying the duration of each trial period (fixation, stimulus, decision) and wrapp it with the pass-reward wrapper which appends the previous reward to the observation. We then plot the structure of the task in a figure that shows:</p> <ol> <li>The observations received by the agent (top panel).</li> <li>The actions taken by a random agent and the correct action at each timestep (second panel).</li> <li>The rewards provided by the environment at each timestep (third panel).</li> <li>The performance of the agent at each trial (bottom panel).</li> </ol> In\u00a0[2]: Copied! <pre>import gymnasium as gym\nimport neurogym as ngym\nfrom neurogym.wrappers import pass_reward\nimport warnings\nwarnings.filterwarnings('ignore')\n# Task name\nname = 'PerceptualDecisionMaking-v0'\n# task specification (here we only specify the duration of the different trial periods)\ntiming = {'fixation': ('constant', 300),\n          'stimulus': ('constant', 500),\n          'decision': ('constant', 300)}\nkwargs = {'dt': 100, 'timing': timing}\n# build task\nenv = gym.make(name, **kwargs)\n# print task properties\nprint(env)\n# wrapp task with pass-reward wrapper\nenv = pass_reward.PassReward(env)\n# plot example trials with random agent\ndata = ngym.utils.plot_env(env, fig_kwargs={'figsize': (12, 12)}, num_steps=100, ob_traces=['Fixation cue', 'Stim 1', 'Stim 2', 'Previous reward'])\n</pre> import gymnasium as gym import neurogym as ngym from neurogym.wrappers import pass_reward import warnings warnings.filterwarnings('ignore') # Task name name = 'PerceptualDecisionMaking-v0' # task specification (here we only specify the duration of the different trial periods) timing = {'fixation': ('constant', 300),           'stimulus': ('constant', 500),           'decision': ('constant', 300)} kwargs = {'dt': 100, 'timing': timing} # build task env = gym.make(name, **kwargs) # print task properties print(env) # wrapp task with pass-reward wrapper env = pass_reward.PassReward(env) # plot example trials with random agent data = ngym.utils.plot_env(env, fig_kwargs={'figsize': (12, 12)}, num_steps=100, ob_traces=['Fixation cue', 'Stim 1', 'Stim 2', 'Previous reward']) <pre>findfont: Font family ['arial'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['arial'] not found. Falling back to DejaVu Sans.\n</pre> <pre>### PerceptualDecisionMaking\nDoc: Two-alternative forced choice task in which the subject has to\n    integrate two stimuli to decide which one is higher on average.\n\n    Args:\n        stim_scale: Controls the difficulty of the experiment. (def: 1., float)\n        sigma: float, input noise level\n        dim_ring: int, dimension of ring input and output\n    \nReference paper \n[The analysis of visual motion: a comparison of neuronal and psychophysical performance](https://www.jneurosci.org/content/12/12/4745)\n\nPeriod timing (ms) \nfixation : constant 300\nstimulus : constant 500\ndelay : constant 0\ndecision : constant 300\n\nReward structure \nabort : -0.1\ncorrect : 1.0\nfail : 0.0\n\nTags: perceptual, two-alternative, supervised.\n\n</pre> In\u00a0[3]: Copied! <pre>import warnings\nfrom stable_baselines3.common.policies import LstmPolicy\nfrom stable_baselines3.common.vec_env import DummyVecEnv\nfrom stable_baselines3 import A2C  # ACER, PPO2\nwarnings.filterwarnings('default')\n\n# Optional: PPO2 requires a vectorized environment to run\n# the env is now wrapped automatically when passing it to the constructor\nenv = DummyVecEnv([lambda: env])\n\nmodel = A2C(LstmPolicy, env, verbose=1, policy_kwargs={'feature_extraction':\"mlp\"})\nmodel.learn(total_timesteps=100000, log_interval=1000)\nenv.close()\n</pre> import warnings from stable_baselines3.common.policies import LstmPolicy from stable_baselines3.common.vec_env import DummyVecEnv from stable_baselines3 import A2C  # ACER, PPO2 warnings.filterwarnings('default')  # Optional: PPO2 requires a vectorized environment to run # the env is now wrapped automatically when passing it to the constructor env = DummyVecEnv([lambda: env])  model = A2C(LstmPolicy, env, verbose=1, policy_kwargs={'feature_extraction':\"mlp\"}) model.learn(total_timesteps=100000, log_interval=1000) env.close() <pre>WARNING:tensorflow:\nThe TensorFlow contrib module will not be included in TensorFlow 2.0.\nFor more information, please see:\n  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n  * https://github.com/tensorflow/addons\n  * https://github.com/tensorflow/io (for I/O related ops)\nIf you depend on functionality not listed there, please file an issue.\n\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/policies.py:116: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/policies.py:420: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse keras.layers.flatten instead.\nWARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `layer.__call__` method instead.\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/tf_layers.py:123: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/distributions.py:326: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/distributions.py:327: The name tf.log is deprecated. Please use tf.math.log instead.\n\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/a2c/a2c.py:158: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/tf_util.py:449: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/tf_util.py:449: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n\nWARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/clip_ops.py:301: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.where in 2.0, which has the same broadcast rule as np.where\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/a2c/a2c.py:182: The name tf.train.RMSPropOptimizer is deprecated. Please use tf.compat.v1.train.RMSPropOptimizer instead.\n\nWARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\nInstructions for updating:\nCall initializer instance with the dtype argument instead of passing it to the constructor\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/a2c/a2c.py:192: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/a2c/a2c.py:194: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n\n---------------------------------\n| explained_variance | -0.477   |\n| fps                | 10       |\n| nupdates           | 1        |\n| policy_entropy     | 1.1      |\n| total_timesteps    | 5        |\n| value_loss         | 0.00289  |\n---------------------------------\n---------------------------------\n| explained_variance | -0.907   |\n| fps                | 351      |\n| nupdates           | 1000     |\n| policy_entropy     | 1.1      |\n| total_timesteps    | 5000     |\n| value_loss         | 0.0147   |\n---------------------------------\n---------------------------------\n| explained_variance | 0.517    |\n| fps                | 356      |\n| nupdates           | 2000     |\n| policy_entropy     | 1.07     |\n| total_timesteps    | 10000    |\n| value_loss         | 0.554    |\n---------------------------------\n---------------------------------\n| explained_variance | 0.182    |\n| fps                | 359      |\n| nupdates           | 3000     |\n| policy_entropy     | 1.05     |\n| total_timesteps    | 15000    |\n| value_loss         | 0.0563   |\n---------------------------------\n---------------------------------\n| explained_variance | 0.558    |\n| fps                | 358      |\n| nupdates           | 4000     |\n| policy_entropy     | 0.808    |\n| total_timesteps    | 20000    |\n| value_loss         | 0.166    |\n---------------------------------\n---------------------------------\n| explained_variance | 0.99     |\n| fps                | 358      |\n| nupdates           | 5000     |\n| policy_entropy     | 0.189    |\n| total_timesteps    | 25000    |\n| value_loss         | 0.00343  |\n---------------------------------\n---------------------------------\n| explained_variance | 0.991    |\n| fps                | 360      |\n| nupdates           | 6000     |\n| policy_entropy     | 0.117    |\n| total_timesteps    | 30000    |\n| value_loss         | 0.00305  |\n---------------------------------\n---------------------------------\n| explained_variance | 0.914    |\n| fps                | 362      |\n| nupdates           | 7000     |\n| policy_entropy     | 0.212    |\n| total_timesteps    | 35000    |\n| value_loss         | 0.013    |\n---------------------------------\n---------------------------------\n| explained_variance | 0.957    |\n| fps                | 362      |\n| nupdates           | 8000     |\n| policy_entropy     | 0.0404   |\n| total_timesteps    | 40000    |\n| value_loss         | 0.026    |\n---------------------------------\n---------------------------------\n| explained_variance | 0.934    |\n| fps                | 360      |\n| nupdates           | 9000     |\n| policy_entropy     | 0.27     |\n| total_timesteps    | 45000    |\n| value_loss         | 0.011    |\n---------------------------------\n---------------------------------\n| explained_variance | 0.976    |\n| fps                | 360      |\n| nupdates           | 10000    |\n| policy_entropy     | 0.509    |\n| total_timesteps    | 50000    |\n| value_loss         | 0.00139  |\n---------------------------------\n---------------------------------\n| explained_variance | 0.991    |\n| fps                | 360      |\n| nupdates           | 11000    |\n| policy_entropy     | 0.0325   |\n| total_timesteps    | 55000    |\n| value_loss         | 0.00196  |\n---------------------------------\n---------------------------------\n| explained_variance | 0.996    |\n| fps                | 361      |\n| nupdates           | 12000    |\n| policy_entropy     | 0.211    |\n| total_timesteps    | 60000    |\n| value_loss         | 0.000678 |\n---------------------------------\n---------------------------------\n| explained_variance | 0.684    |\n| fps                | 361      |\n| nupdates           | 13000    |\n| policy_entropy     | 0.2      |\n| total_timesteps    | 65000    |\n| value_loss         | 0.00527  |\n---------------------------------\n---------------------------------\n| explained_variance | 0.968    |\n| fps                | 361      |\n| nupdates           | 14000    |\n| policy_entropy     | 0.424    |\n| total_timesteps    | 70000    |\n| value_loss         | 0.00391  |\n---------------------------------\n---------------------------------\n| explained_variance | 0.851    |\n| fps                | 362      |\n| nupdates           | 15000    |\n| policy_entropy     | 0.384    |\n| total_timesteps    | 75000    |\n| value_loss         | 0.0313   |\n---------------------------------\n---------------------------------\n| explained_variance | 0.977    |\n| fps                | 361      |\n| nupdates           | 16000    |\n| policy_entropy     | 0.0187   |\n| total_timesteps    | 80000    |\n| value_loss         | 0.00316  |\n---------------------------------\n---------------------------------\n| explained_variance | 0.994    |\n| fps                | 362      |\n| nupdates           | 17000    |\n| policy_entropy     | 0.0252   |\n| total_timesteps    | 85000    |\n| value_loss         | 0.000824 |\n---------------------------------\n---------------------------------\n| explained_variance | 0.957    |\n| fps                | 362      |\n| nupdates           | 18000    |\n| policy_entropy     | 0.206    |\n| total_timesteps    | 90000    |\n| value_loss         | 0.00319  |\n---------------------------------\n---------------------------------\n| explained_variance | 0.992    |\n| fps                | 363      |\n| nupdates           | 19000    |\n| policy_entropy     | 0.0935   |\n| total_timesteps    | 95000    |\n| value_loss         | 0.00294  |\n---------------------------------\n---------------------------------\n| explained_variance | 0.978    |\n| fps                | 363      |\n| nupdates           | 20000    |\n| policy_entropy     | 0.0645   |\n| total_timesteps    | 100000   |\n| value_loss         | 0.000502 |\n---------------------------------\n</pre> In\u00a0[6]: Copied! <pre>env = gym.make(name, **kwargs)\n# print task properties\nprint(env)\n# wrapp task with pass-reward wrapper\nenv = pass_reward.PassReward(env)\nenv = DummyVecEnv([lambda: env])\n# plot example trials with random agent\ndata = ngym.utils.plot_env(env, fig_kwargs={'figsize': (12, 12)}, num_steps=100, ob_traces=['Fixation cue', 'Stim 1', 'Stim 2', 'Previous reward'], model=model)\n</pre> env = gym.make(name, **kwargs) # print task properties print(env) # wrapp task with pass-reward wrapper env = pass_reward.PassReward(env) env = DummyVecEnv([lambda: env]) # plot example trials with random agent data = ngym.utils.plot_env(env, fig_kwargs={'figsize': (12, 12)}, num_steps=100, ob_traces=['Fixation cue', 'Stim 1', 'Stim 2', 'Previous reward'], model=model)  <pre>### PerceptualDecisionMaking\nDoc: Two-alternative forced choice task in which the subject has to\n    integrate two stimuli to decide which one is higher on average.\n\n    Args:\n        stim_scale: Controls the difficulty of the experiment. (def: 1., float)\n        sigma: float, input noise level\n        dim_ring: int, dimension of ring input and output\n    \nReference paper \n[The analysis of visual motion: a comparison of neuronal and psychophysical performance](https://www.jneurosci.org/content/12/12/4745)\n\nPeriod timing (ms) \nfixation : constant 300\nstimulus : constant 500\ndelay : constant 0\ndecision : constant 300\n\nReward structure \nabort : -0.1\ncorrect : 1.0\nfail : 0.0\n\nTags: perceptual, two-alternative, supervised.\n\n</pre>"},{"location":"examples/example_neurogym_rl/#reinforcement-learning-example-with-stable-baselines3","title":"Reinforcement learning example with stable-baselines3\u00b6","text":""},{"location":"examples/example_neurogym_rl/#installation","title":"Installation\u00b6","text":"<p>(only for running in Google Colab)</p>"},{"location":"examples/example_neurogym_rl/#task","title":"Task\u00b6","text":""},{"location":"examples/example_neurogym_rl/#train-a-network","title":"Train a network\u00b6","text":""},{"location":"examples/example_neurogym_rl/#visualize-results","title":"Visualize results\u00b6","text":""},{"location":"examples/template/","title":"Template","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"Example template for contributing new tasks.\"\"\"  # noqa: INP001\n</pre> \"\"\"Example template for contributing new tasks.\"\"\"  # noqa: INP001 In\u00a0[\u00a0]: Copied! <pre>import numpy as np\n</pre> import numpy as np In\u00a0[\u00a0]: Copied! <pre>import neurogym as ngym\nfrom neurogym import spaces\n</pre> import neurogym as ngym from neurogym import spaces In\u00a0[\u00a0]: Copied! <pre>class YourTask(ngym.TrialEnv):\n    def __init__(self, dt=100, rewards=None, timing=None, sigma=1) -&gt; None:\n        super().__init__(dt=dt)\n        # Possible decisions at the end of the trial\n        self.choices = [1, 2]  # e.g. [left, right]\n        self.sigma = sigma / np.sqrt(self.dt)  # Input noise\n\n        # Optional rewards dictionary\n        self.rewards = {\"abort\": -0.1, \"correct\": +1.0, \"fail\": 0.0}\n        if rewards:\n            self.rewards.update(rewards)\n\n        # Optional timing dictionary\n        # if provided, self.add_period can infer timing directly\n        self.timing = {\"fixation\": 100, \"stimulus\": 2000, \"delay\": 0, \"decision\": 100}\n        if timing:\n            self.timing.update(timing)\n\n        # Similar to gymnasium envs, define observations_space and action_space\n        # Optional annotation of the observation space\n        name = {\"fixation\": 0, \"stimulus\": [1, 2]}\n        self.observation_space = spaces.Box(\n            -np.inf,\n            np.inf,\n            shape=(3,),\n            dtype=np.float32,\n            name=name,\n        )\n        # Optional annotation of the action space\n        name = {\"fixation\": 0, \"choice\": [1, 2]}\n        self.action_space = spaces.Discrete(3, name=name)\n\n    def _new_trial(self, **kwargs):\n        \"\"\"Called internally to generate a next trial.\n\n        Typically, you need to\n            set trial: a dictionary of trial information\n            run self.add_period():\n                will add time periods to the trial\n                accesible through dict self.start_t and self.end_t\n            run self.add_ob():\n                will add observation to np array self.ob\n            run self.set_groundtruth():\n                will set groundtruth to np array self.gt\n\n        Returns:\n            trial: dictionary of trial information\n        \"\"\"\n        # Setting trial information\n        trial = {\"ground_truth\": self.rng.choice(self.choices)}\n        trial.update(kwargs)  # allows wrappers to modify the trial\n        ground_truth = trial[\"ground_truth\"]\n\n        # Adding periods sequentially\n        self.add_period([\"fixation\", \"stimulus\", \"delay\", \"decision\"])\n\n        # Setting observations, default all 0\n        # Setting fixation cue to 1 before decision period\n        self.add_ob(1, where=\"fixation\")\n        self.set_ob(0, \"decision\", where=\"fixation\")\n        # Set the stimulus\n        stim = [0, 0, 0]\n        stim[ground_truth] = 1\n        self.add_ob(stim, \"stimulus\")\n        # adding gaussian noise to stimulus with std = self.sigma\n        self.add_randn(0, self.sigma, \"stimulus\", where=\"stimulus\")\n\n        # Setting ground-truth value for supervised learning\n        self.set_groundtruth(ground_truth, \"decision\")\n\n        return trial\n\n    def _step(self, action):\n        \"\"\"Called internally to process one step.\n\n        Receives an action and returns:\n        a new observation, obs\n        reward associated with the action, reward\n        a boolean variable indicating whether the experiment has terminated, terminated\n            See more at https://gymnasium.farama.org/tutorials/gymnasium_basics/handling_time_limits/#termination\n        a boolean variable indicating whether the experiment has been truncated, truncated\n            See more at https://gymnasium.farama.org/tutorials/gymnasium_basics/handling_time_limits/#truncation\n        a dictionary with extra information:\n            ground truth correct response, info['gt']\n            boolean indicating the end of the trial, info['new_trial'].\n        \"\"\"\n        terminated = False\n        truncated = False\n        # rewards\n        reward = 0\n        gt = self.gt_now\n        # Example structure\n        if not self.in_period(\"decision\"):\n            if action != 0:  # if fixation break\n                reward = self.rewards[\"abort\"]\n        elif action != 0:\n            terminated = True\n            reward = self.rewards[\"correct\"] if action == gt else self.rewards[\"fail\"]\n\n        return (\n            self.ob_now,\n            reward,\n            terminated,\n            truncated,\n            {\"new_trial\": terminated, \"gt\": gt},\n        )\n</pre> class YourTask(ngym.TrialEnv):     def __init__(self, dt=100, rewards=None, timing=None, sigma=1) -&gt; None:         super().__init__(dt=dt)         # Possible decisions at the end of the trial         self.choices = [1, 2]  # e.g. [left, right]         self.sigma = sigma / np.sqrt(self.dt)  # Input noise          # Optional rewards dictionary         self.rewards = {\"abort\": -0.1, \"correct\": +1.0, \"fail\": 0.0}         if rewards:             self.rewards.update(rewards)          # Optional timing dictionary         # if provided, self.add_period can infer timing directly         self.timing = {\"fixation\": 100, \"stimulus\": 2000, \"delay\": 0, \"decision\": 100}         if timing:             self.timing.update(timing)          # Similar to gymnasium envs, define observations_space and action_space         # Optional annotation of the observation space         name = {\"fixation\": 0, \"stimulus\": [1, 2]}         self.observation_space = spaces.Box(             -np.inf,             np.inf,             shape=(3,),             dtype=np.float32,             name=name,         )         # Optional annotation of the action space         name = {\"fixation\": 0, \"choice\": [1, 2]}         self.action_space = spaces.Discrete(3, name=name)      def _new_trial(self, **kwargs):         \"\"\"Called internally to generate a next trial.          Typically, you need to             set trial: a dictionary of trial information             run self.add_period():                 will add time periods to the trial                 accesible through dict self.start_t and self.end_t             run self.add_ob():                 will add observation to np array self.ob             run self.set_groundtruth():                 will set groundtruth to np array self.gt          Returns:             trial: dictionary of trial information         \"\"\"         # Setting trial information         trial = {\"ground_truth\": self.rng.choice(self.choices)}         trial.update(kwargs)  # allows wrappers to modify the trial         ground_truth = trial[\"ground_truth\"]          # Adding periods sequentially         self.add_period([\"fixation\", \"stimulus\", \"delay\", \"decision\"])          # Setting observations, default all 0         # Setting fixation cue to 1 before decision period         self.add_ob(1, where=\"fixation\")         self.set_ob(0, \"decision\", where=\"fixation\")         # Set the stimulus         stim = [0, 0, 0]         stim[ground_truth] = 1         self.add_ob(stim, \"stimulus\")         # adding gaussian noise to stimulus with std = self.sigma         self.add_randn(0, self.sigma, \"stimulus\", where=\"stimulus\")          # Setting ground-truth value for supervised learning         self.set_groundtruth(ground_truth, \"decision\")          return trial      def _step(self, action):         \"\"\"Called internally to process one step.          Receives an action and returns:         a new observation, obs         reward associated with the action, reward         a boolean variable indicating whether the experiment has terminated, terminated             See more at https://gymnasium.farama.org/tutorials/gymnasium_basics/handling_time_limits/#termination         a boolean variable indicating whether the experiment has been truncated, truncated             See more at https://gymnasium.farama.org/tutorials/gymnasium_basics/handling_time_limits/#truncation         a dictionary with extra information:             ground truth correct response, info['gt']             boolean indicating the end of the trial, info['new_trial'].         \"\"\"         terminated = False         truncated = False         # rewards         reward = 0         gt = self.gt_now         # Example structure         if not self.in_period(\"decision\"):             if action != 0:  # if fixation break                 reward = self.rewards[\"abort\"]         elif action != 0:             terminated = True             reward = self.rewards[\"correct\"] if action == gt else self.rewards[\"fail\"]          return (             self.ob_now,             reward,             terminated,             truncated,             {\"new_trial\": terminated, \"gt\": gt},         ) In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n    # Instantiate the task\n    env = YourTask()\n    trial = env.new_trial()\n    print(\"Trial info\", trial)\n    print(\"Trial observation shape\", env.ob.shape)\n    print(\"Trial action shape\", env.gt.shape)\n    env.reset()\n    ob, reward, terminated, truncated, info = env.step(env.action_space.sample())\n    print(\"Single time step observation shape\", ob.shape)\n</pre> if __name__ == \"__main__\":     # Instantiate the task     env = YourTask()     trial = env.new_trial()     print(\"Trial info\", trial)     print(\"Trial observation shape\", env.ob.shape)     print(\"Trial action shape\", env.gt.shape)     env.reset()     ob, reward, terminated, truncated, info = env.step(env.action_space.sample())     print(\"Single time step observation shape\", ob.shape)"},{"location":"examples/understanding_neurogym_task/","title":"Custom tasks","text":"In\u00a0[\u00a0]: Copied! <pre># # Install gymnasium\n# ! pip install gymnasium\n\n# # Install neurogym\n# ! git clone https://github.com/gyyang/neurogym.git\n# %cd neurogym/\n# ! pip install -e .\n</pre> # # Install gymnasium # ! pip install gymnasium  # # Install neurogym # ! git clone https://github.com/gyyang/neurogym.git # %cd neurogym/ # ! pip install -e . <p>Neurogym tasks follow basic Gymnasium tasks format. Gymnasium is a maintained fork of OpenAI\u2019s Gym library. Each task is defined as a Python class, inheriting from the <code>gymnasium.Env</code> class.</p> <p>In this section we describe basic structure for an gymnasium task.</p> <p>In the <code>__init__</code> method, it is necessary to define two attributes, <code>self.observation_space</code> and <code>self.action_space</code> which describe the kind of spaces used by observations (network inputs) and actions (network outputs).</p> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport gymnasium as gym\n\nclass MyEnv(gym.Env):\n    def __init__(self):\n        super().__init__()  # Python boilerplate to initialize base class\n\n        # A two-dimensional box with minimum and maximum value set by low and high\n        self.observation_space = gym.spaces.Box(low=0., high=1., shape=(2,))\n\n        # A discrete space with 3 possible values (0, 1, 2)\n        self.action_space = gym.spaces.Discrete(3)\n\n# Instantiate an environment\nenv = MyEnv()\nprint('Sample random observation value:', env.observation_space.sample())\nprint('Sample random action value:', env.action_space.sample())\n</pre> import numpy as np import gymnasium as gym  class MyEnv(gym.Env):     def __init__(self):         super().__init__()  # Python boilerplate to initialize base class          # A two-dimensional box with minimum and maximum value set by low and high         self.observation_space = gym.spaces.Box(low=0., high=1., shape=(2,))          # A discrete space with 3 possible values (0, 1, 2)         self.action_space = gym.spaces.Discrete(3)  # Instantiate an environment env = MyEnv() print('Sample random observation value:', env.observation_space.sample()) print('Sample random action value:', env.action_space.sample()) <p>Another key method that needs to be defined is the <code>step</code> method, which updates the environment and outputs observations and rewards after receiving the agent's action.</p> <p>The <code>step</code> method takes <code>action</code> as inputs, and outputs the agent's next observation <code>observation</code>, a scalar reward received by the agent <code>reward</code>, a boolean describing whether the environment needs to be reset <code>done</code>, and a dictionary holding any additional information <code>info</code>.</p> <p>If the environment is described by internal states, the <code>reset</code> method would reset these internal states. This method returns an initial observation <code>observation</code>.</p> In\u00a0[\u00a0]: Copied! <pre>class MyEnv(gym.Env):\n    def __init__(self):\n        super().__init__()  # Python boilerplate to initialize base class\n        self.observation_space = gym.spaces.Box(low=-10., high=10., shape=(1,))\n        self.action_space = gym.spaces.Discrete(3)\n\n    def step(self, action):\n        ob = self.observation_space.sample()  # random sampling\n        reward = 1.  # reward\n        terminated = False  # never ending\n        truncated = False\n        info = {}  # empty dictionary\n        return ob, reward, terminated, truncated, info\n\n    def reset(self):\n        ob = self.observation_space.sample()\n        return ob, {}\n</pre> class MyEnv(gym.Env):     def __init__(self):         super().__init__()  # Python boilerplate to initialize base class         self.observation_space = gym.spaces.Box(low=-10., high=10., shape=(1,))         self.action_space = gym.spaces.Discrete(3)      def step(self, action):         ob = self.observation_space.sample()  # random sampling         reward = 1.  # reward         terminated = False  # never ending         truncated = False         info = {}  # empty dictionary         return ob, reward, terminated, truncated, info      def reset(self):         ob = self.observation_space.sample()         return ob, {} <p>Below we define a simple task where actions move an agent along a one-dimensional line. The reward is determined by the agent's location on this line.</p> In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\n\ndef get_reward(x):\n    return np.sin(x) * np.exp(-np.abs(x)/3)\n\nxs = np.linspace(-10, 10, 100)\nplt.plot(xs, get_reward(xs))\nplt.xlabel('State value (observation)')\nplt.ylabel('Reward')\n</pre> import matplotlib.pyplot as plt  def get_reward(x):     return np.sin(x) * np.exp(-np.abs(x)/3)  xs = np.linspace(-10, 10, 100) plt.plot(xs, get_reward(xs)) plt.xlabel('State value (observation)') plt.ylabel('Reward') In\u00a0[\u00a0]: Copied! <pre>class MyEnv(gym.Env):\n    def __init__(self):\n        # A one-dimensional box with minimum and maximum value set by low and high\n        self.observation_space = gym.spaces.Box(low=-10., high=10., shape=(1,))\n\n        # A discrete space with 3 possible values (0, 1, 2)\n        self.action_space = gym.spaces.Discrete(3)\n\n        self.state = 0.\n\n    def step(self, action):\n        # Actions 0, 1, 2 correspond to state change of -0.1, 0, +0.1\n        self.state += (action - 1.) * 0.1\n        self.state = np.clip(self.state, -10, 10)\n\n        ob = self.state  # observation\n        reward = get_reward(self.state)  # reward\n        terminated = False  # never ending\n        truncated = False\n        info = {}  # empty dictionary\n        return ob, reward, terminated, truncated, info\n\n    def reset(self):\n        # Re-initialize state\n        self.state = self.observation_space.sample()\n        return self.state, {}\n</pre> class MyEnv(gym.Env):     def __init__(self):         # A one-dimensional box with minimum and maximum value set by low and high         self.observation_space = gym.spaces.Box(low=-10., high=10., shape=(1,))          # A discrete space with 3 possible values (0, 1, 2)         self.action_space = gym.spaces.Discrete(3)          self.state = 0.      def step(self, action):         # Actions 0, 1, 2 correspond to state change of -0.1, 0, +0.1         self.state += (action - 1.) * 0.1         self.state = np.clip(self.state, -10, 10)          ob = self.state  # observation         reward = get_reward(self.state)  # reward         terminated = False  # never ending         truncated = False         info = {}  # empty dictionary         return ob, reward, terminated, truncated, info      def reset(self):         # Re-initialize state         self.state = self.observation_space.sample()         return self.state, {} <p>An agent can interact with the environment iteratively.</p> In\u00a0[\u00a0]: Copied! <pre>env = MyEnv()\nob, _ = env.reset()\nob_log = list()\nreward_log = list()\nfor i in range(10000):\n    action = env.action_space.sample()  # A random agent\n    ob, reward, terminated, truncated, info = env.step(action)\n    ob_log.append(ob)\n    reward_log.append(reward)\n\nplt.plot(ob_log, reward_log)\n</pre> env = MyEnv() ob, _ = env.reset() ob_log = list() reward_log = list() for i in range(10000):     action = env.action_space.sample()  # A random agent     ob, reward, terminated, truncated, info = env.step(action)     ob_log.append(ob)     reward_log.append(reward)  plt.plot(ob_log, reward_log) In\u00a0[\u00a0]: Copied! <pre>import neurogym as ngym\nfrom neurogym import TrialEnv\n\nclass MyTrialEnv(TrialEnv):\n    def __init__(self):\n        super().__init__()\n        self.observation_space = gym.spaces.Box(low=-1., high=1., shape=(1,))\n        self.action_space = gym.spaces.Discrete(2)\n\n        self.next_ob = np.random.uniform(-1, 1, size=(1,))\n\n    def _new_trial(self):\n        ob = self.next_ob  # observation previously computed\n        # Sample observation for the next trial\n        self.next_ob = np.random.uniform(-1, 1, size=(1,))\n\n        trial = dict()\n        # Ground-truth is 1 if ob &gt; 0, else 0\n        trial['ground_truth'] = (ob &gt; 0) * 1.0\n\n        return trial\n\n    def _step(self, action):\n        ob = self.next_ob\n        # If action equals to ground_truth, reward=1, otherwise 0\n        reward = (action == self.trial['ground_truth']) * 1.0\n        terminated = False\n        truncated = False\n        info = {'new_trial': True}\n        return ob, reward, terminated, truncated, info\n</pre> import neurogym as ngym from neurogym import TrialEnv  class MyTrialEnv(TrialEnv):     def __init__(self):         super().__init__()         self.observation_space = gym.spaces.Box(low=-1., high=1., shape=(1,))         self.action_space = gym.spaces.Discrete(2)          self.next_ob = np.random.uniform(-1, 1, size=(1,))      def _new_trial(self):         ob = self.next_ob  # observation previously computed         # Sample observation for the next trial         self.next_ob = np.random.uniform(-1, 1, size=(1,))          trial = dict()         # Ground-truth is 1 if ob &gt; 0, else 0         trial['ground_truth'] = (ob &gt; 0) * 1.0          return trial      def _step(self, action):         ob = self.next_ob         # If action equals to ground_truth, reward=1, otherwise 0         reward = (action == self.trial['ground_truth']) * 1.0         terminated = False         truncated = False         info = {'new_trial': True}         return ob, reward, terminated, truncated, info In\u00a0[\u00a0]: Copied! <pre>env = MyTrialEnv()\nob, _ = env.reset()\n\nprint('Trial', 0)\nprint('Received observation', ob)\n\nfor i in range(5):\n    action = env.action_space.sample()  # A random agent\n    print('Selected action', action)\n    ob, reward, terminated, truncated, info = env.step(action)\n    print('Received reward', reward)\n    print('Trial', i+1)\n    print('Received observation', ob)\n</pre> env = MyTrialEnv() ob, _ = env.reset()  print('Trial', 0) print('Received observation', ob)  for i in range(5):     action = env.action_space.sample()  # A random agent     print('Selected action', action)     ob, reward, terminated, truncated, info = env.step(action)     print('Received reward', reward)     print('Trial', i+1)     print('Received observation', ob) In\u00a0[\u00a0]: Copied! <pre>class MyDecisionEnv(TrialEnv):\n    def __init__(self, dt=100, timing=None):\n        super().__init__(dt=dt)  # dt is passed to base task\n\n        # Setting default task timing\n        self.timing = {'stimulus': 500, 'decision': 500}\n        # Update timing if provided externally\n        if timing:\n            self.timing.update(timing)\n\n        self.observation_space = gym.spaces.Box(low=-1., high=1., shape=(1,))\n        self.action_space = gym.spaces.Discrete(2)\n\n    def _new_trial(self):\n        # Setting time periods for this trial\n        periods = ['stimulus', 'decision']\n        # Will add stimulus and decision periods sequentially using self.timing info\n        self.add_period(periods)\n\n        # Sample observation for the next trial\n        stimulus = np.random.uniform(-1, 1, size=(1,))\n\n        trial = dict()\n        trial['stimulus'] = stimulus\n        # Ground-truth is 1 if stimulus &gt; 0, else 0\n        trial['ground_truth'] = (stimulus &gt; 0) * 1.0\n\n        return trial\n\n    def _step(self, action):\n        # Check if the current time step is in stimulus period\n        if self.in_period('stimulus'):\n            ob = np.array([self.trial['stimulus']])\n            reward = 0.  # no reward\n        else:\n            ob = np.array([0.])  # no observation\n            # If action equals to ground_truth, reward=1, otherwise 0\n            reward = (action == self.trial['ground_truth']) * 1.0\n\n        terminated = False\n        truncated = False\n        # By default, the trial is not ended\n        info = {'new_trial': False}\n        return ob, reward, terminated, truncated, info\n</pre> class MyDecisionEnv(TrialEnv):     def __init__(self, dt=100, timing=None):         super().__init__(dt=dt)  # dt is passed to base task          # Setting default task timing         self.timing = {'stimulus': 500, 'decision': 500}         # Update timing if provided externally         if timing:             self.timing.update(timing)          self.observation_space = gym.spaces.Box(low=-1., high=1., shape=(1,))         self.action_space = gym.spaces.Discrete(2)      def _new_trial(self):         # Setting time periods for this trial         periods = ['stimulus', 'decision']         # Will add stimulus and decision periods sequentially using self.timing info         self.add_period(periods)          # Sample observation for the next trial         stimulus = np.random.uniform(-1, 1, size=(1,))          trial = dict()         trial['stimulus'] = stimulus         # Ground-truth is 1 if stimulus &gt; 0, else 0         trial['ground_truth'] = (stimulus &gt; 0) * 1.0          return trial      def _step(self, action):         # Check if the current time step is in stimulus period         if self.in_period('stimulus'):             ob = np.array([self.trial['stimulus']])             reward = 0.  # no reward         else:             ob = np.array([0.])  # no observation             # If action equals to ground_truth, reward=1, otherwise 0             reward = (action == self.trial['ground_truth']) * 1.0          terminated = False         truncated = False         # By default, the trial is not ended         info = {'new_trial': False}         return ob, reward, terminated, truncated, info <p>Running the environment with a random agent and plotting the agent's observation, action, and rewards</p> In\u00a0[\u00a0]: Copied! <pre># Logging\nlog = {'ob': [], 'gt': [], 'action': [], 'reward': []}\n\nenv = MyDecisionEnv(dt=100)\nob, _ = env.reset()\nlog['ob'].append(float(ob))\nlog['gt'].append(float(ob &gt; 0))\nfor i in range(30):\n    action = env.action_space.sample()  # A random agent\n    ob, reward, terminated, truncated, info = env.step(action)\n\n    log['action'].append(float(action))\n    log['ob'].append(float(ob))\n    log['gt'].append(float(ob &gt; 0))\n    log['reward'].append(float(reward))\n\nlog['ob'] = log['ob'][:-1]  # exclude last observation\nlog['gt'] = log['gt'][:-1]  # exclude last observation\n# Visualize\nf, axes = plt.subplots(len(log), 1, sharex=True)\nfor ax, key in zip(axes, log):\n    ax.plot(log[key], '.-')\n    ax.set_ylabel(key)\n</pre> # Logging log = {'ob': [], 'gt': [], 'action': [], 'reward': []}  env = MyDecisionEnv(dt=100) ob, _ = env.reset() log['ob'].append(float(ob)) log['gt'].append(float(ob &gt; 0)) for i in range(30):     action = env.action_space.sample()  # A random agent     ob, reward, terminated, truncated, info = env.step(action)      log['action'].append(float(action))     log['ob'].append(float(ob))     log['gt'].append(float(ob &gt; 0))     log['reward'].append(float(reward))  log['ob'] = log['ob'][:-1]  # exclude last observation log['gt'] = log['gt'][:-1]  # exclude last observation # Visualize f, axes = plt.subplots(len(log), 1, sharex=True) for ax, key in zip(axes, log):     ax.plot(log[key], '.-')     ax.set_ylabel(key) In\u00a0[\u00a0]: Copied! <pre>class MyDecisionEnv(TrialEnv):\n    def __init__(self, dt=100, timing=None):\n        super().__init__(dt=dt)  # dt is passed to base task\n\n        # Setting default task timing\n        self.timing = {'stimulus': 500, 'decision': 500}\n        # Update timing if provided externally\n        if timing:\n            self.timing.update(timing)\n\n        # Here we use ngym.spaces, which allows setting name of each dimension\n        name = {'fixation': 0, 'stimulus': 1}\n        self.observation_space = ngym.spaces.Box(\n            low=-1., high=1., shape=(2,), name=name)\n        name = {'fixation': 0, 'choice': [1, 2]}\n        self.action_space = ngym.spaces.Discrete(3, name=name)\n\n    def _new_trial(self):\n        # Setting time periods for this trial\n        periods = ['stimulus', 'decision']\n        # Will add stimulus and decision periods sequentially using self.timing info\n        self.add_period(periods)\n\n        # Sample observation for the next trial\n        stimulus = np.random.uniform(-1, 1, size=(1,))\n\n        # Add value 1 to stimulus period at fixation location\n        self.add_ob(1, period='stimulus', where='fixation')\n        # Add value stimulus to stimulus period at stimulus location\n        self.add_ob(stimulus, period='stimulus', where='stimulus')\n\n        # Set ground_truth\n        groundtruth = int(stimulus &gt; 0)\n        self.set_groundtruth(groundtruth, period='decision', where='choice')\n\n        trial = dict()\n        trial['stimulus'] = stimulus\n        trial['ground_truth'] = groundtruth\n\n        return trial\n\n    def _step(self, action):\n        # self.ob_now and self.gt_now correspond to\n        # current step observation and groundtruth\n\n        # If action equals to ground_truth, reward=1, otherwise 0\n        reward = (action == self.gt_now) * 1.0\n\n        terminated = False\n        truncated = False\n        # By default, the trial is not ended\n        info = {'new_trial': False}\n        return self.ob_now, reward, terminated, truncated, info\n</pre> class MyDecisionEnv(TrialEnv):     def __init__(self, dt=100, timing=None):         super().__init__(dt=dt)  # dt is passed to base task          # Setting default task timing         self.timing = {'stimulus': 500, 'decision': 500}         # Update timing if provided externally         if timing:             self.timing.update(timing)          # Here we use ngym.spaces, which allows setting name of each dimension         name = {'fixation': 0, 'stimulus': 1}         self.observation_space = ngym.spaces.Box(             low=-1., high=1., shape=(2,), name=name)         name = {'fixation': 0, 'choice': [1, 2]}         self.action_space = ngym.spaces.Discrete(3, name=name)      def _new_trial(self):         # Setting time periods for this trial         periods = ['stimulus', 'decision']         # Will add stimulus and decision periods sequentially using self.timing info         self.add_period(periods)          # Sample observation for the next trial         stimulus = np.random.uniform(-1, 1, size=(1,))          # Add value 1 to stimulus period at fixation location         self.add_ob(1, period='stimulus', where='fixation')         # Add value stimulus to stimulus period at stimulus location         self.add_ob(stimulus, period='stimulus', where='stimulus')          # Set ground_truth         groundtruth = int(stimulus &gt; 0)         self.set_groundtruth(groundtruth, period='decision', where='choice')          trial = dict()         trial['stimulus'] = stimulus         trial['ground_truth'] = groundtruth          return trial      def _step(self, action):         # self.ob_now and self.gt_now correspond to         # current step observation and groundtruth          # If action equals to ground_truth, reward=1, otherwise 0         reward = (action == self.gt_now) * 1.0          terminated = False         truncated = False         # By default, the trial is not ended         info = {'new_trial': False}         return self.ob_now, reward, terminated, truncated, info <p>Sampling one trial. The trial observation and ground-truth can be used for supervised learning.</p> In\u00a0[\u00a0]: Copied! <pre>env = MyDecisionEnv()\nenv.reset()\n\ntrial = env.new_trial()\nob, gt = env.ob, env.gt\n\nprint('Trial information', trial)\nprint('Observation shape is (N_time, N_unit) =', ob.shape)\nprint('Groundtruth shape is (N_time,) =', gt.shape)\n</pre> env = MyDecisionEnv() env.reset()  trial = env.new_trial() ob, gt = env.ob, env.gt  print('Trial information', trial) print('Observation shape is (N_time, N_unit) =', ob.shape) print('Groundtruth shape is (N_time,) =', gt.shape) <p>Visualizing the environment with a helper function.</p> In\u00a0[\u00a0]: Copied! <pre># Run the environment for 2 trials using a random agent.\nfig = ngym.utils.plot_env(\n    env,\n    ob_traces=['stimulus', 'fixation'],\n    num_trials=2,\n)\n</pre> # Run the environment for 2 trials using a random agent. fig = ngym.utils.plot_env(     env,     ob_traces=['stimulus', 'fixation'],     num_trials=2, ) In\u00a0[\u00a0]: Copied! <pre>class PerceptualDecisionMaking(ngym.TrialEnv):\n    \"\"\"Two-alternative forced choice task in which the subject has to\n    integrate two stimuli to decide which one is higher on average.\n\n    Args:\n        stim_scale: Controls the difficulty of the experiment. (def: 1., float)\n        sigma: float, input noise level\n        dim_ring: int, dimension of ring input and output\n    \"\"\"\n    metadata = {\n        'paper_link': 'https://www.jneurosci.org/content/12/12/4745',\n        'paper_name': '''The analysis of visual motion: a comparison of\n        neuronal and psychophysical performance''',\n        'tags': ['perceptual', 'two-alternative', 'supervised']\n    }\n\n    def __init__(self, dt=100, rewards=None, timing=None, stim_scale=1.,\n                 sigma=1.0, dim_ring=2):\n        super().__init__(dt=dt)\n        # The strength of evidence, modulated by stim_scale\n        self.cohs = np.array([0, 6.4, 12.8, 25.6, 51.2]) * stim_scale\n        self.sigma = sigma / np.sqrt(self.dt)  # Input noise\n\n        # Rewards\n        self.rewards = {'abort': -0.1, 'correct': +1., 'fail': 0.}\n        if rewards:\n            self.rewards.update(rewards)\n\n        self.timing = {\n            'fixation': 100,\n            'stimulus': 2000,\n            'delay': 0,\n            'decision': 100}\n        if timing:\n            self.timing.update(timing)\n\n        self.abort = False\n\n        self.theta = np.linspace(0, 2*np.pi, dim_ring+1)[:-1]\n        self.choices = np.arange(dim_ring)\n\n        name = {'fixation': 0, 'stimulus': range(1, dim_ring+1)}\n        self.observation_space = ngym.spaces.Box(\n            -np.inf, np.inf, shape=(1+dim_ring,), dtype=np.float32, name=name)\n        name = {'fixation': 0, 'choice': range(1, dim_ring+1)}\n        self.action_space = ngym.spaces.Discrete(1+dim_ring, name=name)\n\n    def _new_trial(self, **kwargs):\n        # Trial info\n        trial = {\n            'ground_truth': self.rng.choice(self.choices),\n            'coh': self.rng.choice(self.cohs),\n        }\n        trial.update(kwargs)\n\n        coh = trial['coh']\n        ground_truth = trial['ground_truth']\n        stim_theta = self.theta[ground_truth]\n\n        # Periods\n        self.add_period(['fixation', 'stimulus', 'delay', 'decision'])\n\n        # Observations\n        self.add_ob(1, period=['fixation', 'stimulus', 'delay'], where='fixation')\n        stim = np.cos(self.theta - stim_theta) * (coh/200) + 0.5\n        self.add_ob(stim, 'stimulus', where='stimulus')\n        self.add_randn(0, self.sigma, 'stimulus', where='stimulus')\n\n        # Ground truth\n        self.set_groundtruth(ground_truth, period='decision', where='choice')\n\n        return trial\n\n    def _step(self, action):\n        new_trial = False\n        terminated = False\n        truncated = False\n        # rewards\n        reward = 0\n        gt = self.gt_now\n        # observations\n        if self.in_period('fixation'):\n            if action != 0:  # action = 0 means fixating\n                new_trial = self.abort\n                reward += self.rewards['abort']\n        elif self.in_period('decision'):\n            if action != 0:\n                new_trial = True\n                if action == gt:\n                    reward += self.rewards['correct']\n                    self.performance = 1\n                else:\n                    reward += self.rewards['fail']\n\n        return self.ob_now, reward, terminated, truncated, {'new_trial': new_trial, 'gt': gt}\n</pre> class PerceptualDecisionMaking(ngym.TrialEnv):     \"\"\"Two-alternative forced choice task in which the subject has to     integrate two stimuli to decide which one is higher on average.      Args:         stim_scale: Controls the difficulty of the experiment. (def: 1., float)         sigma: float, input noise level         dim_ring: int, dimension of ring input and output     \"\"\"     metadata = {         'paper_link': 'https://www.jneurosci.org/content/12/12/4745',         'paper_name': '''The analysis of visual motion: a comparison of         neuronal and psychophysical performance''',         'tags': ['perceptual', 'two-alternative', 'supervised']     }      def __init__(self, dt=100, rewards=None, timing=None, stim_scale=1.,                  sigma=1.0, dim_ring=2):         super().__init__(dt=dt)         # The strength of evidence, modulated by stim_scale         self.cohs = np.array([0, 6.4, 12.8, 25.6, 51.2]) * stim_scale         self.sigma = sigma / np.sqrt(self.dt)  # Input noise          # Rewards         self.rewards = {'abort': -0.1, 'correct': +1., 'fail': 0.}         if rewards:             self.rewards.update(rewards)          self.timing = {             'fixation': 100,             'stimulus': 2000,             'delay': 0,             'decision': 100}         if timing:             self.timing.update(timing)          self.abort = False          self.theta = np.linspace(0, 2*np.pi, dim_ring+1)[:-1]         self.choices = np.arange(dim_ring)          name = {'fixation': 0, 'stimulus': range(1, dim_ring+1)}         self.observation_space = ngym.spaces.Box(             -np.inf, np.inf, shape=(1+dim_ring,), dtype=np.float32, name=name)         name = {'fixation': 0, 'choice': range(1, dim_ring+1)}         self.action_space = ngym.spaces.Discrete(1+dim_ring, name=name)      def _new_trial(self, **kwargs):         # Trial info         trial = {             'ground_truth': self.rng.choice(self.choices),             'coh': self.rng.choice(self.cohs),         }         trial.update(kwargs)          coh = trial['coh']         ground_truth = trial['ground_truth']         stim_theta = self.theta[ground_truth]          # Periods         self.add_period(['fixation', 'stimulus', 'delay', 'decision'])          # Observations         self.add_ob(1, period=['fixation', 'stimulus', 'delay'], where='fixation')         stim = np.cos(self.theta - stim_theta) * (coh/200) + 0.5         self.add_ob(stim, 'stimulus', where='stimulus')         self.add_randn(0, self.sigma, 'stimulus', where='stimulus')          # Ground truth         self.set_groundtruth(ground_truth, period='decision', where='choice')          return trial      def _step(self, action):         new_trial = False         terminated = False         truncated = False         # rewards         reward = 0         gt = self.gt_now         # observations         if self.in_period('fixation'):             if action != 0:  # action = 0 means fixating                 new_trial = self.abort                 reward += self.rewards['abort']         elif self.in_period('decision'):             if action != 0:                 new_trial = True                 if action == gt:                     reward += self.rewards['correct']                     self.performance = 1                 else:                     reward += self.rewards['fail']          return self.ob_now, reward, terminated, truncated, {'new_trial': new_trial, 'gt': gt} In\u00a0[\u00a0]: Copied! <pre>env = PerceptualDecisionMaking()\nfig = ngym.utils.plot_env(\n    env,\n    ob_traces=['Stim1', 'Stim2', 'fixation'],\n    num_trials=2,\n)\n</pre> env = PerceptualDecisionMaking() fig = ngym.utils.plot_env(     env,     ob_traces=['Stim1', 'Stim2', 'fixation'],     num_trials=2, )"},{"location":"examples/understanding_neurogym_task/#understanding-neurogym-task","title":"Understanding Neurogym Task\u00b6","text":"<p>This is a tutorial for understanding Neurogym task structure. Here we will go through</p> <ol> <li>Defining a basic gymnasium task</li> <li>Defining a basic trial-based neurogym task</li> <li>Adding observation and ground truth in neurogym tasks</li> </ol>"},{"location":"examples/understanding_neurogym_task/#installation","title":"Installation\u00b6","text":"<p>Only needed if running in Google colab. Uncomment to run.</p>"},{"location":"examples/understanding_neurogym_task/#gymnasium-tasks","title":"Gymnasium tasks\u00b6","text":""},{"location":"examples/understanding_neurogym_task/#visualize-results","title":"Visualize results\u00b6","text":""},{"location":"examples/understanding_neurogym_task/#trial-based-neurogym-tasks","title":"Trial-based Neurogym Tasks\u00b6","text":"<p>Many neuroscience and cognitive science tasks have trial structure. <code>neurogym.TrialEnv</code> provides a class for common trial-based tasks. Its main difference from <code>gymnasium.Env</code> is the <code>_new_trial()</code> method that generates abstract information about a new trial, and optionally, the observation and ground-truth output. Additionally, users provide a <code>_step()</code> method instead of <code>step()</code>.</p> <p>The <code>_new_trial()</code> method takes any key-word arguments (<code>**kwargs</code>), and outputs a dictionary <code>trial</code> containing relevant information about this trial. This dictionary is accesible during <code>_step</code> as <code>self.trial</code>.</p> <p>Here we define a simple task where the agent needs to make a binary decision on every trial based on its observation. Each trial is only one time step.</p>"},{"location":"examples/understanding_neurogym_task/#including-time-period-and-observation-in-trial-based-tasks","title":"Including time, period, and observation in trial-based tasks\u00b6","text":"<p>Most neuroscience and cognitive science tasks follow additional temporal structures that are incorporated into <code>neurogym.TrialEnv</code>. These tasks typically</p> <ol> <li>Are described in real time instead of discrete time steps. For example, the task can last 3 seconds.</li> <li>Contain multiple time periods in each trial, such as a stimulus period and a response period.</li> </ol> <p>To include these features, neurogym tasks typically support setting the time length of each step in <code>dt</code> (in ms), and the time length of each time period in <code>timing</code>.</p> <p>For example, consider the following binary decision-making task with a 500ms stimulus period, followed by a 500ms decision period. The periods are added to each trial through <code>self.add_period()</code> in <code>self._new_trial()</code>. During <code>_step()</code>, you can check which period the task is currently in with <code>self.in_period(period_name)</code>.</p>"},{"location":"examples/understanding_neurogym_task/#setting-observation-and-ground-truth-at-the-beginning-of-each-trial","title":"Setting observation and ground-truth at the beginning of each trial\u00b6","text":"<p>In many tasks, the observation and ground-truth are pre-determined for each trial, and can be set in <code>self._new_trial()</code>. The generated observation and ground-truth can then be used as inputs and targets for supervised learning.</p> <p>Observation and ground_truth can be set in <code>self._new_trial()</code> with the <code>self.add_ob()</code> and <code>self.set_groundtruth</code> methods. Users can specify the period and location of the observation using their names. For example, <code>self.add_ob(1, period='stimulus', where='fixation')</code>.</p> <p>This allows the users to access the observation and groundtruth of the entire trial with <code>self.ob</code> and <code>self.gt</code>, and access their values with <code>self.ob_now</code> and <code>self.gt_now</code>.</p>"},{"location":"examples/understanding_neurogym_task/#an-example-perceptual-decision-making-task","title":"An example perceptual decision-making task\u00b6","text":"<p>Using the above style, we can define a simple perceptual decision-making task (the PerceptualDecisionMaking task from neurogym).</p>"}]}