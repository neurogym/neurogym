{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to NeuroGym","text":"<p>NeuroGym is a curated collection of neuroscience tasks with a common interface. The goal is to facilitate training of neural network models on neuroscience tasks.</p> <p>NeuroGym inherits from the machine learning toolkit Gymnasium, a maintained fork of OpenAI\u2019s Gym library. It allows a wide range of well established machine learning algorithms to be easily trained on behavioral paradigms relevant for the neuroscience community. NeuroGym also incorporates several properties and functions (e.g. continuous-time and trial-based tasks) that are important for neuroscience applications. The toolkit also includes various modifier functions that allow easy configuration of new tasks.</p> <p></p> <p>\ud83d\udc1b Bugs reports and \u2b50 features requests here</p> <p>\ud83d\udd27 Pull Requests</p> <p>For more details about how to contribute, see the contribution guidelines.</p>"},{"location":"code_of_conduct/","title":"Code of Conduct","text":""},{"location":"code_of_conduct/#our-pledge","title":"Our Pledge","text":"<p>In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p>"},{"location":"code_of_conduct/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to creating a positive environment include:</p> <ul> <li>Using welcoming and inclusive language</li> <li>Being respectful of differing viewpoints and experiences</li> <li>Gracefully accepting constructive criticism</li> <li>Focusing on what is best for the community</li> <li>Showing empathy towards other community members</li> </ul> <p>Examples of unacceptable behavior by participants include:</p> <ul> <li>The use of sexualized language or imagery and unwelcome sexual attention or   advances</li> <li>Trolling, insulting/derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or electronic   address, without explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"code_of_conduct/#our-responsibilities","title":"Our Responsibilities","text":"<p>Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.</p> <p>Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.</p>"},{"location":"code_of_conduct/#scope","title":"Scope","text":"<p>This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.</p>"},{"location":"code_of_conduct/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at g.crocioni@esciencecenter.nl. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.</p> <p>Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.</p>"},{"location":"code_of_conduct/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html</p>"},{"location":"contributing/","title":"Contributing","text":"<p>We welcome any kind of contribution to our software, from simple comment or question to a full fledged pull request. Please read and follow our Code of Conduct.</p> <p>A contribution can be one of the following cases:</p> <ol> <li>you have a question;</li> <li>you think you may have found a bug (including unexpected behavior);</li> <li>you want to make some kind of change to the code base (e.g. to fix a bug, to add a new feature, to update documentation);</li> <li>you want to make a new release of the code base.</li> </ol> <p>The sections below outline the steps in each case.</p>"},{"location":"contributing/#you-have-a-question","title":"You have a question","text":"<ol> <li>use the search functionality here to see if someone already filed the same issue;</li> <li>if your issue search did not yield any relevant results, make a new issue;</li> <li>apply the \"Question\" label; apply other labels when relevant.</li> </ol>"},{"location":"contributing/#you-think-you-may-have-found-a-bug","title":"You think you may have found a bug","text":"<ol> <li>use the search functionality here to see if someone already filed the same issue;</li> <li>if your issue search did not yield any relevant results, make a new issue, making sure to provide enough information to the rest of the community to understand the cause and context of the problem. Depending on the issue, you may want to include:</li> <li>the SHA hashcode of the commit that is causing your problem;</li> <li>some identifying information (name and version number) for dependencies you're using;</li> <li>information about the operating system;</li> <li>apply relevant labels to the newly created issue.</li> </ol>"},{"location":"contributing/#you-want-to-make-some-kind-of-change-to-the-code-base","title":"You want to make some kind of change to the code base","text":"<ol> <li>(important) announce your plan to the rest of the community before you start working. This announcement should be in the form of a (new) issue;</li> <li>(important) wait until some kind of consensus is reached about your idea being a good idea;</li> <li>if needed, fork the repository to your own Github profile and create your own feature branch off of the latest master commit. While working on your feature branch, make sure to stay up to date with the master branch by pulling in changes, possibly from the 'upstream' repository (follow the instructions here and here);</li> <li>make sure the existing tests still work by running <code>pytest</code>;</li> <li>add your own tests (if necessary);</li> <li>update or expand the documentation;</li> <li>update the <code>CHANGELOG.md</code> file with change;</li> <li>push your feature branch to (your fork of) the annubes repository on GitHub;</li> <li>create the pull request, e.g. following the instructions here.</li> </ol> <p>In case you feel like you've made a valuable contribution, but you don't know how to write or run tests for it, or how to generate the documentation: don't let this discourage you from making the pull request; we can help you! Just go ahead and submit the pull request, but keep in mind that you might be asked to append additional commits to your pull request.</p>"},{"location":"installation/","title":"Installation","text":"<p>Create and activate a virtual environment to install the current package, e.g. using conda (please refer to their site for questions about creating the environment):</p> <pre><code>conda activate # ensures you are in the base environment\nconda create -n neurogym python=3.11\nconda activate neurogym\n</code></pre> <p>Then install neurogym as follows:</p> <pre><code>git clone https://github.com/neurogym/neurogym.git\ncd neurogym\npip install -e .\n</code></pre>"},{"location":"installation/#psychopy-installation","title":"Psychopy installation","text":"<p>If you need psychopy for your project, additionally run</p> <pre><code>pip install psychopy\"\n</code></pre>"},{"location":"license/","title":"License","text":"<p>Apache License</p> <p>Copyright 2024, Giulia Crocioni, Dani L. Bodor, The Netherlands eScience Center</p> <p>Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at</p> <p>http://www.apache.org/licenses/LICENSE-2.0</p> <p>Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p>"},{"location":"neurogym/","title":"NeuroGym","text":""},{"location":"neurogym/#tasks","title":"Tasks","text":"<p>Currently implemented tasks can be found here.</p>"},{"location":"neurogym/#wrappers","title":"Wrappers","text":"<p>Wrappers (see list) are short scripts that allow introducing modifications the original tasks. For instance, the Random Dots Motion task can be transformed into a reaction time task by passing it through the reaction_time wrapper. Alternatively, the combine wrapper allows training an agent in two different tasks simultaneously.</p>"},{"location":"neurogym/#examples","title":"Examples","text":"<p>NeuroGym is compatible with most packages that use gymnasium. In this example jupyter notebook we show how to train a neural network with reinforcement learning algorithms using the Stable-Baselines3 toolbox.</p>"},{"location":"neurogym/#custom-tasks","title":"Custom tasks","text":"<p>Creating custom new tasks should be easy. You can contribute tasks using the regular gymnasium format. If your task has a trial/period structure, this template provides the basic structure that we recommend a task to have:</p> <pre><code>from gymnasium import spaces\nimport neurogym as ngym\n\nclass YourTask(ngym.PeriodEnv):\n    metadata = {}\n\n    def __init__(self, dt=100, timing=None, extra_input_param=None):\n        super().__init__(dt=dt)\n\n\n    def new_trial(self, **kwargs):\n        \"\"\"\n        new_trial() is called when a trial ends to generate the next trial.\n        Here you have to set:\n        The trial periods: fixation, stimulus...\n        Optionally, you can set:\n        The ground truth: the correct answer for the created trial.\n        \"\"\"\n\n    def _step(self, action):\n        \"\"\"\n        _step receives an action and returns:\n            a new observation, obs\n            reward associated with the action, reward\n            a boolean variable indicating whether the experiment has terminated, terminated\n                See more at https://gymnasium.farama.org/tutorials/gymnasium_basics/handling_time_limits/#termination\n            a boolean variable indicating whether the experiment has been truncated, truncated\n                See more at https://gymnasium.farama.org/tutorials/gymnasium_basics/handling_time_limits/#truncation\n            a dictionary with extra information:\n                ground truth correct response, info['gt']\n                boolean indicating the end of the trial, info['new_trial']\n        \"\"\"\n\n        return obs, reward, terminated, truncated, {'new_trial': new_trial, 'gt': gt}\n</code></pre>"},{"location":"api/core/","title":"Core","text":""},{"location":"api/core/#neurogym.core.TrialEnv","title":"TrialEnv","text":"<pre><code>TrialEnv(dt=100, num_trials_before_reset=10000000, r_tmax=0)\n</code></pre> <p>               Bases: <code>BaseEnv</code></p> <p>The main Neurogym class for trial-based envs.</p> Source code in <code>neurogym/core.py</code> <pre><code>def __init__(self, dt=100, num_trials_before_reset=10000000, r_tmax=0) -&gt; None:\n    super().__init__(dt=dt)\n    self.r_tmax = r_tmax\n    self.num_tr = 0\n    self.num_tr_exp = num_trials_before_reset\n    self.trial: dict | None = None\n    self._ob_built = False\n    self._gt_built = False\n    self._has_gt = False  # check if the task ever defined gt\n\n    self._default_ob_value = None  # default to 0\n\n    # For optional periods\n    self.timing: dict = {}\n    self.start_t: dict = {}\n    self.end_t: dict = {}\n    self.start_ind: dict = {}\n    self.end_ind: dict = {}\n    self._tmax = 0  # Length of each trial\n\n    self._top = self\n    self._duration: dict = {}\n</code></pre>"},{"location":"api/core/#neurogym.core.TrialEnv.seed","title":"seed","text":"<pre><code>seed(seed=None)\n</code></pre> <p>Set random seed.</p> Source code in <code>neurogym/core.py</code> <pre><code>def seed(self, seed=None):\n    \"\"\"Set random seed.\"\"\"\n    self.rng = np.random.RandomState(seed)\n    if hasattr(self, \"action_space\") and self.action_space is not None:\n        self.action_space.seed(seed)\n    for val in self.timing.values():\n        with contextlib.suppress(AttributeError):\n            val.seed(seed)\n    return [seed]\n</code></pre>"},{"location":"api/core/#neurogym.core.TrialEnv.post_step","title":"post_step","text":"<pre><code>post_step(ob, reward, terminated, truncated, info)\n</code></pre> <p>Optional task-specific wrapper applied at the end of step.</p> <p>It allows to modify ob online (e.g. provide a specific observation for different actions made by the agent)</p> Source code in <code>neurogym/core.py</code> <pre><code>def post_step(self, ob, reward, terminated, truncated, info):\n    \"\"\"Optional task-specific wrapper applied at the end of step.\n\n    It allows to modify ob online (e.g. provide a specific observation for different actions made by the agent)\n    \"\"\"\n    return ob, reward, terminated, truncated, info\n</code></pre>"},{"location":"api/core/#neurogym.core.TrialEnv.new_trial","title":"new_trial","text":"<pre><code>new_trial(**kwargs)\n</code></pre> <p>Public interface for starting a new trial.</p> <p>Returns:</p> Name Type Description <code>trial</code> <p>dict of trial information. Available to step function as self.trial</p> Source code in <code>neurogym/core.py</code> <pre><code>def new_trial(self, **kwargs):\n    \"\"\"Public interface for starting a new trial.\n\n    Returns:\n        trial: dict of trial information. Available to step function as\n            self.trial\n    \"\"\"\n    # Reset for next trial\n    self._tmax = 0  # reset, self.tmax not reset so it can be used in step\n    self._ob_built = False\n    self._gt_built = False\n    trial = self._new_trial(**kwargs)\n    self.trial = trial\n    self.num_tr += 1  # Increment trial count\n    self._has_gt = self._gt_built\n    return trial\n</code></pre>"},{"location":"api/core/#neurogym.core.TrialEnv.step","title":"step","text":"<pre><code>step(action)\n</code></pre> <p>Public interface for the environment.</p> Source code in <code>neurogym/core.py</code> <pre><code>def step(self, action):\n    \"\"\"Public interface for the environment.\"\"\"\n    ob, reward, terminated, truncated, info = self._step(action)\n\n    if \"new_trial\" not in info:\n        info[\"new_trial\"] = False\n\n    if self._has_gt and \"gt\" not in info:\n        # If gt is built, default gt to gt_now\n        # must run before incrementing t\n        info[\"gt\"] = self.gt_now\n\n    self.t += self.dt  # increment within trial time count\n    self.t_ind += 1\n\n    if self.t + self.dt &gt; self.tmax and not info[\"new_trial\"]:\n        info[\"new_trial\"] = True\n        reward += self.r_tmax\n\n    # TODO: new_trial happens after step, so trial indx precedes obs change\n    if info[\"new_trial\"]:\n        info[\"performance\"] = self.performance\n        self.t = self.t_ind = 0  # Reset within trial time count\n        trial = self._top.new_trial()\n        self.performance = 0\n        info[\"trial\"] = trial\n    if ob is OBNOW:\n        ob = self.ob[self.t_ind]\n    return self.post_step(ob, reward, terminated, truncated, info)\n</code></pre>"},{"location":"api/core/#neurogym.core.TrialEnv.reset","title":"reset","text":"<pre><code>reset(seed=None, options=None)\n</code></pre> <p>Reset the environment.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <p>random seed, overwrites self.seed if not None</p> <code>None</code> <code>options</code> <p>additional options used to reset the env. Can include 'step_fn' and 'no_step'. <code>step_fn</code> can be a function or None. If function, overwrite original <code>self.step</code> method. <code>no_step</code> is a bool. If True, no step is taken and observation randomly sampled. It defaults to False.</p> <code>None</code> Source code in <code>neurogym/core.py</code> <pre><code>def reset(self, seed=None, options=None):\n    \"\"\"Reset the environment.\n\n    Args:\n        seed: random seed, overwrites self.seed if not None\n        options: additional options used to reset the env.\n            Can include 'step_fn' and 'no_step'.\n            `step_fn` can be a function or None. If function, overwrite original\n            `self.step` method.\n            `no_step` is a bool. If True, no step is taken and observation randomly\n            sampled. It defaults to False.\n    \"\"\"\n    super().reset(seed=seed)\n\n    self.num_tr = 0\n    self.t = self.t_ind = 0\n\n    step_fn = options.get(\"step_fn\") if options else None\n    no_step = options.get(\"no_step\", False) if options else False\n\n    self._top.new_trial()\n\n    # have to also call step() to get the initial ob since some wrappers modify step() but not new_trial()\n    self.action_space.seed(0)\n    if no_step:\n        return self.observation_space.sample(), {}\n    if step_fn is None:\n        ob, _, _, _, _ = self._top.step(self.action_space.sample())\n    else:\n        ob, _, _, _, _ = step_fn(self.action_space.sample())\n    return ob, {}\n</code></pre>"},{"location":"api/core/#neurogym.core.TrialEnv.render","title":"render","text":"<pre><code>render(mode='human') -&gt; None\n</code></pre> <p>Plots relevant variables/parameters.</p> Source code in <code>neurogym/core.py</code> <pre><code>def render(self, mode=\"human\") -&gt; None:\n    \"\"\"Plots relevant variables/parameters.\"\"\"\n</code></pre>"},{"location":"api/core/#neurogym.core.TrialEnv.set_top","title":"set_top","text":"<pre><code>set_top(wrapper) -&gt; None\n</code></pre> <p>Set top to be wrapper.</p> Source code in <code>neurogym/core.py</code> <pre><code>def set_top(self, wrapper) -&gt; None:\n    \"\"\"Set top to be wrapper.\"\"\"\n    self._top = wrapper\n</code></pre>"},{"location":"api/core/#neurogym.core.TrialEnv.add_period","title":"add_period","text":"<pre><code>add_period(period, duration=None, before=None, after=None, last_period=False) -&gt; None\n</code></pre> <p>Add an period.</p> <p>Parameters:</p> Name Type Description Default <code>period</code> <p>string or list of strings, name of the period</p> required <code>duration</code> <p>float or None, duration of the period if None, inferred from timing_fn</p> <code>None</code> <code>before</code> <p>(optional) str, name of period that this period is before</p> <code>None</code> <code>after</code> <p>(optional) str, name of period that this period is after or float, time of period start</p> <code>None</code> <code>last_period</code> <p>bool, default False. If True, then this is last period will generate self.tmax, self.tind, and self.ob</p> <code>False</code> Source code in <code>neurogym/core.py</code> <pre><code>def add_period(\n    self,\n    period,\n    duration=None,\n    before=None,\n    after=None,\n    last_period=False,\n) -&gt; None:\n    \"\"\"Add an period.\n\n    Args:\n        period: string or list of strings, name of the period\n        duration: float or None, duration of the period\n            if None, inferred from timing_fn\n        before: (optional) str, name of period that this period is before\n        after: (optional) str, name of period that this period is after\n            or float, time of period start\n        last_period: bool, default False. If True, then this is last period\n            will generate self.tmax, self.tind, and self.ob\n    \"\"\"\n    if self._ob_built:\n        msg = \"Cannot add period after ob is built, i.e. after running add_ob.\"\n        raise InvalidOperationError(msg)\n    if isinstance(period, str):\n        pass\n    else:\n        if duration is None:\n            duration = [None] * len(period)\n        elif len(duration) != len(period):\n            msg = f\"{len(duration)=} and {len(period)=} must be the same.\"\n            raise ValueError(msg)\n\n        # Recursively calling itself\n        self.add_period(period[0], duration=duration[0], after=after)\n        for i in range(1, len(period)):\n            is_last = (i == len(period) - 1) and last_period\n            self.add_period(\n                period[i],\n                duration=duration[i],\n                after=period[i - 1],\n                last_period=is_last,\n            )\n        return\n\n    if duration is None:\n        duration = self.sample_time(period)\n    self._duration[period] = duration\n\n    if after is not None:\n        start = self.end_t[after] if isinstance(after, str) else after\n    elif before is not None:\n        start = self.start_t[before] - duration\n    else:\n        start = 0  # default start with 0\n\n    self.start_t[period] = start\n    self.end_t[period] = start + duration\n    self.start_ind[period] = int(start / self.dt)\n    self.end_ind[period] = int((start + duration) / self.dt)\n\n    self._tmax = max(self._tmax, start + duration)\n    self.tmax = int(self._tmax / self.dt) * self.dt\n</code></pre>"},{"location":"api/core/#neurogym.core.TrialEnv.view_ob","title":"view_ob","text":"<pre><code>view_ob(period=None)\n</code></pre> <p>View observation of an period.</p> Source code in <code>neurogym/core.py</code> <pre><code>def view_ob(self, period=None):\n    \"\"\"View observation of an period.\"\"\"\n    if not self._ob_built:\n        self._init_ob()\n\n    if period is None:\n        return self.ob\n    return self.ob[self.start_ind[period] : self.end_ind[period]]\n</code></pre>"},{"location":"api/core/#neurogym.core.TrialEnv.add_ob","title":"add_ob","text":"<pre><code>add_ob(value, period=None, where=None) -&gt; None\n</code></pre> <p>Add value to observation.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <p>array-like (ob_space.shape, ...)</p> required <code>period</code> <p>string, must be name of an added period</p> <code>None</code> <code>where</code> <p>string or np array, location of stimulus to be added</p> <code>None</code> Source code in <code>neurogym/core.py</code> <pre><code>def add_ob(self, value, period=None, where=None) -&gt; None:\n    \"\"\"Add value to observation.\n\n    Args:\n        value: array-like (ob_space.shape, ...)\n        period: string, must be name of an added period\n        where: string or np array, location of stimulus to be added\n    \"\"\"\n    self._add_ob(value, period, where, reset=False)\n</code></pre>"},{"location":"api/core/#neurogym.core.TrialEnv.set_groundtruth","title":"set_groundtruth","text":"<pre><code>set_groundtruth(value, period=None, where=None) -&gt; None\n</code></pre> <p>Set groundtruth value.</p> Source code in <code>neurogym/core.py</code> <pre><code>def set_groundtruth(self, value, period=None, where=None) -&gt; None:\n    \"\"\"Set groundtruth value.\"\"\"\n    if not self._gt_built:\n        self._init_gt()\n\n    if where is not None:\n        # TODO: Only works for Discrete action_space, make it work for Box\n        value = self.action_space.name[where][value]  # type: ignore[attr-defined]\n    if isinstance(period, str):\n        self.gt[self.start_ind[period] : self.end_ind[period]] = value\n    elif period is None:\n        self.gt[:] = value\n    else:\n        for p in period:\n            self.set_groundtruth(value, p)\n</code></pre>"},{"location":"api/core/#neurogym.core.TrialEnv.view_groundtruth","title":"view_groundtruth","text":"<pre><code>view_groundtruth(period)\n</code></pre> <p>View observation of an period.</p> Source code in <code>neurogym/core.py</code> <pre><code>def view_groundtruth(self, period):\n    \"\"\"View observation of an period.\"\"\"\n    if not self._gt_built:\n        self._init_gt()\n    return self.gt[self.start_ind[period] : self.end_ind[period]]\n</code></pre>"},{"location":"api/core/#neurogym.core.TrialEnv.in_period","title":"in_period","text":"<pre><code>in_period(period, t=None)\n</code></pre> <p>Check if current time or time t is in period.</p> Source code in <code>neurogym/core.py</code> <pre><code>def in_period(self, period, t=None):\n    \"\"\"Check if current time or time t is in period.\"\"\"\n    if t is None:\n        t = self.t  # Default\n    return self.start_t[period] &lt;= t &lt; self.end_t[period]\n</code></pre>"},{"location":"api/core/#neurogym.core.BaseEnv","title":"BaseEnv","text":"<pre><code>BaseEnv(dt=100)\n</code></pre> <p>               Bases: <code>Env</code></p> <p>The base Neurogym class to include dt.</p> Source code in <code>neurogym/core.py</code> <pre><code>def __init__(self, dt=100) -&gt; None:\n    super().__init__()\n    self.dt = dt\n    self.t = self.t_ind = 0\n    self.tmax = 10000  # maximum time steps\n    self.performance = 0\n    self.rewards: dict = {}\n    self.rng = np.random.RandomState()\n</code></pre>"},{"location":"api/core/#neurogym.core.BaseEnv.seed","title":"seed","text":"<pre><code>seed(seed=None)\n</code></pre> <p>Set random seed.</p> Source code in <code>neurogym/core.py</code> <pre><code>def seed(self, seed=None):\n    \"\"\"Set random seed.\"\"\"\n    self.rng = np.random.RandomState(seed)\n    if self.action_space is not None:\n        self.action_space.seed(seed)\n    return [seed]\n</code></pre>"},{"location":"api/envs/tags/","title":"Tags","text":""},{"location":"api/envs/tags/#confidence","title":"Confidence","text":""},{"location":"api/envs/tags/#neurogym.envs.postdecisionwager.PostDecisionWager","title":"PostDecisionWager","text":"<pre><code>PostDecisionWager(dt=100, rewards=None, timing=None, dim_ring=2, sigma=1.0)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>Post-decision wagering task assessing confidence.</p> <p>The agent first performs a perceptual discrimination task (see for more details the PerceptualDecisionMaking task). On a random half of the trials, the agent is given the option to abort the sensory discrimination and to choose instead a sure-bet option that guarantees a small reward. Therefore, the agent is encouraged to choose the sure-bet option when it is uncertain about its perceptual decision.</p> Source code in <code>neurogym/envs/postdecisionwager.py</code> <pre><code>def __init__(self, dt=100, rewards=None, timing=None, dim_ring=2, sigma=1.0) -&gt; None:\n    super().__init__(dt=dt)\n\n    self.wagers = [True, False]\n    self.theta = np.linspace(0, 2 * np.pi, dim_ring + 1)[:-1]\n    self.choices = np.arange(dim_ring)\n    self.cohs = [0, 3.2, 6.4, 12.8, 25.6, 51.2]\n    self.sigma = sigma / np.sqrt(self.dt)  # Input noise\n\n    # Rewards\n    self.rewards = {\"abort\": -0.1, \"correct\": +1.0, \"fail\": 0.0}\n    if rewards:\n        self.rewards.update(rewards)\n    self.rewards[\"sure\"] = 0.7 * self.rewards[\"correct\"]\n\n    self.timing = {\n        \"fixation\": 100,\n        # 'target':  0,  # noqa: ERA001\n        \"stimulus\": ngym.random.TruncExp(180, 100, 900),\n        \"delay\": ngym.random.TruncExp(1350, 1200, 1800),\n        \"pre_sure\": lambda: self.rng.uniform(500, 750),\n        \"decision\": 100,\n    }\n    if timing:\n        self.timing.update(timing)\n\n    self.abort = False\n\n    # set action and observation space\n    name = {\"fixation\": 0, \"stimulus\": [1, 2], \"sure\": 3}\n    self.observation_space = spaces.Box(\n        -np.inf,\n        np.inf,\n        shape=(4,),\n        dtype=np.float32,\n        name=name,\n    )\n    name = {\"fixation\": 0, \"choice\": [1, 2], \"sure\": 3}\n    self.action_space = spaces.Discrete(4, name=name)\n</code></pre>"},{"location":"api/envs/tags/#context-dependent","title":"Context Dependent","text":""},{"location":"api/envs/tags/#neurogym.envs.contextdecisionmaking.ContextDecisionMaking","title":"ContextDecisionMaking","text":"<pre><code>ContextDecisionMaking(dt=100, rewards=None, timing=None, sigma=1.0)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>Context-dependent decision-making task.</p> <p>The agent simultaneously receives stimulus inputs from two modalities ( for example, a colored random dot motion pattern with color and motion modalities). The agent needs to make a perceptual decision based on only one of the two modalities, while ignoring the other. The relevant modality is explicitly indicated by a rule signal.</p> Source code in <code>neurogym/envs/contextdecisionmaking.py</code> <pre><code>def __init__(self, dt=100, rewards=None, timing=None, sigma=1.0) -&gt; None:\n    super().__init__(dt=dt)\n\n    # trial conditions\n    self.contexts = [0, 1]  # index for context inputs\n    self.choices = [1, 2]  # left, right choice\n    self.cohs = [5, 15, 50]\n    self.sigma = sigma / np.sqrt(self.dt)  # Input noise\n\n    # Rewards\n    self.rewards = {\"abort\": -0.1, \"correct\": +1.0}\n    if rewards:\n        self.rewards.update(rewards)\n\n    self.timing = {\n        \"fixation\": 300,\n        # 'target': 350, # noqa: ERA001\n        \"stimulus\": 750,\n        \"delay\": ngym.random.TruncExp(600, 300, 3000),\n        \"decision\": 100,\n    }\n    if timing:\n        self.timing.update(timing)\n\n    self.abort = False\n\n    # set action and observation space\n    names = [\n        \"fixation\",\n        \"stim1_mod1\",\n        \"stim2_mod1\",\n        \"stim1_mod2\",\n        \"stim2_mod2\",\n        \"context1\",\n        \"context2\",\n    ]\n    name = {name: i for i, name in enumerate(names)}\n    self.observation_space = spaces.Box(\n        -np.inf,\n        np.inf,\n        shape=(7,),\n        dtype=np.float32,\n        name=name,\n    )\n\n    name = {\"fixation\": 0, \"choice1\": 1, \"choice2\": 2}\n    self.action_space = spaces.Discrete(3, name=name)\n</code></pre>"},{"location":"api/envs/tags/#neurogym.envs.contextdecisionmaking.SingleContextDecisionMaking","title":"SingleContextDecisionMaking","text":"<pre><code>SingleContextDecisionMaking(dt=100, context=0, rewards=None, timing=None, sigma=1.0, dim_ring=2)\n</code></pre> <p>               Bases: <code>TrialEnv</code></p> <p>Context-dependent decision-making task.</p> <p>The agent simultaneously receives stimulus inputs from two modalities ( for example, a colored random dot motion pattern with color and motion modalities). The agent needs to make a perceptual decision based on only one of the two modalities, while ignoring the other. The agent reports its decision during the decision period, with an optional delay period in between the stimulus period and the decision period. The relevant modality is not explicitly signaled.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <p>int, 0 or 1 for the two context (rules). If 0, need to focus on modality 0 (the first one)</p> <code>0</code> Source code in <code>neurogym/envs/contextdecisionmaking.py</code> <pre><code>def __init__(\n    self,\n    dt=100,\n    context=0,\n    rewards=None,\n    timing=None,\n    sigma=1.0,\n    dim_ring=2,\n) -&gt; None:\n    super().__init__(dt=dt)\n\n    # trial conditions\n    self.cohs = [5, 15, 50]\n    self.sigma = sigma / np.sqrt(self.dt)  # Input noise\n    self.context = context\n\n    # Rewards\n    self.rewards = {\"abort\": -0.1, \"correct\": +1.0}\n    if rewards:\n        self.rewards.update(rewards)\n\n    self.timing = {\n        \"fixation\": 300,\n        # 'target': 350, # noqa: ERA001\n        \"stimulus\": 750,\n        \"delay\": ngym.random.TruncExp(600, 300, 3000),\n        \"decision\": 100,\n    }\n    if timing:\n        self.timing.update(timing)\n\n    self.abort = False\n\n    # set action and observation space\n    self.theta = np.linspace(0, 2 * np.pi, dim_ring + 1)[:-1]\n    self.choices = np.arange(dim_ring)\n\n    name = {\n        \"fixation\": 0,\n        \"stimulus_mod1\": range(1, dim_ring + 1),\n        \"stimulus_mod2\": range(dim_ring + 1, 2 * dim_ring + 1),\n    }\n    shape = (1 + 2 * dim_ring,)\n    self.observation_space = spaces.Box(\n        -np.inf,\n        np.inf,\n        shape=shape,\n        dtype=np.float32,\n        name=name,\n    )\n\n    name = {\"fixation\": 0, \"choice\": range(1, dim_ring + 1)}\n    self.action_space = spaces.Discrete(1 + dim_ring, name=name)\n</code></pre>"},{"location":"examples/demo/","title":"Demo","text":"<p>NeuroGym is a comprehensive toolkit that allows training any network model on many established neuroscience tasks using Reinforcement Learning techniques. It includes working memory tasks, value-based decision tasks and context-dependent perceptual categorization tasks.</p> <p>In this notebook we first show how to install the relevant toolbox.</p> <p>We then show how to access the available tasks and their relevant information.</p> <p>Finally we train an LSTM network on the Random Dots Motion task using the A2C algorithm Mnih et al. 2016 implemented in the stable-baselines3 toolbox, and plot the results.</p> <p>You can easily change the code to train a network on any other available task or using a different algorithm (e.g. ACER, PPO2).</p> In\u00a0[6]: Copied! <pre># Install gymnasium\n! pip install gymnasium\n# Install neurogym\n! git clone https://github.com/gyyang/neurogym.git\n%cd neurogym/\n! pip install -e .\n# Install stable-baselines3\n! pip install stable-baselines3\n</pre> # Install gymnasium ! pip install gymnasium # Install neurogym ! git clone https://github.com/gyyang/neurogym.git %cd neurogym/ ! pip install -e . # Install stable-baselines3 ! pip install stable-baselines3 <pre>UsageError: Line magic function `%tensorflow_version` not found.\n</pre> In\u00a0[49]: Copied! <pre>import warnings\nimport gymnasium as gym\nimport neurogym as ngym\nfrom neurogym.utils import info, plotting\nwarnings.filterwarnings('ignore')\ninfo.all_tasks()\n</pre> import warnings import gymnasium as gym import neurogym as ngym from neurogym.utils import info, plotting warnings.filterwarnings('ignore') info.all_tasks() <pre>AntiReach-v0\nBandit-v0\nContextDecisionMaking-v0\nDawTwoStep-v0\nDelayComparison-v0\nDelayMatchCategory-v0\nDelayMatchSample-v0\nDelayMatchSampleDistractor1D-v0\nDelayPairedAssociation-v0\nDualDelayMatchSample-v0\nEconomicDecisionMaking-v0\nGoNogo-v0\nHierarchicalReasoning-v0\nIntervalDiscrimination-v0\nMotorTiming-v0\nMultiSensoryIntegration-v0\nNull-v0\nOneTwoThreeGo-v0\nPerceptualDecisionMaking-v0\nPerceptualDecisionMakingDelayResponse-v0\nPostDecisionWager-v0\nProbabilisticReasoning-v0\nPulseDecisionMaking-v0\nReaching1D-v0\nReaching1DWithSelfDistraction-v0\nReachingDelayResponse-v0\nReadySetGo-v0\nSingleContextDecisionMaking-v0\npsychopy.RandomDotMotion-v0\npsychopy.SpatialSuppressMotion-v0\npsychopy.VisualSearch-v0\n</pre> In\u00a0[50]: Copied! <pre>task = 'GoNogo-v0'\nenv = gym.make(task)\nprint(env)\n# plotting.plot_env(env, num_steps=300, def_act=0, ob_traces=['Fixation cue', 'Stim1', 'Stim2'], fig_kwargs={'figsize': (12, 12)})\nfig = plotting.plot_env(\n    env,\n    num_steps=100,\n    # def_act=0,\n    ob_traces=['Fixation cue', 'NoGo', 'Go'],\n    # fig_kwargs={'figsize': (12, 12)}\n    )\n</pre> task = 'GoNogo-v0' env = gym.make(task) print(env) # plotting.plot_env(env, num_steps=300, def_act=0, ob_traces=['Fixation cue', 'Stim1', 'Stim2'], fig_kwargs={'figsize': (12, 12)}) fig = plotting.plot_env(     env,     num_steps=100,     # def_act=0,     ob_traces=['Fixation cue', 'NoGo', 'Go'],     # fig_kwargs={'figsize': (12, 12)}     ) <pre>&lt;OrderEnforcing&lt;PassiveEnvChecker&lt;GoNogo&gt;&gt;&gt;\n</pre> In\u00a0[51]: Copied! <pre>info.all_wrappers()\n</pre> info.all_wrappers() <pre>Monitor-v0\nNoise-v0\nPassAction-v0\nPassReward-v0\nRandomGroundTruth-v0\nReactionTime-v0\nScheduleAttr-v0\nScheduleEnvs-v0\nSideBias-v0\nTrialHistoryV2-v0\n</pre> In\u00a0[52]: Copied! <pre>info.info_wrapper('TrialHistoryV2-v0', show_code=True)\n</pre> info.info_wrapper('TrialHistoryV2-v0', show_code=True) Out[52]: <pre>'### TrialHistoryV2-v0\\n\\nLogic: Missing description\\n\\n\\n#### Source code #### \\n\\nclass TrialHistoryV2(TrialWrapper):\\n    \"\"\"Change ground truth probability based on previous outcome.\\n\\n    Args:\\n        probs: matrix of probabilities of the current choice conditioned\\n            on the previous. Shape, num-choices x num-choices\\n    \"\"\"\\n\\n    def __init__(self, env, probs=None):\\n        super().__init__(env)\\n        try:\\n            self.n_ch = len(self.choices)  # max num of choices\\n        except AttributeError:\\n            raise AttributeError(\\n                \"TrialHistory requires task to \" \"have attribute choices\"\\n            )\\n        if probs is None:\\n            probs = np.ones((self.n_ch, self.n_ch)) / self.n_ch  # uniform\\n        self.probs = probs\\n        assert self.probs.shape == (self.n_ch, self.n_ch), (\\n            \"probs shape wrong, should be\" + str((self.n_ch, self.n_ch))\\n        )\\n        self.prev_trial = self.rng.choice(self.n_ch)  # random initialization\\n\\n    def new_trial(self, **kwargs):\\n        if \"probs\" in kwargs:\\n            probs = kwargs[\"probs\"]\\n        else:\\n            probs = self.probs\\n        p = probs[self.prev_trial, :]\\n        # Choose ground truth and update previous trial info\\n        self.prev_trial = self.rng.choice(self.n_ch, p=p)\\n        ground_truth = self.choices[self.prev_trial]\\n        kwargs.update({\"ground_truth\": ground_truth, \"probs\": probs})\\n        return self.env.new_trial(**kwargs)\\n\\n\\n'</pre> In\u00a0[53]: Copied! <pre>import warnings\nimport numpy as np\nfrom neurogym.wrappers import monitor, TrialHistoryV2\nfrom stable_baselines3.common.vec_env import DummyVecEnv\nfrom stable_baselines3 import A2C  # ACER, PPO2\nwarnings.filterwarnings('default')\n# task paremters\ntiming = {'fixation': ('constant', 300),\n          'stimulus': ('constant', 700),\n          'decision': ('constant', 300)}\nkwargs = {'dt': 100, 'timing': timing}\n# wrapper parameters\nn_ch = 2\np = 0.8\nnum_blocks = 2\nprobs = np.array([[p, 1-p], [1-p, p]])  # repeating block\n\n# build task\nenv = gym.make(task, **kwargs)\n# Apply the wrapper\nenv = TrialHistoryV2(env, probs=probs)\nenv = monitor.Monitor(env, folder='content/tests/', sv_per=10000, verbose=1, sv_fig=True, num_stps_sv_fig=100)\n# the env is now wrapped automatically when passing it to the constructor\nenv = DummyVecEnv([lambda: env])\nmodel = A2C(\"MlpPolicy\", env, verbose=1, policy_kwargs={'net_arch': [64, 64]})\nmodel.learn(total_timesteps=500000, log_interval=100000)\nenv.close()\n</pre> import warnings import numpy as np from neurogym.wrappers import monitor, TrialHistoryV2 from stable_baselines3.common.vec_env import DummyVecEnv from stable_baselines3 import A2C  # ACER, PPO2 warnings.filterwarnings('default') # task paremters timing = {'fixation': ('constant', 300),           'stimulus': ('constant', 700),           'decision': ('constant', 300)} kwargs = {'dt': 100, 'timing': timing} # wrapper parameters n_ch = 2 p = 0.8 num_blocks = 2 probs = np.array([[p, 1-p], [1-p, p]])  # repeating block  # build task env = gym.make(task, **kwargs) # Apply the wrapper env = TrialHistoryV2(env, probs=probs) env = monitor.Monitor(env, folder='content/tests/', sv_per=10000, verbose=1, sv_fig=True, num_stps_sv_fig=100) # the env is now wrapped automatically when passing it to the constructor env = DummyVecEnv([lambda: env]) model = A2C(\"MlpPolicy\", env, verbose=1, policy_kwargs={'net_arch': [64, 64]}) model.learn(total_timesteps=500000, log_interval=100000) env.close() <pre>/Users/giuliacrocioni/miniforge3/envs/neurogym/lib/python3.10/site-packages/gymnasium/envs/registration.py:481: UserWarning: WARN: The environment creator metadata doesn't include `render_modes`, contains: ['paper_link', 'paper_name', 'tags']\n  logger.warn(\n/Users/giuliacrocioni/miniforge3/envs/neurogym/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: WARN: env.choices to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.choices` for environment variables or `env.get_wrapper_attr('choices')` that will search the reminding wrappers.\n  logger.warn(\n/Users/giuliacrocioni/miniforge3/envs/neurogym/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: WARN: env.rng to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.rng` for environment variables or `env.get_wrapper_attr('rng')` that will search the reminding wrappers.\n  logger.warn(\n/Users/giuliacrocioni/miniforge3/envs/neurogym/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: WARN: env.new_trial to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.new_trial` for environment variables or `env.get_wrapper_attr('new_trial')` that will search the reminding wrappers.\n  logger.warn(\n</pre> <pre>Using cpu device\n--------------------\nNumber of steps:  10000.0\nAverage reward:  0.27335\n--------------------\n--------------------\nNumber of steps:  20000.0\nAverage reward:  0.2262\n--------------------\n--------------------\nNumber of steps:  30000.0\nAverage reward:  0.20145\n--------------------\n-------------------------------------\n| time/                 |           |\n|    fps                | 2300      |\n|    iterations         | 100000    |\n|    time_elapsed       | 217       |\n|    total_timesteps    | 500000    |\n| train/                |           |\n|    entropy_loss       | -0.000595 |\n|    explained_variance | -6.48     |\n|    learning_rate      | 0.0007    |\n|    n_updates          | 99999     |\n|    policy_loss        | -1.45e-06 |\n|    value_loss         | 0.000583  |\n-------------------------------------\n</pre> In\u00a0[56]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n# Create task\nenv = gym.make(task, **kwargs)\n# Apply the wrapper\nenv = TrialHistoryV2(env, probs=probs)\nenv = DummyVecEnv([lambda: env])\nfig = plotting.plot_env(\n    env,\n    num_steps=100,\n    # def_act=0,\n    ob_traces=['Fixation cue', 'NoGo', 'Go'],\n    # fig_kwargs={'figsize': (12, 12)},\n    model=model)\n</pre> import numpy as np import matplotlib.pyplot as plt # Create task env = gym.make(task, **kwargs) # Apply the wrapper env = TrialHistoryV2(env, probs=probs) env = DummyVecEnv([lambda: env]) fig = plotting.plot_env(     env,     num_steps=100,     # def_act=0,     ob_traces=['Fixation cue', 'NoGo', 'Go'],     # fig_kwargs={'figsize': (12, 12)},     model=model) <pre>/Users/giuliacrocioni/miniforge3/envs/neurogym/lib/python3.10/site-packages/gymnasium/envs/registration.py:481: UserWarning: WARN: The environment creator metadata doesn't include `render_modes`, contains: ['paper_link', 'paper_name', 'tags']\n  logger.warn(\n/Users/giuliacrocioni/miniforge3/envs/neurogym/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: WARN: env.choices to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.choices` for environment variables or `env.get_wrapper_attr('choices')` that will search the reminding wrappers.\n  logger.warn(\n/Users/giuliacrocioni/miniforge3/envs/neurogym/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: WARN: env.rng to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.rng` for environment variables or `env.get_wrapper_attr('rng')` that will search the reminding wrappers.\n  logger.warn(\n/Users/giuliacrocioni/miniforge3/envs/neurogym/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: WARN: env.new_trial to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.new_trial` for environment variables or `env.get_wrapper_attr('new_trial')` that will search the reminding wrappers.\n  logger.warn(\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/demo/#exploring-neurogym-tasks","title":"Exploring NeuroGym tasks\u00b6","text":""},{"location":"examples/demo/#installation-on-google-colab","title":"Installation on google colab\u00b6","text":""},{"location":"examples/demo/#explore-tasks","title":"Explore tasks\u00b6","text":""},{"location":"examples/demo/#visualize-a-single-task","title":"Visualize a single task\u00b6","text":""},{"location":"examples/demo/#explore-wrappers","title":"Explore wrappers\u00b6","text":""},{"location":"examples/demo/#train-a-network","title":"Train a network\u00b6","text":""},{"location":"examples/demo/#visualize-results","title":"Visualize results\u00b6","text":""},{"location":"examples/example_annubes/","title":"Example annubes","text":"<p>This notebook is a simple example of how to use the <code>AnnubesEnv</code> class to create a custom environment and use it to train a reinforcement learning agent with <code>stable_baselines3</code>.</p> In\u00a0[\u00a0]: Copied! <pre>import neurogym as ngym\nfrom neurogym.envs.annubes import AnnubesEnv\nfrom stable_baselines3.common.env_checker import check_env\n\nenv = AnnubesEnv()\n\n# check the custom environment and output additional warnings (if any)\ncheck_env(env)\n\n# check the environment with a random agent\nobs, info = env.reset()\nn_steps = 10\nfor _ in range(n_steps):\n    # random action\n    action = env.action_space.sample()\n    obs, reward, terminated, truncated, info = env.step(action)\n    if terminated:\n        obs, info = env.reset()\n\nprint(env.timing)\nprint(\"----------------\")\nprint(env.observation_space)\nprint(env.observation_space.name)\nprint(\"----------------\")\nprint(env.action_space)\nprint(env.action_space.name)\n</pre> import neurogym as ngym from neurogym.envs.annubes import AnnubesEnv from stable_baselines3.common.env_checker import check_env  env = AnnubesEnv()  # check the custom environment and output additional warnings (if any) check_env(env)  # check the environment with a random agent obs, info = env.reset() n_steps = 10 for _ in range(n_steps):     # random action     action = env.action_space.sample()     obs, reward, terminated, truncated, info = env.step(action)     if terminated:         obs, info = env.reset()  print(env.timing) print(\"----------------\") print(env.observation_space) print(env.observation_space.name) print(\"----------------\") print(env.action_space) print(env.action_space.name) In\u00a0[\u00a0]: Copied! <pre>fig = ngym.utils.plot_env(\n    env,\n    ob_traces=[\"fixation\", \"start\", \"v\", \"a\"],\n    num_trials=10\n)\n</pre> fig = ngym.utils.plot_env(     env,     ob_traces=[\"fixation\", \"start\", \"v\", \"a\"],     num_trials=10 ) In\u00a0[\u00a0]: Copied! <pre>import warnings\n\nfrom stable_baselines3 import A2C\nfrom stable_baselines3.common.vec_env import DummyVecEnv\n\nwarnings.filterwarnings(\"default\")\n\n# train agent\nenv = AnnubesEnv()\nenv_vec = DummyVecEnv([lambda: env])\nmodel = A2C(\"MlpPolicy\", env_vec, verbose=0)\nmodel.learn(total_timesteps=20000, log_interval=1000)\nenv_vec.close()\n\n# plot example trials with trained agent\ndata = ngym.utils.plot_env(env, num_trials=10, ob_traces=[\"fixation\", \"start\", \"v\", \"a\"], model=model)\n</pre> import warnings  from stable_baselines3 import A2C from stable_baselines3.common.vec_env import DummyVecEnv  warnings.filterwarnings(\"default\")  # train agent env = AnnubesEnv() env_vec = DummyVecEnv([lambda: env]) model = A2C(\"MlpPolicy\", env_vec, verbose=0) model.learn(total_timesteps=20000, log_interval=1000) env_vec.close()  # plot example trials with trained agent data = ngym.utils.plot_env(env, num_trials=10, ob_traces=[\"fixation\", \"start\", \"v\", \"a\"], model=model) In\u00a0[\u00a0]: Copied! <pre>env1 = AnnubesEnv({\"v\": 1, \"a\": 0})\nenv1_vec = DummyVecEnv([lambda: env1])\n# create a model and train it with the first environment\nmodel = A2C(\"MlpPolicy\", env1_vec, verbose=0)\nmodel.learn(total_timesteps=10000)\nenv1_vec.close()\n\n# plot example trials with trained agent\ndata = ngym.utils.plot_env(env1, num_trials=10, ob_traces=[\"fixation\", \"start\", \"v\", \"a\"], model=model)\n</pre> env1 = AnnubesEnv({\"v\": 1, \"a\": 0}) env1_vec = DummyVecEnv([lambda: env1]) # create a model and train it with the first environment model = A2C(\"MlpPolicy\", env1_vec, verbose=0) model.learn(total_timesteps=10000) env1_vec.close()  # plot example trials with trained agent data = ngym.utils.plot_env(env1, num_trials=10, ob_traces=[\"fixation\", \"start\", \"v\", \"a\"], model=model) In\u00a0[\u00a0]: Copied! <pre># switch to the second environment and continue training\nenv2 = AnnubesEnv({\"v\": 0, \"a\": 1})\nenv2_vec = DummyVecEnv([lambda: env2])\n# set the model's environment to the new environment\nmodel.set_env(env2_vec)\nmodel.learn(total_timesteps=10000)\nenv2_vec.close()\n\n# plot example trials with trained agent\ndata = ngym.utils.plot_env(env2, num_trials=10, ob_traces=[\"fixation\", \"start\", \"v\", \"a\"], model=model)\n</pre> # switch to the second environment and continue training env2 = AnnubesEnv({\"v\": 0, \"a\": 1}) env2_vec = DummyVecEnv([lambda: env2]) # set the model's environment to the new environment model.set_env(env2_vec) model.learn(total_timesteps=10000) env2_vec.close()  # plot example trials with trained agent data = ngym.utils.plot_env(env2, num_trials=10, ob_traces=[\"fixation\", \"start\", \"v\", \"a\"], model=model) In\u00a0[\u00a0]: Copied! <pre># Switch to the third environment and finish training\nenv3 = AnnubesEnv({\"v\": 0.5, \"a\": 0.5})\nenv3_vec = DummyVecEnv([lambda: env3])\n# set the model's environment to the new environment\nmodel.set_env(env3_vec)\nmodel.learn(total_timesteps=20000)\nenv3_vec.close()\n\n# plot example trials with trained agent\ndata = ngym.utils.plot_env(env3, num_trials=10, ob_traces=[\"fixation\", \"start\", \"v\", \"a\"], model=model)\n</pre> # Switch to the third environment and finish training env3 = AnnubesEnv({\"v\": 0.5, \"a\": 0.5}) env3_vec = DummyVecEnv([lambda: env3]) # set the model's environment to the new environment model.set_env(env3_vec) model.learn(total_timesteps=20000) env3_vec.close()  # plot example trials with trained agent data = ngym.utils.plot_env(env3, num_trials=10, ob_traces=[\"fixation\", \"start\", \"v\", \"a\"], model=model) In\u00a0[\u00a0]: Copied! <pre># Save the final model after all training\nmodel.save(\"final_model\")\n</pre> # Save the final model after all training model.save(\"final_model\")"},{"location":"examples/example_annubes/#annubesenv-environment","title":"<code>AnnubesEnv</code> environment\u00b6","text":"<p>Let's create an environment, check it works and visualize it.</p>"},{"location":"examples/example_annubes/#training-annubesenv","title":"Training <code>AnnubesEnv</code>\u00b6","text":""},{"location":"examples/example_annubes/#1-regular-training","title":"1. Regular training\u00b6","text":"<p>We can train <code>AnnubesEnv</code> using one of the models defined in <code>stable_baselines3</code>, for example <code>A2C</code>.</p>"},{"location":"examples/example_annubes/#2-sequential-training","title":"2. Sequential training\u00b6","text":"<p>We can also train <code>AnnubesEnv</code> using a sequential training approach. This is useful when we want to train the agent in multiple stages, each with a different environment configuration. This can be useful for:</p> <ul> <li><p>Curriculum learning: Gradually increase the difficulty of the environments. Start with simpler tasks and progressively move to more complex ones, allowing the agent to build on its previous experiences.</p> </li> <li><p>Domain randomization: Vary the environment dynamics (e.g., physics, obstacles) during training to improve the agent's robustness to changes in the environment.</p> </li> <li><p>Transfer learning: If you have access to different agents or architectures, you can use transfer learning techniques to fine-tune the model on a new environment.</p> </li> </ul> <p>In this case it is important to include all the possible observations in each environment, even if not all of them are used. This is because the model is initialized with the first environment's observation space and it is not possible to change it later.</p>"},{"location":"examples/example_neurogym_keras/","title":"NeuroGym with Keras","text":"<p>NeuroGym is a comprehensive toolkit that allows training any network model on many established neuroscience tasks using Reinforcement Learning techniques. It includes working memory tasks, value-based decision tasks and context-dependent perceptual categorization tasks.</p> <p>In this notebook we first show how to install the relevant toolbox.</p> <p>We then show how to access the available tasks and their relevant information.</p> <p>Finally we train an LSTM network on the Random Dots Motion task using standard supervised learning techniques (with Keras), and plot the results.</p> In\u00a0[\u00a0]: Copied! <pre>%tensorflow_version 1.x\n# Install gymnasium\n! pip install gymnasium\n# Install neurogym\n! git clone https://github.com/gyyang/neurogym.git\n%cd neurogym/\n! pip install -e .\n</pre> %tensorflow_version 1.x # Install gymnasium ! pip install gymnasium # Install neurogym ! git clone https://github.com/gyyang/neurogym.git %cd neurogym/ ! pip install -e . In\u00a0[\u00a0]: Copied! <pre>import warnings\n\nimport numpy as np\nimport neurogym as ngym\n\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, LSTM, TimeDistributed, Input\n\nwarnings.filterwarnings('ignore')\nwarnings.filterwarnings('default')\n\n# Environment\ntask = 'PerceptualDecisionMaking-v0'\nkwargs = {'dt': 100}\nseq_len = 100\n\n# Make supervised dataset\ndataset = ngym.Dataset(task, env_kwargs=kwargs, batch_size=16,\n                       seq_len=seq_len)\nenv = dataset.env\nobs_size = env.observation_space.shape[0]\nact_size = env.action_space.n\n\n# Model\nnum_h = 64\n# from https://www.tensorflow.org/guide/keras/rnn\nxin = Input(batch_shape=(None, None, obs_size), dtype='float32')\nseq = LSTM(num_h, return_sequences=True, time_major=True)(xin)\nmlp = TimeDistributed(Dense(act_size, activation='softmax'))(seq)\nmodel = Model(inputs=xin, outputs=mlp)\nmodel.summary()\nmodel.compile(optimizer='Adam', loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\n# Train network\nsteps_per_epoch = 2000\ndata_generator = (dataset() for i in range(steps_per_epoch))\nhistory = model.fit(data_generator, steps_per_epoch=steps_per_epoch)\n</pre> import warnings  import numpy as np import neurogym as ngym  from tensorflow.keras.models import Model from tensorflow.keras.layers import Dense, LSTM, TimeDistributed, Input  warnings.filterwarnings('ignore') warnings.filterwarnings('default')  # Environment task = 'PerceptualDecisionMaking-v0' kwargs = {'dt': 100} seq_len = 100  # Make supervised dataset dataset = ngym.Dataset(task, env_kwargs=kwargs, batch_size=16,                        seq_len=seq_len) env = dataset.env obs_size = env.observation_space.shape[0] act_size = env.action_space.n  # Model num_h = 64 # from https://www.tensorflow.org/guide/keras/rnn xin = Input(batch_shape=(None, None, obs_size), dtype='float32') seq = LSTM(num_h, return_sequences=True, time_major=True)(xin) mlp = TimeDistributed(Dense(act_size, activation='softmax'))(seq) model = Model(inputs=xin, outputs=mlp) model.summary() model.compile(optimizer='Adam', loss='sparse_categorical_crossentropy',               metrics=['accuracy'])  # Train network steps_per_epoch = 2000 data_generator = (dataset() for i in range(steps_per_epoch)) history = model.fit(data_generator, steps_per_epoch=steps_per_epoch) <pre>/Users/gryang/tf1/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n/Users/gryang/tf1/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n/Users/gryang/tf1/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n/Users/gryang/tf1/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n/Users/gryang/tf1/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n/Users/gryang/tf1/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n</pre> <pre>WARNING:tensorflow:From /Users/gryang/tf1/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\nInstructions for updating:\nCall initializer instance with the dtype argument instead of passing it to the constructor\n</pre> <pre>/Users/gryang/tf1/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n/Users/gryang/tf1/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n/Users/gryang/tf1/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n/Users/gryang/tf1/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n/Users/gryang/tf1/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n/Users/gryang/tf1/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n/Users/gryang/Dropbox/Code/MyPython/neurogym/neurogym/core.py:253: UserWarning: Warning: Time for period fixation 100.000000  lasts only one timestep. Agents will not have time to respond (e.g. make a choice) on time.\n  ' time to respond (e.g. make a choice) on time.')\n/Users/gryang/Dropbox/Code/MyPython/neurogym/neurogym/core.py:253: UserWarning: Warning: Time for period decision 100.000000  lasts only one timestep. Agents will not have time to respond (e.g. make a choice) on time.\n  ' time to respond (e.g. make a choice) on time.')\n</pre> <pre>Model: \"model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(None, None, 3)]         0         \n_________________________________________________________________\nlstm (LSTM)                  (None, None, 64)          17408     \n_________________________________________________________________\ntime_distributed (TimeDistri (None, None, 3)           195       \n=================================================================\nTotal params: 17,603\nTrainable params: 17,603\nNon-trainable params: 0\n_________________________________________________________________\nWARNING:tensorflow:From /Users/gryang/tf1/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.&lt;locals&gt;.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.where in 2.0, which has the same broadcast rule as np.where\n   7/2000 [..............................] - ETA: 5:35 - loss: 1.0581 - acc: 0.4196</pre> <pre>/Users/gryang/Dropbox/Code/MyPython/neurogym/neurogym/core.py:253: UserWarning: Warning: Time for period fixation 100.000000  lasts only one timestep. Agents will not have time to respond (e.g. make a choice) on time.\n  ' time to respond (e.g. make a choice) on time.')\n/Users/gryang/Dropbox/Code/MyPython/neurogym/neurogym/core.py:253: UserWarning: Warning: Time for period decision 100.000000  lasts only one timestep. Agents will not have time to respond (e.g. make a choice) on time.\n  ' time to respond (e.g. make a choice) on time.')\n</pre> <pre>2000/2000 [==============================] - 106s 53ms/step - loss: 0.0569 - acc: 0.9835\n</pre> In\u00a0[2]: Copied! <pre>perf = 0\nnum_trial = 200\nfor i in range(num_trial):\n    env.new_trial()\n    obs, gt = env.obs, env.gt\n    obs = obs[:, np.newaxis, :]\n\n    action_pred = model.predict(obs)\n    action_pred = np.argmax(action_pred, axis=-1)\n    perf += gt[-1] == action_pred[-1, 0]\n\nperf /= num_trial\nprint(perf)\n</pre> perf = 0 num_trial = 200 for i in range(num_trial):     env.new_trial()     obs, gt = env.obs, env.gt     obs = obs[:, np.newaxis, :]      action_pred = model.predict(obs)     action_pred = np.argmax(action_pred, axis=-1)     perf += gt[-1] == action_pred[-1, 0]  perf /= num_trial print(perf) <pre>0.895\n</pre> In\u00a0[3]: Copied! <pre>_ = ngym.utils.plotting.fig_(obs[0], action_pred[0], gt)\n</pre> _ = ngym.utils.plotting.fig_(obs[0], action_pred[0], gt) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/example_neurogym_keras/#keras-example-of-supervised-learning-a-neurogym-task","title":"Keras example of supervised learning a NeuroGym task\u00b6","text":""},{"location":"examples/example_neurogym_keras/#installation-when-used-on-google-colab","title":"Installation when used on Google Colab\u00b6","text":""},{"location":"examples/example_neurogym_keras/#task-network-and-training","title":"Task, network, and training\u00b6","text":""},{"location":"examples/example_neurogym_keras/#analysis","title":"Analysis\u00b6","text":""},{"location":"examples/example_neurogym_pytorch/","title":"NeuroGym with PyTorch","text":"In\u00a0[\u00a0]: Copied! <pre># Install gymnasium\n! pip install gymnasium\n# Install neurogym\n! git clone https://github.com/gyyang/neurogym.git\n%cd neurogym/\n! pip install -e .\n</pre> # Install gymnasium ! pip install gymnasium # Install neurogym ! git clone https://github.com/gyyang/neurogym.git %cd neurogym/ ! pip install -e . In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport torch\nimport torch.nn as nn\n\nimport neurogym as ngym\n\n# Environment\ntask = 'PerceptualDecisionMaking-v0'\nkwargs = {'dt': 100}\nseq_len = 100\n\n# Make supervised dataset\ndataset = ngym.Dataset(task, env_kwargs=kwargs, batch_size=16,\n                       seq_len=seq_len)\nenv = dataset.env\nob_size = env.observation_space.shape[0]\nact_size = env.action_space.n\n</pre> import numpy as np import torch import torch.nn as nn  import neurogym as ngym  # Environment task = 'PerceptualDecisionMaking-v0' kwargs = {'dt': 100} seq_len = 100  # Make supervised dataset dataset = ngym.Dataset(task, env_kwargs=kwargs, batch_size=16,                        seq_len=seq_len) env = dataset.env ob_size = env.observation_space.shape[0] act_size = env.action_space.n In\u00a0[2]: Copied! <pre>class Net(nn.Module):\n    def __init__(self, num_h):\n        super(Net, self).__init__()\n        self.lstm = nn.LSTM(ob_size, num_h)\n        self.linear = nn.Linear(num_h, act_size)\n\n    def forward(self, x):\n        out, hidden = self.lstm(x)\n        x = self.linear(out)\n        return x\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nnet = Net(num_h=64).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(net.parameters(), lr=1e-2)\n\nrunning_loss = 0.0\nfor i in range(2000):\n    inputs, labels = dataset()\n    inputs = torch.from_numpy(inputs).type(torch.float).to(device)\n    labels = torch.from_numpy(labels.flatten()).type(torch.long).to(device)\n\n    # zero the parameter gradients\n    optimizer.zero_grad()\n\n    # forward + backward + optimize\n    outputs = net(inputs)\n\n    loss = criterion(outputs.view(-1, act_size), labels)\n    loss.backward()\n    optimizer.step()\n\n    # print statistics\n    running_loss += loss.item()\n    if i % 200 == 199:\n        print('{:d} loss: {:0.5f}'.format(i + 1, running_loss / 200))\n        running_loss = 0.0\n\nprint('Finished Training')\n</pre> class Net(nn.Module):     def __init__(self, num_h):         super(Net, self).__init__()         self.lstm = nn.LSTM(ob_size, num_h)         self.linear = nn.Linear(num_h, act_size)      def forward(self, x):         out, hidden = self.lstm(x)         x = self.linear(out)         return x  device = 'cuda' if torch.cuda.is_available() else 'cpu' net = Net(num_h=64).to(device) criterion = nn.CrossEntropyLoss() optimizer = torch.optim.Adam(net.parameters(), lr=1e-2)  running_loss = 0.0 for i in range(2000):     inputs, labels = dataset()     inputs = torch.from_numpy(inputs).type(torch.float).to(device)     labels = torch.from_numpy(labels.flatten()).type(torch.long).to(device)      # zero the parameter gradients     optimizer.zero_grad()      # forward + backward + optimize     outputs = net(inputs)      loss = criterion(outputs.view(-1, act_size), labels)     loss.backward()     optimizer.step()      # print statistics     running_loss += loss.item()     if i % 200 == 199:         print('{:d} loss: {:0.5f}'.format(i + 1, running_loss / 200))         running_loss = 0.0  print('Finished Training') <pre>200 loss: 0.11658\n400 loss: 0.03191\n600 loss: 0.01894\n800 loss: 0.01393\n1000 loss: 0.01351\n1200 loss: 0.01275\n1400 loss: 0.01280\n1600 loss: 0.01279\n1800 loss: 0.01223\n2000 loss: 0.01239\nFinished Training\n</pre> In\u00a0[4]: Copied! <pre>perf = 0\nnum_trial = 200\nfor i in range(num_trial):\n    env.new_trial()\n    ob, gt = env.ob, env.gt\n    ob = ob[:, np.newaxis, :]  # Add batch axis\n    inputs = torch.from_numpy(ob).type(torch.float).to(device)\n\n    action_pred = net(inputs)\n    action_pred = action_pred.detach().numpy()\n    action_pred = np.argmax(action_pred, axis=-1)\n    perf += gt[-1] == action_pred[-1, 0]\n\nperf /= num_trial\nprint('Average performance in {:d} trials'.format(num_trial))\nprint(perf)\n</pre> perf = 0 num_trial = 200 for i in range(num_trial):     env.new_trial()     ob, gt = env.ob, env.gt     ob = ob[:, np.newaxis, :]  # Add batch axis     inputs = torch.from_numpy(ob).type(torch.float).to(device)      action_pred = net(inputs)     action_pred = action_pred.detach().numpy()     action_pred = np.argmax(action_pred, axis=-1)     perf += gt[-1] == action_pred[-1, 0]  perf /= num_trial print('Average performance in {:d} trials'.format(num_trial)) print(perf) <pre>Average performance in 200 trials\n0.83\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/example_neurogym_pytorch/#pytorch-supervised-learning-of-perceptual-decision-making-task","title":"Pytorch supervised learning of perceptual decision making task\u00b6","text":"<p>Pytorch-based example code for training a RNN on a perceptual decision-making task.</p>"},{"location":"examples/example_neurogym_pytorch/#installation-when-used-on-google-colab","title":"Installation when used on Google Colab\u00b6","text":""},{"location":"examples/example_neurogym_pytorch/#dataset","title":"Dataset\u00b6","text":""},{"location":"examples/example_neurogym_pytorch/#network-and-training","title":"Network and Training\u00b6","text":""},{"location":"examples/example_neurogym_pytorch/#analysis","title":"Analysis\u00b6","text":""},{"location":"examples/example_neurogym_rl/","title":"NeuroGym with RL","text":"<p>NeuroGym is a toolkit that allows training any network model on many established neuroscience tasks techniques such as standard Supervised  Learning or Reinforcement Learning (RL). In this notebook we will use RL to train an LSTM network on the classical Random Dots Motion (RDM) task (Britten et al. 1992).</p> <p>We first show how to install the relevant toolboxes. We then show how build the task of interest (in the example the RDM task), wrapp it with the pass-reward wrapper in one line and visualize the structure of the final task. Finally we train an LSTM network on the task using the A2C algorithm Mnih et al. 2016 implemented in the stable-baselines3 toolbox, and plot the results.</p> <p>It is straightforward to change the code to train a network on any other available task or using a different RL algorithm (e.g. ACER, PPO2).</p> In\u00a0[1]: Copied! <pre>%tensorflow_version 1.x\n# Install gymnasium\n! pip install gymnasium\n# Install neurogym\n! git clone https://github.com/gyyang/neurogym.git\n%cd neurogym/\n! pip install -e .\n# Install stable-baselines3\n! pip install stable-baselines3\n</pre> %tensorflow_version 1.x # Install gymnasium ! pip install gymnasium # Install neurogym ! git clone https://github.com/gyyang/neurogym.git %cd neurogym/ ! pip install -e . # Install stable-baselines3 ! pip install stable-baselines3 <pre>TensorFlow 1.x selected.\nRequirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.17.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1)\nRequirement already satisfied: cloudpickle&lt;1.4.0,&gt;=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.0)\nRequirement already satisfied: numpy&gt;=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.18.5)\nRequirement already satisfied: pyglet&lt;=1.5.0,&gt;=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.5.0)\nRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet&lt;=1.5.0,&gt;=1.4.0-&gt;gym) (0.16.0)\nfatal: destination path 'neurogym' already exists and is not an empty directory.\n/content/neurogym\nObtaining file:///content/neurogym\nRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from neurogym==0.0.1) (1.18.5)\nRequirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from neurogym==0.0.1) (0.17.2)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from neurogym==0.0.1) (3.2.2)\nRequirement already satisfied: pyglet&lt;=1.5.0,&gt;=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym-&gt;neurogym==0.0.1) (1.5.0)\nRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym-&gt;neurogym==0.0.1) (1.4.1)\nRequirement already satisfied: cloudpickle&lt;1.4.0,&gt;=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym-&gt;neurogym==0.0.1) (1.3.0)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib-&gt;neurogym==0.0.1) (1.2.0)\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib-&gt;neurogym==0.0.1) (2.4.7)\nRequirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib-&gt;neurogym==0.0.1) (0.10.0)\nRequirement already satisfied: python-dateutil&gt;=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib-&gt;neurogym==0.0.1) (2.8.1)\nRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet&lt;=1.5.0,&gt;=1.4.0-&gt;gym-&gt;neurogym==0.0.1) (0.16.0)\nRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler&gt;=0.10-&gt;matplotlib-&gt;neurogym==0.0.1) (1.12.0)\nInstalling collected packages: neurogym\n  Found existing installation: neurogym 0.0.1\n    Can't uninstall 'neurogym'. No files were found to uninstall.\n  Running setup.py develop for neurogym\nSuccessfully installed neurogym\nRequirement already up-to-date: stable-baselines in /usr/local/lib/python3.6/dist-packages (2.10.0)\nRequirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from stable-baselines) (1.4.1)\nRequirement already satisfied, skipping upgrade: opencv-python in /usr/local/lib/python3.6/dist-packages (from stable-baselines) (4.1.2.30)\nRequirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from stable-baselines) (1.18.5)\nRequirement already satisfied, skipping upgrade: matplotlib in /usr/local/lib/python3.6/dist-packages (from stable-baselines) (3.2.2)\nRequirement already satisfied, skipping upgrade: gym[atari,classic_control]&gt;=0.11 in /usr/local/lib/python3.6/dist-packages (from stable-baselines) (0.17.2)\nRequirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.6/dist-packages (from stable-baselines) (0.15.1)\nRequirement already satisfied, skipping upgrade: cloudpickle&gt;=0.5.5 in /usr/local/lib/python3.6/dist-packages (from stable-baselines) (1.3.0)\nRequirement already satisfied, skipping upgrade: pandas in /usr/local/lib/python3.6/dist-packages (from stable-baselines) (1.0.5)\nRequirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib-&gt;stable-baselines) (2.4.7)\nRequirement already satisfied, skipping upgrade: python-dateutil&gt;=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib-&gt;stable-baselines) (2.8.1)\nRequirement already satisfied, skipping upgrade: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib-&gt;stable-baselines) (1.2.0)\nRequirement already satisfied, skipping upgrade: cycler&gt;=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib-&gt;stable-baselines) (0.10.0)\nRequirement already satisfied, skipping upgrade: pyglet&lt;=1.5.0,&gt;=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]&gt;=0.11-&gt;stable-baselines) (1.5.0)\nRequirement already satisfied, skipping upgrade: Pillow; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]&gt;=0.11-&gt;stable-baselines) (7.0.0)\nRequirement already satisfied, skipping upgrade: atari-py~=0.2.0; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]&gt;=0.11-&gt;stable-baselines) (0.2.6)\nRequirement already satisfied, skipping upgrade: pytz&gt;=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas-&gt;stable-baselines) (2018.9)\nRequirement already satisfied, skipping upgrade: six&gt;=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil&gt;=2.1-&gt;matplotlib-&gt;stable-baselines) (1.12.0)\nRequirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from pyglet&lt;=1.5.0,&gt;=1.4.0-&gt;gym[atari,classic_control]&gt;=0.11-&gt;stable-baselines) (0.16.0)\n</pre> <p>here we build the Random Dots Motion task, specifying the duration of each trial period (fixation, stimulus, decision) and wrapp it with the pass-reward wrapper which appends the previous reward to the observation. We then plot the structure of the task in a figure that shows:</p> <ol> <li>The observations received by the agent (top panel).</li> <li>The actions taken by a random agent and the correct action at each timestep (second panel).</li> <li>The rewards provided by the environment at each timestep (third panel).</li> <li>The performance of the agent at each trial (bottom panel).</li> </ol> In\u00a0[2]: Copied! <pre>import gymnasium as gym\nimport neurogym as ngym\nfrom neurogym.wrappers import pass_reward\nimport warnings\nwarnings.filterwarnings('ignore')\n# Task name\nname = 'PerceptualDecisionMaking-v0'\n# task specification (here we only specify the duration of the different trial periods)\ntiming = {'fixation': ('constant', 300),\n          'stimulus': ('constant', 500),\n          'decision': ('constant', 300)}\nkwargs = {'dt': 100, 'timing': timing}\n# build task\nenv = gym.make(name, **kwargs)\n# print task properties\nprint(env)\n# wrapp task with pass-reward wrapper\nenv = pass_reward.PassReward(env)\n# plot example trials with random agent\ndata = ngym.utils.plot_env(env, fig_kwargs={'figsize': (12, 12)}, num_steps=100, ob_traces=['Fixation cue', 'Stim 1', 'Stim 2', 'Previous reward'])\n</pre> import gymnasium as gym import neurogym as ngym from neurogym.wrappers import pass_reward import warnings warnings.filterwarnings('ignore') # Task name name = 'PerceptualDecisionMaking-v0' # task specification (here we only specify the duration of the different trial periods) timing = {'fixation': ('constant', 300),           'stimulus': ('constant', 500),           'decision': ('constant', 300)} kwargs = {'dt': 100, 'timing': timing} # build task env = gym.make(name, **kwargs) # print task properties print(env) # wrapp task with pass-reward wrapper env = pass_reward.PassReward(env) # plot example trials with random agent data = ngym.utils.plot_env(env, fig_kwargs={'figsize': (12, 12)}, num_steps=100, ob_traces=['Fixation cue', 'Stim 1', 'Stim 2', 'Previous reward']) <pre>findfont: Font family ['arial'] not found. Falling back to DejaVu Sans.\nfindfont: Font family ['arial'] not found. Falling back to DejaVu Sans.\n</pre> <pre>### PerceptualDecisionMaking\nDoc: Two-alternative forced choice task in which the subject has to\n    integrate two stimuli to decide which one is higher on average.\n\n    Args:\n        stim_scale: Controls the difficulty of the experiment. (def: 1., float)\n        sigma: float, input noise level\n        dim_ring: int, dimension of ring input and output\n    \nReference paper \n[The analysis of visual motion: a comparison of neuronal and psychophysical performance](https://www.jneurosci.org/content/12/12/4745)\n\nPeriod timing (ms) \nfixation : constant 300\nstimulus : constant 500\ndelay : constant 0\ndecision : constant 300\n\nReward structure \nabort : -0.1\ncorrect : 1.0\nfail : 0.0\n\nTags: perceptual, two-alternative, supervised.\n\n</pre> In\u00a0[3]: Copied! <pre>import warnings\nfrom stable_baselines3.common.policies import LstmPolicy\nfrom stable_baselines3.common.vec_env import DummyVecEnv\nfrom stable_baselines3 import A2C  # ACER, PPO2\nwarnings.filterwarnings('default')\n\n# Optional: PPO2 requires a vectorized environment to run\n# the env is now wrapped automatically when passing it to the constructor\nenv = DummyVecEnv([lambda: env])\n\nmodel = A2C(LstmPolicy, env, verbose=1, policy_kwargs={'feature_extraction':\"mlp\"})\nmodel.learn(total_timesteps=100000, log_interval=1000)\nenv.close()\n</pre> import warnings from stable_baselines3.common.policies import LstmPolicy from stable_baselines3.common.vec_env import DummyVecEnv from stable_baselines3 import A2C  # ACER, PPO2 warnings.filterwarnings('default')  # Optional: PPO2 requires a vectorized environment to run # the env is now wrapped automatically when passing it to the constructor env = DummyVecEnv([lambda: env])  model = A2C(LstmPolicy, env, verbose=1, policy_kwargs={'feature_extraction':\"mlp\"}) model.learn(total_timesteps=100000, log_interval=1000) env.close() <pre>WARNING:tensorflow:\nThe TensorFlow contrib module will not be included in TensorFlow 2.0.\nFor more information, please see:\n  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n  * https://github.com/tensorflow/addons\n  * https://github.com/tensorflow/io (for I/O related ops)\nIf you depend on functionality not listed there, please file an issue.\n\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/policies.py:116: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/policies.py:420: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse keras.layers.flatten instead.\nWARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `layer.__call__` method instead.\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/tf_layers.py:123: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/distributions.py:326: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/distributions.py:327: The name tf.log is deprecated. Please use tf.math.log instead.\n\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/a2c/a2c.py:158: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/tf_util.py:449: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/tf_util.py:449: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n\nWARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/clip_ops.py:301: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.where in 2.0, which has the same broadcast rule as np.where\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/a2c/a2c.py:182: The name tf.train.RMSPropOptimizer is deprecated. Please use tf.compat.v1.train.RMSPropOptimizer instead.\n\nWARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\nInstructions for updating:\nCall initializer instance with the dtype argument instead of passing it to the constructor\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/a2c/a2c.py:192: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n\nWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/a2c/a2c.py:194: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n\n---------------------------------\n| explained_variance | -0.477   |\n| fps                | 10       |\n| nupdates           | 1        |\n| policy_entropy     | 1.1      |\n| total_timesteps    | 5        |\n| value_loss         | 0.00289  |\n---------------------------------\n---------------------------------\n| explained_variance | -0.907   |\n| fps                | 351      |\n| nupdates           | 1000     |\n| policy_entropy     | 1.1      |\n| total_timesteps    | 5000     |\n| value_loss         | 0.0147   |\n---------------------------------\n---------------------------------\n| explained_variance | 0.517    |\n| fps                | 356      |\n| nupdates           | 2000     |\n| policy_entropy     | 1.07     |\n| total_timesteps    | 10000    |\n| value_loss         | 0.554    |\n---------------------------------\n---------------------------------\n| explained_variance | 0.182    |\n| fps                | 359      |\n| nupdates           | 3000     |\n| policy_entropy     | 1.05     |\n| total_timesteps    | 15000    |\n| value_loss         | 0.0563   |\n---------------------------------\n---------------------------------\n| explained_variance | 0.558    |\n| fps                | 358      |\n| nupdates           | 4000     |\n| policy_entropy     | 0.808    |\n| total_timesteps    | 20000    |\n| value_loss         | 0.166    |\n---------------------------------\n---------------------------------\n| explained_variance | 0.99     |\n| fps                | 358      |\n| nupdates           | 5000     |\n| policy_entropy     | 0.189    |\n| total_timesteps    | 25000    |\n| value_loss         | 0.00343  |\n---------------------------------\n---------------------------------\n| explained_variance | 0.991    |\n| fps                | 360      |\n| nupdates           | 6000     |\n| policy_entropy     | 0.117    |\n| total_timesteps    | 30000    |\n| value_loss         | 0.00305  |\n---------------------------------\n---------------------------------\n| explained_variance | 0.914    |\n| fps                | 362      |\n| nupdates           | 7000     |\n| policy_entropy     | 0.212    |\n| total_timesteps    | 35000    |\n| value_loss         | 0.013    |\n---------------------------------\n---------------------------------\n| explained_variance | 0.957    |\n| fps                | 362      |\n| nupdates           | 8000     |\n| policy_entropy     | 0.0404   |\n| total_timesteps    | 40000    |\n| value_loss         | 0.026    |\n---------------------------------\n---------------------------------\n| explained_variance | 0.934    |\n| fps                | 360      |\n| nupdates           | 9000     |\n| policy_entropy     | 0.27     |\n| total_timesteps    | 45000    |\n| value_loss         | 0.011    |\n---------------------------------\n---------------------------------\n| explained_variance | 0.976    |\n| fps                | 360      |\n| nupdates           | 10000    |\n| policy_entropy     | 0.509    |\n| total_timesteps    | 50000    |\n| value_loss         | 0.00139  |\n---------------------------------\n---------------------------------\n| explained_variance | 0.991    |\n| fps                | 360      |\n| nupdates           | 11000    |\n| policy_entropy     | 0.0325   |\n| total_timesteps    | 55000    |\n| value_loss         | 0.00196  |\n---------------------------------\n---------------------------------\n| explained_variance | 0.996    |\n| fps                | 361      |\n| nupdates           | 12000    |\n| policy_entropy     | 0.211    |\n| total_timesteps    | 60000    |\n| value_loss         | 0.000678 |\n---------------------------------\n---------------------------------\n| explained_variance | 0.684    |\n| fps                | 361      |\n| nupdates           | 13000    |\n| policy_entropy     | 0.2      |\n| total_timesteps    | 65000    |\n| value_loss         | 0.00527  |\n---------------------------------\n---------------------------------\n| explained_variance | 0.968    |\n| fps                | 361      |\n| nupdates           | 14000    |\n| policy_entropy     | 0.424    |\n| total_timesteps    | 70000    |\n| value_loss         | 0.00391  |\n---------------------------------\n---------------------------------\n| explained_variance | 0.851    |\n| fps                | 362      |\n| nupdates           | 15000    |\n| policy_entropy     | 0.384    |\n| total_timesteps    | 75000    |\n| value_loss         | 0.0313   |\n---------------------------------\n---------------------------------\n| explained_variance | 0.977    |\n| fps                | 361      |\n| nupdates           | 16000    |\n| policy_entropy     | 0.0187   |\n| total_timesteps    | 80000    |\n| value_loss         | 0.00316  |\n---------------------------------\n---------------------------------\n| explained_variance | 0.994    |\n| fps                | 362      |\n| nupdates           | 17000    |\n| policy_entropy     | 0.0252   |\n| total_timesteps    | 85000    |\n| value_loss         | 0.000824 |\n---------------------------------\n---------------------------------\n| explained_variance | 0.957    |\n| fps                | 362      |\n| nupdates           | 18000    |\n| policy_entropy     | 0.206    |\n| total_timesteps    | 90000    |\n| value_loss         | 0.00319  |\n---------------------------------\n---------------------------------\n| explained_variance | 0.992    |\n| fps                | 363      |\n| nupdates           | 19000    |\n| policy_entropy     | 0.0935   |\n| total_timesteps    | 95000    |\n| value_loss         | 0.00294  |\n---------------------------------\n---------------------------------\n| explained_variance | 0.978    |\n| fps                | 363      |\n| nupdates           | 20000    |\n| policy_entropy     | 0.0645   |\n| total_timesteps    | 100000   |\n| value_loss         | 0.000502 |\n---------------------------------\n</pre> In\u00a0[6]: Copied! <pre>env = gym.make(name, **kwargs)\n# print task properties\nprint(env)\n# wrapp task with pass-reward wrapper\nenv = pass_reward.PassReward(env)\nenv = DummyVecEnv([lambda: env])\n# plot example trials with random agent\ndata = ngym.utils.plot_env(env, fig_kwargs={'figsize': (12, 12)}, num_steps=100, ob_traces=['Fixation cue', 'Stim 1', 'Stim 2', 'Previous reward'], model=model)\n</pre> env = gym.make(name, **kwargs) # print task properties print(env) # wrapp task with pass-reward wrapper env = pass_reward.PassReward(env) env = DummyVecEnv([lambda: env]) # plot example trials with random agent data = ngym.utils.plot_env(env, fig_kwargs={'figsize': (12, 12)}, num_steps=100, ob_traces=['Fixation cue', 'Stim 1', 'Stim 2', 'Previous reward'], model=model)  <pre>### PerceptualDecisionMaking\nDoc: Two-alternative forced choice task in which the subject has to\n    integrate two stimuli to decide which one is higher on average.\n\n    Args:\n        stim_scale: Controls the difficulty of the experiment. (def: 1., float)\n        sigma: float, input noise level\n        dim_ring: int, dimension of ring input and output\n    \nReference paper \n[The analysis of visual motion: a comparison of neuronal and psychophysical performance](https://www.jneurosci.org/content/12/12/4745)\n\nPeriod timing (ms) \nfixation : constant 300\nstimulus : constant 500\ndelay : constant 0\ndecision : constant 300\n\nReward structure \nabort : -0.1\ncorrect : 1.0\nfail : 0.0\n\nTags: perceptual, two-alternative, supervised.\n\n</pre>"},{"location":"examples/example_neurogym_rl/#reinforcement-learning-example-with-stable-baselines3","title":"Reinforcement learning example with stable-baselines3\u00b6","text":""},{"location":"examples/example_neurogym_rl/#installation","title":"Installation\u00b6","text":"<p>(only for running in Google Colab)</p>"},{"location":"examples/example_neurogym_rl/#task","title":"Task\u00b6","text":""},{"location":"examples/example_neurogym_rl/#train-a-network","title":"Train a network\u00b6","text":""},{"location":"examples/example_neurogym_rl/#visualize-results","title":"Visualize results\u00b6","text":""},{"location":"examples/template/","title":"Template","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"Example template for contributing new tasks.\"\"\"  # noqa: INP001\n</pre> \"\"\"Example template for contributing new tasks.\"\"\"  # noqa: INP001 In\u00a0[\u00a0]: Copied! <pre>import numpy as np\n</pre> import numpy as np In\u00a0[\u00a0]: Copied! <pre>import neurogym as ngym\nfrom neurogym import spaces\n</pre> import neurogym as ngym from neurogym import spaces In\u00a0[\u00a0]: Copied! <pre>class YourTask(ngym.TrialEnv):\n    def __init__(self, dt=100, rewards=None, timing=None, sigma=1) -&gt; None:\n        super().__init__(dt=dt)\n        # Possible decisions at the end of the trial\n        self.choices = [1, 2]  # e.g. [left, right]\n        self.sigma = sigma / np.sqrt(self.dt)  # Input noise\n\n        # Optional rewards dictionary\n        self.rewards = {\"abort\": -0.1, \"correct\": +1.0, \"fail\": 0.0}\n        if rewards:\n            self.rewards.update(rewards)\n\n        # Optional timing dictionary\n        # if provided, self.add_period can infer timing directly\n        self.timing = {\"fixation\": 100, \"stimulus\": 2000, \"delay\": 0, \"decision\": 100}\n        if timing:\n            self.timing.update(timing)\n\n        # Similar to gymnasium envs, define observations_space and action_space\n        # Optional annotation of the observation space\n        name = {\"fixation\": 0, \"stimulus\": [1, 2]}\n        self.observation_space = spaces.Box(\n            -np.inf,\n            np.inf,\n            shape=(3,),\n            dtype=np.float32,\n            name=name,\n        )\n        # Optional annotation of the action space\n        name = {\"fixation\": 0, \"choice\": [1, 2]}\n        self.action_space = spaces.Discrete(3, name=name)\n\n    def _new_trial(self, **kwargs):\n        \"\"\"Called internally to generate a next trial.\n\n        Typically, you need to\n            set trial: a dictionary of trial information\n            run self.add_period():\n                will add time periods to the trial\n                accesible through dict self.start_t and self.end_t\n            run self.add_ob():\n                will add observation to np array self.ob\n            run self.set_groundtruth():\n                will set groundtruth to np array self.gt\n\n        Returns:\n            trial: dictionary of trial information\n        \"\"\"\n        # Setting trial information\n        trial = {\"ground_truth\": self.rng.choice(self.choices)}\n        trial.update(kwargs)  # allows wrappers to modify the trial\n        ground_truth = trial[\"ground_truth\"]\n\n        # Adding periods sequentially\n        self.add_period([\"fixation\", \"stimulus\", \"delay\", \"decision\"])\n\n        # Setting observations, default all 0\n        # Setting fixation cue to 1 before decision period\n        self.add_ob(1, where=\"fixation\")\n        self.set_ob(0, \"decision\", where=\"fixation\")\n        # Set the stimulus\n        stim = [0, 0, 0]\n        stim[ground_truth] = 1\n        self.add_ob(stim, \"stimulus\")\n        # adding gaussian noise to stimulus with std = self.sigma\n        self.add_randn(0, self.sigma, \"stimulus\", where=\"stimulus\")\n\n        # Setting ground-truth value for supervised learning\n        self.set_groundtruth(ground_truth, \"decision\")\n\n        return trial\n\n    def _step(self, action):\n        \"\"\"Called internally to process one step.\n\n        Receives an action and returns:\n        a new observation, obs\n        reward associated with the action, reward\n        a boolean variable indicating whether the experiment has terminated, terminated\n            See more at https://gymnasium.farama.org/tutorials/gymnasium_basics/handling_time_limits/#termination\n        a boolean variable indicating whether the experiment has been truncated, truncated\n            See more at https://gymnasium.farama.org/tutorials/gymnasium_basics/handling_time_limits/#truncation\n        a dictionary with extra information:\n            ground truth correct response, info['gt']\n            boolean indicating the end of the trial, info['new_trial'].\n        \"\"\"\n        terminated = False\n        truncated = False\n        # rewards\n        reward = 0\n        gt = self.gt_now\n        # Example structure\n        if not self.in_period(\"decision\"):\n            if action != 0:  # if fixation break\n                reward = self.rewards[\"abort\"]\n        elif action != 0:\n            terminated = True\n            reward = self.rewards[\"correct\"] if action == gt else self.rewards[\"fail\"]\n\n        return (\n            self.ob_now,\n            reward,\n            terminated,\n            truncated,\n            {\"new_trial\": terminated, \"gt\": gt},\n        )\n</pre> class YourTask(ngym.TrialEnv):     def __init__(self, dt=100, rewards=None, timing=None, sigma=1) -&gt; None:         super().__init__(dt=dt)         # Possible decisions at the end of the trial         self.choices = [1, 2]  # e.g. [left, right]         self.sigma = sigma / np.sqrt(self.dt)  # Input noise          # Optional rewards dictionary         self.rewards = {\"abort\": -0.1, \"correct\": +1.0, \"fail\": 0.0}         if rewards:             self.rewards.update(rewards)          # Optional timing dictionary         # if provided, self.add_period can infer timing directly         self.timing = {\"fixation\": 100, \"stimulus\": 2000, \"delay\": 0, \"decision\": 100}         if timing:             self.timing.update(timing)          # Similar to gymnasium envs, define observations_space and action_space         # Optional annotation of the observation space         name = {\"fixation\": 0, \"stimulus\": [1, 2]}         self.observation_space = spaces.Box(             -np.inf,             np.inf,             shape=(3,),             dtype=np.float32,             name=name,         )         # Optional annotation of the action space         name = {\"fixation\": 0, \"choice\": [1, 2]}         self.action_space = spaces.Discrete(3, name=name)      def _new_trial(self, **kwargs):         \"\"\"Called internally to generate a next trial.          Typically, you need to             set trial: a dictionary of trial information             run self.add_period():                 will add time periods to the trial                 accesible through dict self.start_t and self.end_t             run self.add_ob():                 will add observation to np array self.ob             run self.set_groundtruth():                 will set groundtruth to np array self.gt          Returns:             trial: dictionary of trial information         \"\"\"         # Setting trial information         trial = {\"ground_truth\": self.rng.choice(self.choices)}         trial.update(kwargs)  # allows wrappers to modify the trial         ground_truth = trial[\"ground_truth\"]          # Adding periods sequentially         self.add_period([\"fixation\", \"stimulus\", \"delay\", \"decision\"])          # Setting observations, default all 0         # Setting fixation cue to 1 before decision period         self.add_ob(1, where=\"fixation\")         self.set_ob(0, \"decision\", where=\"fixation\")         # Set the stimulus         stim = [0, 0, 0]         stim[ground_truth] = 1         self.add_ob(stim, \"stimulus\")         # adding gaussian noise to stimulus with std = self.sigma         self.add_randn(0, self.sigma, \"stimulus\", where=\"stimulus\")          # Setting ground-truth value for supervised learning         self.set_groundtruth(ground_truth, \"decision\")          return trial      def _step(self, action):         \"\"\"Called internally to process one step.          Receives an action and returns:         a new observation, obs         reward associated with the action, reward         a boolean variable indicating whether the experiment has terminated, terminated             See more at https://gymnasium.farama.org/tutorials/gymnasium_basics/handling_time_limits/#termination         a boolean variable indicating whether the experiment has been truncated, truncated             See more at https://gymnasium.farama.org/tutorials/gymnasium_basics/handling_time_limits/#truncation         a dictionary with extra information:             ground truth correct response, info['gt']             boolean indicating the end of the trial, info['new_trial'].         \"\"\"         terminated = False         truncated = False         # rewards         reward = 0         gt = self.gt_now         # Example structure         if not self.in_period(\"decision\"):             if action != 0:  # if fixation break                 reward = self.rewards[\"abort\"]         elif action != 0:             terminated = True             reward = self.rewards[\"correct\"] if action == gt else self.rewards[\"fail\"]          return (             self.ob_now,             reward,             terminated,             truncated,             {\"new_trial\": terminated, \"gt\": gt},         ) In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n    # Instantiate the task\n    env = YourTask()\n    trial = env.new_trial()\n    print(\"Trial info\", trial)\n    print(\"Trial observation shape\", env.ob.shape)\n    print(\"Trial action shape\", env.gt.shape)\n    env.reset()\n    ob, reward, terminated, truncated, info = env.step(env.action_space.sample())\n    print(\"Single time step observation shape\", ob.shape)\n</pre> if __name__ == \"__main__\":     # Instantiate the task     env = YourTask()     trial = env.new_trial()     print(\"Trial info\", trial)     print(\"Trial observation shape\", env.ob.shape)     print(\"Trial action shape\", env.gt.shape)     env.reset()     ob, reward, terminated, truncated, info = env.step(env.action_space.sample())     print(\"Single time step observation shape\", ob.shape)"},{"location":"examples/understanding_neurogym_task/","title":"Custom tasks","text":"In\u00a0[\u00a0]: Copied! <pre># # Install gymnasium\n# ! pip install gymnasium\n\n# # Install neurogym\n# ! git clone https://github.com/gyyang/neurogym.git\n# %cd neurogym/\n# ! pip install -e .\n</pre> # # Install gymnasium # ! pip install gymnasium  # # Install neurogym # ! git clone https://github.com/gyyang/neurogym.git # %cd neurogym/ # ! pip install -e . <p>Neurogym tasks follow basic Gymnasium tasks format. Gymnasium is a maintained fork of OpenAI\u2019s Gym library. Each task is defined as a Python class, inheriting from the <code>gymnasium.Env</code> class.</p> <p>In this section we describe basic structure for an gymnasium task.</p> <p>In the <code>__init__</code> method, it is necessary to define two attributes, <code>self.observation_space</code> and <code>self.action_space</code> which describe the kind of spaces used by observations (network inputs) and actions (network outputs).</p> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport gymnasium as gym\n\nclass MyEnv(gym.Env):\n    def __init__(self):\n        super().__init__()  # Python boilerplate to initialize base class\n\n        # A two-dimensional box with minimum and maximum value set by low and high\n        self.observation_space = gym.spaces.Box(low=0., high=1., shape=(2,))\n\n        # A discrete space with 3 possible values (0, 1, 2)\n        self.action_space = gym.spaces.Discrete(3)\n\n# Instantiate an environment\nenv = MyEnv()\nprint('Sample random observation value:', env.observation_space.sample())\nprint('Sample random action value:', env.action_space.sample())\n</pre> import numpy as np import gymnasium as gym  class MyEnv(gym.Env):     def __init__(self):         super().__init__()  # Python boilerplate to initialize base class          # A two-dimensional box with minimum and maximum value set by low and high         self.observation_space = gym.spaces.Box(low=0., high=1., shape=(2,))          # A discrete space with 3 possible values (0, 1, 2)         self.action_space = gym.spaces.Discrete(3)  # Instantiate an environment env = MyEnv() print('Sample random observation value:', env.observation_space.sample()) print('Sample random action value:', env.action_space.sample()) <p>Another key method that needs to be defined is the <code>step</code> method, which updates the environment and outputs observations and rewards after receiving the agent's action.</p> <p>The <code>step</code> method takes <code>action</code> as inputs, and outputs the agent's next observation <code>observation</code>, a scalar reward received by the agent <code>reward</code>, a boolean describing whether the environment needs to be reset <code>done</code>, and a dictionary holding any additional information <code>info</code>.</p> <p>If the environment is described by internal states, the <code>reset</code> method would reset these internal states. This method returns an initial observation <code>observation</code>.</p> In\u00a0[\u00a0]: Copied! <pre>class MyEnv(gym.Env):\n    def __init__(self):\n        super().__init__()  # Python boilerplate to initialize base class\n        self.observation_space = gym.spaces.Box(low=-10., high=10., shape=(1,))\n        self.action_space = gym.spaces.Discrete(3)\n\n    def step(self, action):\n        ob = self.observation_space.sample()  # random sampling\n        reward = 1.  # reward\n        terminated = False  # never ending\n        truncated = False\n        info = {}  # empty dictionary\n        return ob, reward, terminated, truncated, info\n\n    def reset(self):\n        ob = self.observation_space.sample()\n        return ob, {}\n</pre> class MyEnv(gym.Env):     def __init__(self):         super().__init__()  # Python boilerplate to initialize base class         self.observation_space = gym.spaces.Box(low=-10., high=10., shape=(1,))         self.action_space = gym.spaces.Discrete(3)      def step(self, action):         ob = self.observation_space.sample()  # random sampling         reward = 1.  # reward         terminated = False  # never ending         truncated = False         info = {}  # empty dictionary         return ob, reward, terminated, truncated, info      def reset(self):         ob = self.observation_space.sample()         return ob, {} <p>Below we define a simple task where actions move an agent along a one-dimensional line. The reward is determined by the agent's location on this line.</p> In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\n\ndef get_reward(x):\n    return np.sin(x) * np.exp(-np.abs(x)/3)\n\nxs = np.linspace(-10, 10, 100)\nplt.plot(xs, get_reward(xs))\nplt.xlabel('State value (observation)')\nplt.ylabel('Reward')\n</pre> import matplotlib.pyplot as plt  def get_reward(x):     return np.sin(x) * np.exp(-np.abs(x)/3)  xs = np.linspace(-10, 10, 100) plt.plot(xs, get_reward(xs)) plt.xlabel('State value (observation)') plt.ylabel('Reward') In\u00a0[\u00a0]: Copied! <pre>class MyEnv(gym.Env):\n    def __init__(self):\n        # A one-dimensional box with minimum and maximum value set by low and high\n        self.observation_space = gym.spaces.Box(low=-10., high=10., shape=(1,))\n\n        # A discrete space with 3 possible values (0, 1, 2)\n        self.action_space = gym.spaces.Discrete(3)\n\n        self.state = 0.\n\n    def step(self, action):\n        # Actions 0, 1, 2 correspond to state change of -0.1, 0, +0.1\n        self.state += (action - 1.) * 0.1\n        self.state = np.clip(self.state, -10, 10)\n\n        ob = self.state  # observation\n        reward = get_reward(self.state)  # reward\n        terminated = False  # never ending\n        truncated = False\n        info = {}  # empty dictionary\n        return ob, reward, terminated, truncated, info\n\n    def reset(self):\n        # Re-initialize state\n        self.state = self.observation_space.sample()\n        return self.state, {}\n</pre> class MyEnv(gym.Env):     def __init__(self):         # A one-dimensional box with minimum and maximum value set by low and high         self.observation_space = gym.spaces.Box(low=-10., high=10., shape=(1,))          # A discrete space with 3 possible values (0, 1, 2)         self.action_space = gym.spaces.Discrete(3)          self.state = 0.      def step(self, action):         # Actions 0, 1, 2 correspond to state change of -0.1, 0, +0.1         self.state += (action - 1.) * 0.1         self.state = np.clip(self.state, -10, 10)          ob = self.state  # observation         reward = get_reward(self.state)  # reward         terminated = False  # never ending         truncated = False         info = {}  # empty dictionary         return ob, reward, terminated, truncated, info      def reset(self):         # Re-initialize state         self.state = self.observation_space.sample()         return self.state, {} <p>An agent can interact with the environment iteratively.</p> In\u00a0[\u00a0]: Copied! <pre>env = MyEnv()\nob, _ = env.reset()\nob_log = list()\nreward_log = list()\nfor i in range(10000):\n    action = env.action_space.sample()  # A random agent\n    ob, reward, terminated, truncated, info = env.step(action)\n    ob_log.append(ob)\n    reward_log.append(reward)\n\nplt.plot(ob_log, reward_log)\n</pre> env = MyEnv() ob, _ = env.reset() ob_log = list() reward_log = list() for i in range(10000):     action = env.action_space.sample()  # A random agent     ob, reward, terminated, truncated, info = env.step(action)     ob_log.append(ob)     reward_log.append(reward)  plt.plot(ob_log, reward_log) In\u00a0[\u00a0]: Copied! <pre>import neurogym as ngym\nfrom neurogym import TrialEnv\n\nclass MyTrialEnv(TrialEnv):\n    def __init__(self):\n        super().__init__()\n        self.observation_space = gym.spaces.Box(low=-1., high=1., shape=(1,))\n        self.action_space = gym.spaces.Discrete(2)\n\n        self.next_ob = np.random.uniform(-1, 1, size=(1,))\n\n    def _new_trial(self):\n        ob = self.next_ob  # observation previously computed\n        # Sample observation for the next trial\n        self.next_ob = np.random.uniform(-1, 1, size=(1,))\n\n        trial = dict()\n        # Ground-truth is 1 if ob &gt; 0, else 0\n        trial['ground_truth'] = (ob &gt; 0) * 1.0\n\n        return trial\n\n    def _step(self, action):\n        ob = self.next_ob\n        # If action equals to ground_truth, reward=1, otherwise 0\n        reward = (action == self.trial['ground_truth']) * 1.0\n        terminated = False\n        truncated = False\n        info = {'new_trial': True}\n        return ob, reward, terminated, truncated, info\n</pre> import neurogym as ngym from neurogym import TrialEnv  class MyTrialEnv(TrialEnv):     def __init__(self):         super().__init__()         self.observation_space = gym.spaces.Box(low=-1., high=1., shape=(1,))         self.action_space = gym.spaces.Discrete(2)          self.next_ob = np.random.uniform(-1, 1, size=(1,))      def _new_trial(self):         ob = self.next_ob  # observation previously computed         # Sample observation for the next trial         self.next_ob = np.random.uniform(-1, 1, size=(1,))          trial = dict()         # Ground-truth is 1 if ob &gt; 0, else 0         trial['ground_truth'] = (ob &gt; 0) * 1.0          return trial      def _step(self, action):         ob = self.next_ob         # If action equals to ground_truth, reward=1, otherwise 0         reward = (action == self.trial['ground_truth']) * 1.0         terminated = False         truncated = False         info = {'new_trial': True}         return ob, reward, terminated, truncated, info In\u00a0[\u00a0]: Copied! <pre>env = MyTrialEnv()\nob, _ = env.reset()\n\nprint('Trial', 0)\nprint('Received observation', ob)\n\nfor i in range(5):\n    action = env.action_space.sample()  # A random agent\n    print('Selected action', action)\n    ob, reward, terminated, truncated, info = env.step(action)\n    print('Received reward', reward)\n    print('Trial', i+1)\n    print('Received observation', ob)\n</pre> env = MyTrialEnv() ob, _ = env.reset()  print('Trial', 0) print('Received observation', ob)  for i in range(5):     action = env.action_space.sample()  # A random agent     print('Selected action', action)     ob, reward, terminated, truncated, info = env.step(action)     print('Received reward', reward)     print('Trial', i+1)     print('Received observation', ob) In\u00a0[\u00a0]: Copied! <pre>class MyDecisionEnv(TrialEnv):\n    def __init__(self, dt=100, timing=None):\n        super().__init__(dt=dt)  # dt is passed to base task\n\n        # Setting default task timing\n        self.timing = {'stimulus': 500, 'decision': 500}\n        # Update timing if provided externally\n        if timing:\n            self.timing.update(timing)\n\n        self.observation_space = gym.spaces.Box(low=-1., high=1., shape=(1,))\n        self.action_space = gym.spaces.Discrete(2)\n\n    def _new_trial(self):\n        # Setting time periods for this trial\n        periods = ['stimulus', 'decision']\n        # Will add stimulus and decision periods sequentially using self.timing info\n        self.add_period(periods)\n\n        # Sample observation for the next trial\n        stimulus = np.random.uniform(-1, 1, size=(1,))\n\n        trial = dict()\n        trial['stimulus'] = stimulus\n        # Ground-truth is 1 if stimulus &gt; 0, else 0\n        trial['ground_truth'] = (stimulus &gt; 0) * 1.0\n\n        return trial\n\n    def _step(self, action):\n        # Check if the current time step is in stimulus period\n        if self.in_period('stimulus'):\n            ob = np.array([self.trial['stimulus']])\n            reward = 0.  # no reward\n        else:\n            ob = np.array([0.])  # no observation\n            # If action equals to ground_truth, reward=1, otherwise 0\n            reward = (action == self.trial['ground_truth']) * 1.0\n\n        terminated = False\n        truncated = False\n        # By default, the trial is not ended\n        info = {'new_trial': False}\n        return ob, reward, terminated, truncated, info\n</pre> class MyDecisionEnv(TrialEnv):     def __init__(self, dt=100, timing=None):         super().__init__(dt=dt)  # dt is passed to base task          # Setting default task timing         self.timing = {'stimulus': 500, 'decision': 500}         # Update timing if provided externally         if timing:             self.timing.update(timing)          self.observation_space = gym.spaces.Box(low=-1., high=1., shape=(1,))         self.action_space = gym.spaces.Discrete(2)      def _new_trial(self):         # Setting time periods for this trial         periods = ['stimulus', 'decision']         # Will add stimulus and decision periods sequentially using self.timing info         self.add_period(periods)          # Sample observation for the next trial         stimulus = np.random.uniform(-1, 1, size=(1,))          trial = dict()         trial['stimulus'] = stimulus         # Ground-truth is 1 if stimulus &gt; 0, else 0         trial['ground_truth'] = (stimulus &gt; 0) * 1.0          return trial      def _step(self, action):         # Check if the current time step is in stimulus period         if self.in_period('stimulus'):             ob = np.array([self.trial['stimulus']])             reward = 0.  # no reward         else:             ob = np.array([0.])  # no observation             # If action equals to ground_truth, reward=1, otherwise 0             reward = (action == self.trial['ground_truth']) * 1.0          terminated = False         truncated = False         # By default, the trial is not ended         info = {'new_trial': False}         return ob, reward, terminated, truncated, info <p>Running the environment with a random agent and plotting the agent's observation, action, and rewards</p> In\u00a0[\u00a0]: Copied! <pre># Logging\nlog = {'ob': [], 'gt': [], 'action': [], 'reward': []}\n\nenv = MyDecisionEnv(dt=100)\nob, _ = env.reset()\nlog['ob'].append(float(ob))\nlog['gt'].append(float(ob &gt; 0))\nfor i in range(30):\n    action = env.action_space.sample()  # A random agent\n    ob, reward, terminated, truncated, info = env.step(action)\n\n    log['action'].append(float(action))\n    log['ob'].append(float(ob))\n    log['gt'].append(float(ob &gt; 0))\n    log['reward'].append(float(reward))\n\nlog['ob'] = log['ob'][:-1]  # exclude last observation\nlog['gt'] = log['gt'][:-1]  # exclude last observation\n# Visualize\nf, axes = plt.subplots(len(log), 1, sharex=True)\nfor ax, key in zip(axes, log):\n    ax.plot(log[key], '.-')\n    ax.set_ylabel(key)\n</pre> # Logging log = {'ob': [], 'gt': [], 'action': [], 'reward': []}  env = MyDecisionEnv(dt=100) ob, _ = env.reset() log['ob'].append(float(ob)) log['gt'].append(float(ob &gt; 0)) for i in range(30):     action = env.action_space.sample()  # A random agent     ob, reward, terminated, truncated, info = env.step(action)      log['action'].append(float(action))     log['ob'].append(float(ob))     log['gt'].append(float(ob &gt; 0))     log['reward'].append(float(reward))  log['ob'] = log['ob'][:-1]  # exclude last observation log['gt'] = log['gt'][:-1]  # exclude last observation # Visualize f, axes = plt.subplots(len(log), 1, sharex=True) for ax, key in zip(axes, log):     ax.plot(log[key], '.-')     ax.set_ylabel(key) In\u00a0[\u00a0]: Copied! <pre>class MyDecisionEnv(TrialEnv):\n    def __init__(self, dt=100, timing=None):\n        super().__init__(dt=dt)  # dt is passed to base task\n\n        # Setting default task timing\n        self.timing = {'stimulus': 500, 'decision': 500}\n        # Update timing if provided externally\n        if timing:\n            self.timing.update(timing)\n\n        # Here we use ngym.spaces, which allows setting name of each dimension\n        name = {'fixation': 0, 'stimulus': 1}\n        self.observation_space = ngym.spaces.Box(\n            low=-1., high=1., shape=(2,), name=name)\n        name = {'fixation': 0, 'choice': [1, 2]}\n        self.action_space = ngym.spaces.Discrete(3, name=name)\n\n    def _new_trial(self):\n        # Setting time periods for this trial\n        periods = ['stimulus', 'decision']\n        # Will add stimulus and decision periods sequentially using self.timing info\n        self.add_period(periods)\n\n        # Sample observation for the next trial\n        stimulus = np.random.uniform(-1, 1, size=(1,))\n\n        # Add value 1 to stimulus period at fixation location\n        self.add_ob(1, period='stimulus', where='fixation')\n        # Add value stimulus to stimulus period at stimulus location\n        self.add_ob(stimulus, period='stimulus', where='stimulus')\n\n        # Set ground_truth\n        groundtruth = int(stimulus &gt; 0)\n        self.set_groundtruth(groundtruth, period='decision', where='choice')\n\n        trial = dict()\n        trial['stimulus'] = stimulus\n        trial['ground_truth'] = groundtruth\n\n        return trial\n\n    def _step(self, action):\n        # self.ob_now and self.gt_now correspond to\n        # current step observation and groundtruth\n\n        # If action equals to ground_truth, reward=1, otherwise 0\n        reward = (action == self.gt_now) * 1.0\n\n        terminated = False\n        truncated = False\n        # By default, the trial is not ended\n        info = {'new_trial': False}\n        return self.ob_now, reward, terminated, truncated, info\n</pre> class MyDecisionEnv(TrialEnv):     def __init__(self, dt=100, timing=None):         super().__init__(dt=dt)  # dt is passed to base task          # Setting default task timing         self.timing = {'stimulus': 500, 'decision': 500}         # Update timing if provided externally         if timing:             self.timing.update(timing)          # Here we use ngym.spaces, which allows setting name of each dimension         name = {'fixation': 0, 'stimulus': 1}         self.observation_space = ngym.spaces.Box(             low=-1., high=1., shape=(2,), name=name)         name = {'fixation': 0, 'choice': [1, 2]}         self.action_space = ngym.spaces.Discrete(3, name=name)      def _new_trial(self):         # Setting time periods for this trial         periods = ['stimulus', 'decision']         # Will add stimulus and decision periods sequentially using self.timing info         self.add_period(periods)          # Sample observation for the next trial         stimulus = np.random.uniform(-1, 1, size=(1,))          # Add value 1 to stimulus period at fixation location         self.add_ob(1, period='stimulus', where='fixation')         # Add value stimulus to stimulus period at stimulus location         self.add_ob(stimulus, period='stimulus', where='stimulus')          # Set ground_truth         groundtruth = int(stimulus &gt; 0)         self.set_groundtruth(groundtruth, period='decision', where='choice')          trial = dict()         trial['stimulus'] = stimulus         trial['ground_truth'] = groundtruth          return trial      def _step(self, action):         # self.ob_now and self.gt_now correspond to         # current step observation and groundtruth          # If action equals to ground_truth, reward=1, otherwise 0         reward = (action == self.gt_now) * 1.0          terminated = False         truncated = False         # By default, the trial is not ended         info = {'new_trial': False}         return self.ob_now, reward, terminated, truncated, info <p>Sampling one trial. The trial observation and ground-truth can be used for supervised learning.</p> In\u00a0[\u00a0]: Copied! <pre>env = MyDecisionEnv()\nenv.reset()\n\ntrial = env.new_trial()\nob, gt = env.ob, env.gt\n\nprint('Trial information', trial)\nprint('Observation shape is (N_time, N_unit) =', ob.shape)\nprint('Groundtruth shape is (N_time,) =', gt.shape)\n</pre> env = MyDecisionEnv() env.reset()  trial = env.new_trial() ob, gt = env.ob, env.gt  print('Trial information', trial) print('Observation shape is (N_time, N_unit) =', ob.shape) print('Groundtruth shape is (N_time,) =', gt.shape) <p>Visualizing the environment with a helper function.</p> In\u00a0[\u00a0]: Copied! <pre># Run the environment for 2 trials using a random agent.\nfig = ngym.utils.plot_env(\n    env,\n    ob_traces=['stimulus', 'fixation'],\n    num_trials=2,\n)\n</pre> # Run the environment for 2 trials using a random agent. fig = ngym.utils.plot_env(     env,     ob_traces=['stimulus', 'fixation'],     num_trials=2, ) In\u00a0[\u00a0]: Copied! <pre>class PerceptualDecisionMaking(ngym.TrialEnv):\n    \"\"\"Two-alternative forced choice task in which the subject has to\n    integrate two stimuli to decide which one is higher on average.\n\n    Args:\n        stim_scale: Controls the difficulty of the experiment. (def: 1., float)\n        sigma: float, input noise level\n        dim_ring: int, dimension of ring input and output\n    \"\"\"\n    metadata = {\n        'paper_link': 'https://www.jneurosci.org/content/12/12/4745',\n        'paper_name': '''The analysis of visual motion: a comparison of\n        neuronal and psychophysical performance''',\n        'tags': ['perceptual', 'two-alternative', 'supervised']\n    }\n\n    def __init__(self, dt=100, rewards=None, timing=None, stim_scale=1.,\n                 sigma=1.0, dim_ring=2):\n        super().__init__(dt=dt)\n        # The strength of evidence, modulated by stim_scale\n        self.cohs = np.array([0, 6.4, 12.8, 25.6, 51.2]) * stim_scale\n        self.sigma = sigma / np.sqrt(self.dt)  # Input noise\n\n        # Rewards\n        self.rewards = {'abort': -0.1, 'correct': +1., 'fail': 0.}\n        if rewards:\n            self.rewards.update(rewards)\n\n        self.timing = {\n            'fixation': 100,\n            'stimulus': 2000,\n            'delay': 0,\n            'decision': 100}\n        if timing:\n            self.timing.update(timing)\n\n        self.abort = False\n\n        self.theta = np.linspace(0, 2*np.pi, dim_ring+1)[:-1]\n        self.choices = np.arange(dim_ring)\n\n        name = {'fixation': 0, 'stimulus': range(1, dim_ring+1)}\n        self.observation_space = ngym.spaces.Box(\n            -np.inf, np.inf, shape=(1+dim_ring,), dtype=np.float32, name=name)\n        name = {'fixation': 0, 'choice': range(1, dim_ring+1)}\n        self.action_space = ngym.spaces.Discrete(1+dim_ring, name=name)\n\n    def _new_trial(self, **kwargs):\n        # Trial info\n        trial = {\n            'ground_truth': self.rng.choice(self.choices),\n            'coh': self.rng.choice(self.cohs),\n        }\n        trial.update(kwargs)\n\n        coh = trial['coh']\n        ground_truth = trial['ground_truth']\n        stim_theta = self.theta[ground_truth]\n\n        # Periods\n        self.add_period(['fixation', 'stimulus', 'delay', 'decision'])\n\n        # Observations\n        self.add_ob(1, period=['fixation', 'stimulus', 'delay'], where='fixation')\n        stim = np.cos(self.theta - stim_theta) * (coh/200) + 0.5\n        self.add_ob(stim, 'stimulus', where='stimulus')\n        self.add_randn(0, self.sigma, 'stimulus', where='stimulus')\n\n        # Ground truth\n        self.set_groundtruth(ground_truth, period='decision', where='choice')\n\n        return trial\n\n    def _step(self, action):\n        new_trial = False\n        terminated = False\n        truncated = False\n        # rewards\n        reward = 0\n        gt = self.gt_now\n        # observations\n        if self.in_period('fixation'):\n            if action != 0:  # action = 0 means fixating\n                new_trial = self.abort\n                reward += self.rewards['abort']\n        elif self.in_period('decision'):\n            if action != 0:\n                new_trial = True\n                if action == gt:\n                    reward += self.rewards['correct']\n                    self.performance = 1\n                else:\n                    reward += self.rewards['fail']\n\n        return self.ob_now, reward, terminated, truncated, {'new_trial': new_trial, 'gt': gt}\n</pre> class PerceptualDecisionMaking(ngym.TrialEnv):     \"\"\"Two-alternative forced choice task in which the subject has to     integrate two stimuli to decide which one is higher on average.      Args:         stim_scale: Controls the difficulty of the experiment. (def: 1., float)         sigma: float, input noise level         dim_ring: int, dimension of ring input and output     \"\"\"     metadata = {         'paper_link': 'https://www.jneurosci.org/content/12/12/4745',         'paper_name': '''The analysis of visual motion: a comparison of         neuronal and psychophysical performance''',         'tags': ['perceptual', 'two-alternative', 'supervised']     }      def __init__(self, dt=100, rewards=None, timing=None, stim_scale=1.,                  sigma=1.0, dim_ring=2):         super().__init__(dt=dt)         # The strength of evidence, modulated by stim_scale         self.cohs = np.array([0, 6.4, 12.8, 25.6, 51.2]) * stim_scale         self.sigma = sigma / np.sqrt(self.dt)  # Input noise          # Rewards         self.rewards = {'abort': -0.1, 'correct': +1., 'fail': 0.}         if rewards:             self.rewards.update(rewards)          self.timing = {             'fixation': 100,             'stimulus': 2000,             'delay': 0,             'decision': 100}         if timing:             self.timing.update(timing)          self.abort = False          self.theta = np.linspace(0, 2*np.pi, dim_ring+1)[:-1]         self.choices = np.arange(dim_ring)          name = {'fixation': 0, 'stimulus': range(1, dim_ring+1)}         self.observation_space = ngym.spaces.Box(             -np.inf, np.inf, shape=(1+dim_ring,), dtype=np.float32, name=name)         name = {'fixation': 0, 'choice': range(1, dim_ring+1)}         self.action_space = ngym.spaces.Discrete(1+dim_ring, name=name)      def _new_trial(self, **kwargs):         # Trial info         trial = {             'ground_truth': self.rng.choice(self.choices),             'coh': self.rng.choice(self.cohs),         }         trial.update(kwargs)          coh = trial['coh']         ground_truth = trial['ground_truth']         stim_theta = self.theta[ground_truth]          # Periods         self.add_period(['fixation', 'stimulus', 'delay', 'decision'])          # Observations         self.add_ob(1, period=['fixation', 'stimulus', 'delay'], where='fixation')         stim = np.cos(self.theta - stim_theta) * (coh/200) + 0.5         self.add_ob(stim, 'stimulus', where='stimulus')         self.add_randn(0, self.sigma, 'stimulus', where='stimulus')          # Ground truth         self.set_groundtruth(ground_truth, period='decision', where='choice')          return trial      def _step(self, action):         new_trial = False         terminated = False         truncated = False         # rewards         reward = 0         gt = self.gt_now         # observations         if self.in_period('fixation'):             if action != 0:  # action = 0 means fixating                 new_trial = self.abort                 reward += self.rewards['abort']         elif self.in_period('decision'):             if action != 0:                 new_trial = True                 if action == gt:                     reward += self.rewards['correct']                     self.performance = 1                 else:                     reward += self.rewards['fail']          return self.ob_now, reward, terminated, truncated, {'new_trial': new_trial, 'gt': gt} In\u00a0[\u00a0]: Copied! <pre>env = PerceptualDecisionMaking()\nfig = ngym.utils.plot_env(\n    env,\n    ob_traces=['Stim1', 'Stim2', 'fixation'],\n    num_trials=2,\n)\n</pre> env = PerceptualDecisionMaking() fig = ngym.utils.plot_env(     env,     ob_traces=['Stim1', 'Stim2', 'fixation'],     num_trials=2, )"},{"location":"examples/understanding_neurogym_task/#understanding-neurogym-task","title":"Understanding Neurogym Task\u00b6","text":"<p>This is a tutorial for understanding Neurogym task structure. Here we will go through</p> <ol> <li>Defining a basic gymnasium task</li> <li>Defining a basic trial-based neurogym task</li> <li>Adding observation and ground truth in neurogym tasks</li> </ol>"},{"location":"examples/understanding_neurogym_task/#installation","title":"Installation\u00b6","text":"<p>Only needed if running in Google colab. Uncomment to run.</p>"},{"location":"examples/understanding_neurogym_task/#gymnasium-tasks","title":"Gymnasium tasks\u00b6","text":""},{"location":"examples/understanding_neurogym_task/#visualize-results","title":"Visualize results\u00b6","text":""},{"location":"examples/understanding_neurogym_task/#trial-based-neurogym-tasks","title":"Trial-based Neurogym Tasks\u00b6","text":"<p>Many neuroscience and cognitive science tasks have trial structure. <code>neurogym.TrialEnv</code> provides a class for common trial-based tasks. Its main difference from <code>gymnasium.Env</code> is the <code>_new_trial()</code> method that generates abstract information about a new trial, and optionally, the observation and ground-truth output. Additionally, users provide a <code>_step()</code> method instead of <code>step()</code>.</p> <p>The <code>_new_trial()</code> method takes any key-word arguments (<code>**kwargs</code>), and outputs a dictionary <code>trial</code> containing relevant information about this trial. This dictionary is accesible during <code>_step</code> as <code>self.trial</code>.</p> <p>Here we define a simple task where the agent needs to make a binary decision on every trial based on its observation. Each trial is only one time step.</p>"},{"location":"examples/understanding_neurogym_task/#including-time-period-and-observation-in-trial-based-tasks","title":"Including time, period, and observation in trial-based tasks\u00b6","text":"<p>Most neuroscience and cognitive science tasks follow additional temporal structures that are incorporated into <code>neurogym.TrialEnv</code>. These tasks typically</p> <ol> <li>Are described in real time instead of discrete time steps. For example, the task can last 3 seconds.</li> <li>Contain multiple time periods in each trial, such as a stimulus period and a response period.</li> </ol> <p>To include these features, neurogym tasks typically support setting the time length of each step in <code>dt</code> (in ms), and the time length of each time period in <code>timing</code>.</p> <p>For example, consider the following binary decision-making task with a 500ms stimulus period, followed by a 500ms decision period. The periods are added to each trial through <code>self.add_period()</code> in <code>self._new_trial()</code>. During <code>_step()</code>, you can check which period the task is currently in with <code>self.in_period(period_name)</code>.</p>"},{"location":"examples/understanding_neurogym_task/#setting-observation-and-ground-truth-at-the-beginning-of-each-trial","title":"Setting observation and ground-truth at the beginning of each trial\u00b6","text":"<p>In many tasks, the observation and ground-truth are pre-determined for each trial, and can be set in <code>self._new_trial()</code>. The generated observation and ground-truth can then be used as inputs and targets for supervised learning.</p> <p>Observation and ground_truth can be set in <code>self._new_trial()</code> with the <code>self.add_ob()</code> and <code>self.set_groundtruth</code> methods. Users can specify the period and location of the observation using their names. For example, <code>self.add_ob(1, period='stimulus', where='fixation')</code>.</p> <p>This allows the users to access the observation and groundtruth of the entire trial with <code>self.ob</code> and <code>self.gt</code>, and access their values with <code>self.ob_now</code> and <code>self.gt_now</code>.</p>"},{"location":"examples/understanding_neurogym_task/#an-example-perceptual-decision-making-task","title":"An example perceptual decision-making task\u00b6","text":"<p>Using the above style, we can define a simple perceptual decision-making task (the PerceptualDecisionMaking task from neurogym).</p>"}]}